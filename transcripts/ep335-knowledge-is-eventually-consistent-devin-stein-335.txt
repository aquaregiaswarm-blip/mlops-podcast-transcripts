Um, so code is like a really really good system of record. And maybe you can extend the analogy to other types of system, records, internally where, you know, they are truth. And you can monitor changes on those and reflect those back into the knowledge base. But unless you have like a digital system of record, it's really hard to reconcile things. So there's still going to be meeting notes and like other types of documents that might be floating around, but certain types of knowledge, like, how to our product actually work. Like, I think there should always be a very clear answer
 Devon Stein.
 CEO and founder of dosu, and drink my coffee black. Generally, pour overs in the morning, but we have an espresso machine at the office, so also drink quite a bit of this recipe.
 Let's talk about the facts agent. Can you break it down for me? What is it? Exactly. Before we go into like the details?
 Yeah, I mean, basically it's our. So before we get into the details, you know, like it's dose was a product. We got our start helping out with open source maintenance, basically, kind of the premise of dosu was
 As an engineer actually, don't spend that much time coding especially as I grew more senior, my career. And as I open source container, like a lot of my time was spent answering questions in triaging issues. So, I started those who to focus on like, hey, can we answer questions in triage issues, like, Engineers can by looking at code, commits conversations and tickets, like everything kind of around the code base. And the product kind of Engineers are unique in organizations because code is truth, like it really tells you how your part actually works. And so whenever there's ambiguity, you need to escalate something to an engineer. And so we can build those who kind of our initial agent to answer questions and triage issues whether they're like incoming in slack or on GitHub issues. In the case of Open Source, maintenance and dose has been, you know very popular with an open source, helping answer questions and triage incoming GitHub issues. And we just launched kind of our second iteration of our Asian we're calling our faced
 reasoning agent and the premise is exactly this idea of that like, hey,
 Users ask very related questions over and over again and those who, you know, does an investigation every time kind of starting from scratch, like most agents do like, hey okay, we're gonna have searched the code base. We're gonna look at recent PR's. We're gonna you know, scroll through slack and but a lot of that work is kind of duplicated across different agent runs and so this new design has it. So as doses doing research in a given prune, it's learning what we call facts, which are like claim supported by evidence that found and it uses those facts when generating its response now and if the response is correct which we get from, you know, either direct user feedback or from like a maintainer expert that jumps into the thread, then those facts are committed to its knowledge base.
 And then next time, someone asks a similar question about related topics. Those who first take stock of like, hey, what do I know about these Topics? In terms of The Facts of my knowledge base? Do I have enough information to respond? If not what am I missing and then it'll do additional research to find that information respond. And it's kind of this, nice sort of learning Loop where the more you use the product, the more facts, it learns to better, it gets and the faster it gets, okay. So the facts flywheel is fascinating to me, and all. So the whole idea of
 How? And when you get the agent to jump in is probably the most fascinating because
 How does it get solidified that a decision has been made? How does an agent know that a decision has been made? And it's not just continuing a conversation or it needs to be picked up. Again, when XYZ happens like there's so many
 X factors and intangibles. That feels like, it's a very hard problem for an agent to solve.
 Or maybe not yes, in the general case, I think it is very hard to have that human intuition of like, when I should jump into this, like, what is my expertise needed? I think, in the domain we work in, it's usually who is asking the question? Like the audience actually matters a lot if we have its you know the kind of ticket or issue is coming from a maintainer. They probably know generally about the problem and maybe like a good response with then be like pointers and like where in the code base or recent works or no response at all because they probably have it under control. First is if a user is asking a question and they are baby non-technical or less technical or new to the project then usually help is like, you know, any information to help them go from where they are to kind of closer to where they want to be in terms of resolving or answering. Their question is, welcome. So, I think audience
 plays like a pretty important role when thinking about, like, doesn't agent want to jump into this conversation. So do you have to have user profiles built up? That the agent is aware of
 Yes, we have kind of a pretty simple structure right now where we have basically experts who are users in the app, the curators of knowledge and then normal users, who are less familiar with the domain in the future. I think there's a lot of cool things we could do around, you know, different types of audiences, whether you're fully non-technical, and you can only understand things in a product terms versus, you're an engineer that is just unfamiliar with this code base, but you can read and write code.
 Yes. And where does it?
 Triage items from do you plug into jira do plug into confluent notion, like click up? Is it all of that? Because, I know documentation, and I was expressing this before we hit record, it can be such a pain
 And a lot of times, it's a pain because it's just so dispersed.
 Yes, I think that's one of the really interesting things and sort of about llms is that end humans, right? If you think about human memory, something, you know, Engineers like our senior engineer at a company or an open source maintainer has been on a project for a while. Something that they do that is, you know, really, really unique is they're able to see the activity across all these different apps, see what's going on in slack. Maybe in clickup and jira across PR, what has been merged? What has been like, you know what issues have come up? And then actually connect the dots between these different data sources, kind of see the connections in terms of topics how they relate together, and we try to take a similar Approach at dosu around. Basically, trying to figure out, you know, what is the product or engineering ontology for your organization? So, you know what are the key? Concepts topics product features components on the engineering side and then how can we
 Relate the conversations that are happening across these disparate apps. The documentation that lives across these different apps back to those Core Concepts and start building the connections between them. It makes it easier for us to then, you know, when we're talking about a specific part of the product where that information might live across everything,
 This episode is brought to you by mlflow the open source platform. Trusted by teams worldwide, to manage the entire ML and gen AI lifecycle. And with free managed ml flow on databricks. You get all the benefits of mlflow plus automated infrastructure unified experiment, tracking model, versioning observability and enterprise-grade governance. All in one place, ship reliable models to production faster with less hassle. Get started at mlflow dot or check out the recent talk that we had from Eric Peter at our SF agent Builders Summit.
 Mlflow dot org. Links in the description, we had on here.
 a few months ago, donate who created a
 data analyst agent, and one of the hardest challenges that she talked about was how
 Words.
 Don't mean the same thing, always.
 words are hard, basically, was the long and short of it and she used it in the context of if you're a data analyst, you have all this jargon that you say,
 and that you want, you ask questions to an agent about but those questions and those words all that jargon you have to
 Explain to an agent, what that means because and I always go back to the most simple answer that I can wrap my head around, which is an mql at one company or marketing qualified lead at one company.
 Can mean one thing?
 At this company. But at the other company you have to do five different things to become an mql. So even though we're using the same word, it is not the same. When the data analysts are asking questions of the LMS. And what they did to fix, that is what they essentially had to create a glossary of terms, so that when a data analyst would use this jargon, the llm could reference that, and it could say, okay, cool. I understand what an mql is at this company.
 How have you dealt with that? Because I can only imagine in your world. It is Multiplied times 100.
 Yeah. I mean we have a very similar approach where four topics they have aliases or synonyms that, you know, also referred to as it's kind of like the relationship. So, you know, we have a lot of examples of this. I think every company does, you know, where we call something one, name in the front end, but then in the back end actually has a historic name that's completely different. And so we just go back and forth internally using either the front end term or the backend term or another thing is like, you know, the word task, we have like three or four different terms for tasks in our backend code base. Could be a celery task, it could be an llm tasks, could be this background tasks and, you know, how should we differentiate for giving the same term? You know, what other what what is the meaning of this term in this context is also like a related and hard problem. Another thing that's kind of
 On and relates back to like auto reply, or like jumping into conversations is like conversation and Precure. Like what is the implied meeting of a comment in slack? Especially like you when you someone will post in slack, like, you know, how do we do this thing? And you know, what is we in this case? Is it like the team they're on? Is it, you know, the engineering team like of the channel? There's a lot of like, implied meanings of, like, the way people Converse in shared channels. When it's like, you kind of know the dough kind of domain of discourse, like, what people are generally talking about and having the L and do the same sort of, you know, figuring out what is the implied, meaning. What is this person actually looking for is fun and challenging? It's so fuzzy, isn't it? It's just like, yeah, we Who you calling? We, yeah, right. What do you mean by that?
 And how do you?
 think about
 adding value but not being too noisy because in llm is always gonna say that it knows something and it's probably always going to want to jump into a conversation but that actually adds cognitive load at the end of the day. If I have to read a one-page report for a simple question that I have and
 It really isn't getting to that sense of the question. I'm going to be pretty pissed at that tool.
 Yep, I think it's one of the hardest problems generally in the agent space. So I think there's two pieces of that. One is conciseness is generally very hard with LMS partially the way I think they're both in pre-training and kind of fine-tuned. They tend to be more verbose than what a human counterpart would actually say. And so we've invested a lot in reducing verbosity like keeping responses concise because you're right. Like the it's one thing to be wrong and have a two sentence answer but to be wrong with like a two paragraph or page answer. Then it takes a lot of time and cognitive low to figure out. Hey the LMS not correct. So I think the way you deliver information we've thought a lot about it. I think that we solve a long way to go in order to kind of be concise be you know point back to reference sources as well can help with that. So instead of like we stating what exists in the
 Kind of reference source or fact, kind of Link back to it. Can help shorten things. The other side of it is, you know, when do llms know if they have, you know, do they know the answer or they confident enough to respond also like open research problem. I actually just saw a really interesting paper, I can't remember the name. It was something to do with abundance was in the title that came out of fair at meta.
 Yesterday where they actually found that reasoning agents are worse at knowing that they don't know the answer. So, there are no because, you know, it's almost this overthinking problem where they will even in their Chain of Thought, They might say, they don't know. But by the end of the time, they finish thinking they're like, I got this. It talked himself into it. Yeah, exactly. It's like, yeah, coaching them kind of like, okay, I can respond to this and so, you know, there's ways in which, you know, I think given a specific domain, you can help figure out better like what is your confidence level?
 Um, one is again like audience, so who is asking the question? I think helps dictate quite a bit on. Like what is an appropriate response? And like if they want help and then the other side is kind of going back to the fact-based. Agent is can you ensure that all these statements you make are coming from facts and those facts have been confirmed before either by human expert or kind of, you know, from previous conversations. So the quality of your knowledge. I think also dictates, you know, the quality of your responses.
 let's put a pin in the quality of your knowledge, which is I want to dive into how you
 Think that can improve over time.
 The last thing while we're on this topic of
 These fact-based agents is.
 Do I need to give explicit consent or approval win a fact is immortalized so that it can go and then be rerouted and put into a knowledge base or is that just happening in the background and it's constantly updating. If there's been enough
 Insinuating at it because that feels like something that could easily go off the wheels too. Right. Like you just start going on this circular
 Kind of false fake news type of vicious cycle. And next thing you know like wait, how did that become something that quote unquote, we do here.
 Yes, this is something we think a lot about and actually have I think it relates generally to, like UI or I guess ux for agents in that when we started kind of rolling this out we actually said hey we're going to just generate facts based off all the data that we ingest. So no human looped fully automated and where we ended up is you know we would generate a lot of knowledge. I think the knowledge is correct but you don't know. And so it is this sort of scary Middle Ground of like maybe you're kind of propagating some like false claim. Yeah, that's actually cascading failure. And so where We've Ended up and I think like from a plot product philosophy just makes sense generally for Asians is sort of like three stages to automation. First is the human the loop. So, you know, a like in the open source case a open source.
 Tanner says, you know, like great response either by dosu or they added a response and they're like I want to make sure that those who knows about this topic and they can take an explicit action to save it to their knowledge base. And so there's like, you know, the human is and they then see like a preview of what get saved and they can edit it. So there's like, you know, very high quality knowledge that the human, you know, took the initiative to say save this to my knowledge base and review it. It feels like that is you're asking a lot of someone to do that.
 Yes and no. I think what's unique somewhat about the domain we're in is that usually those who operates in like public forums, so whether that's like an internal slack or you know open source project where there's a lot of people asking questions or looking for help and then there are a few experts and they're you know, even if those who doesn't answer it, like someone will generally, you know, it's like someone's gonna respond to you in slack hopefully and if you don't you'll keep pinging them. And so we actually do usually get a resolution on all the threads that were on. Whether it's those who, you know, driven resolution or human driven resolution. And so it's like a lot less work to say, you know, command to save this to the knowledge base, then it is to like go and update your documentation, which is like kind of the alternative and there's an incentive to do so because, you know, if you do this then next time who's gonna get that, and you won't have to answer that same question. So I think we're fortunate in like our domain that are users.
 Our experts are very incentivized to make the product better. And so that's the human in the loop modality. Then the next one is more, you know, I would say maybe ai-driven it's like kind of in the crawl, walk run more like the walk stage where, you know, dosu is reviewing threads and you know, whether it's a PR or like a conversation and it is saying like, hey, you said something that I either one conflicts with what is in my knowledge base or I don't have in my knowledge base, but it seems important which and and then can actually reach out to that person either in the thread or in like a direct message or can of in the app and say, you know, should I save this to my knowledge base off? What you suggested? So it's kind of a little bit of a proactive. Yeah. Where, you know, we're not saving it directly but we're doing the work to kind of get everything ready in like a preview draft State and then you just
 Have to approve it. It's like a sleeper agent that's there in the background recognizing and how does it know when it seems important? Is that just like my use of
 Uh, like bad words and caps locks? Yeah, it's similar to, you know, a lot of it has been to doing. I think we're still iterating. Yeah. Again like we're all kind of fortunate and that, like, the domain that is discussed is like very it is well-defined. Usually what you want to save to acknowledge base or to your right to your documentation. It's like, hey there was some investigation where the seam like a lot of work, a lot of back and forth or a question that, you know, was very relevant to the domain of like, how do I get started? How do I troubleshoot? So there's like a class of stuff that we're looking for that is likely what you want to put back into the knowledge base. Yeah.
 Nice. And so then let's yeah. You know, the fully automated is kind of where we originally thought we wanted to start. Which is, we get so confident and like we get enough approvals. And all these drops that we can within, you know, start generating knowledge where we're confident automatically, you don't have to be in the loop and then maybe if we're not confident, then that can I get. It's moved to address state for you to review but generally it's like, hands off. You know, those who is extracting knowledge for you generating. Previews when it needs your help and, and then, you know, important, then is like actually maintaining that knowledge, which is the second piece of a puzzle.
 so,
 Because of the rise of all the coding agents.
 How have you seen things change?
 Things are changing in a few different ways. So I think one
 is the
 Like importance. Like ironically I think the importance of written knowledge is only increasing because unlike humans ai's at least currently do not have good memories. And so, you know, it doesn't like you know, there's the internet analogies made off and with like oh, you know agents like having 1000 interns or 100 interns but interns, hopefully will get better over time. Agents are, you know, very, very smart, but they are forgetful, you know, they typically like don't learn from doing in the same way. And so, what we've seen from kind of our users and talking to customers is that agents to much much better when they have in all knowledge to reference of, like, you know, I'm working on billing. Like, where is billing? How does it work in this code base? Or like, you know, how should I think about billing conceptually for this product? And so they, you know, agents need more documentation and the
 Format in which they consume them is different, you know, agents, like unlike people can like read giant Blobs of text and that's good docs for them. You know, it's like very informational dense. Yeah, it doesn't have to be aesthetically pleasing and also I think similarly like connecting like the best docks for agents connect product, Concepts back to the code base. So it's not just talking about like billing in the abstract, but it has references back to like, where it's implemented in what directories, which is, you know, important for people. But I think even more important for agents because they're often like operating on the code base and they want to, they don't want to like, learn about a concept and then have to figure out where it lives the code base, they just want to know where to go in a code base immediately.
 So that's one piece I would say.
 The other side is that.
 You know, agents quoting agents excel in smaller projects, you know, the vibe coding examples but at larger that kind of at scale.
 The it's kind of, it can be very scary to use a coding agent because if you are an engineer and you're asked to make a change and you don't know how to make the change and then you ask an agent to do it. It's, you know, really, you know, it only with fire. Yeah, you play with fire and so kind of a prerequisite to making changes. Is generally understanding the system, like, what is the impact of your changes? And so that also is kind of where documentation or just knowledge becomes more important. But for humans, just like, understand, like, what is the impact of this change? Where should it change? So then you can actually like review and be sort of co-pilot for the coating agent.
 so incredible about the ways that they consume information because that lines up with everything and it's funny, how
 Making the link. So giving them almost like grounding them in code, gives them a better chance of success and then throwing as much information at them as possible really lines up with. Yeah, like give all the context you can and they can suss out. What is actually important for them. Now, this
 Feels like a perfect segue into the quality of knowledge and how you're thinking about making knowledge higher quality in general. Because as I was mentioning before we hit record again, was I love documentation. I'm a huge fan of trying to write. I think a writing helps you, clarify your thoughts. It helps you get down the most important stuff that you want to then take forward.
 But what I've noticed is,
 You have to constantly be updating docs as things change and you all. So have to recognize that
 a lot of things are just going to not be relevant after a certain amount of time.
 And you kind of need to know which ones are relevant in those moments of time. So, how does that knowing that? Like, if I look through my notion, which is where I keep all of my documentation for the community and all of that stuff, if I look through that, I'm not going to say like 80% is obsolete, but it's not.
 It's definitely a majority.
 Is Obsolete. And that's because like every podcast that we've had for the last five years, you know, I have a notion page on them. I'm not using those anymore but they're there and maybe they can be referenced. Maybe it can be something or there's like strategy documents that I've written up in 2022 and those are not relevant or like reflection documents all of this stuff.
 Feels like it would muddy up the waters on documentation and if you want the highest quality documentation for your business, how do you think about like keeping it high quality?
 So, I think you're not alone in that, you know, 80% of your notion is stale. I think that's probably the norm you know. You know what's the saying is like the instant you write docs they're stale and I think that's really true I think the way we think about it and I think generally like a good framing is you want to like lean on humans on for what they're good at and then AI what they're good at and humans are really good about knowing what is actually important so I think like you know writing is still very useful and important exercise of like you usually as like an expert on some tough topic or having just done research like no what are the things that someone in the future need to know and sort of like maybe the story that you want to tell around it and you know, AI can help you, right? But at the end of the day like, you know, your the Nuggets of, you know, expertise or like knowledge that you as the expert, like, put into it, like, that's what's really, really important to have the human in the loop. And so we
 We focus on like what is like, how do we make tools to make it really easy for you have something that you think is important. How do you get it down in a written format? You know, save to a knowledge base or to your documentation as easily as possible because I think, you know, at least from my experience and I know from my cloud of other Engineers, this was there's a lot of things I would like to document, but I just don't have time. Yeah. And so I think part of the puzzle is really being making it easy like 10x. 100x easier to get information out of experts and then the second part of the story is like, okay, how do we maintain it? And how do we, you know what it, what do you how do we know what is truth?
 The way we're approaching it. I think it's Unique to our product, and I think it's, you know, at least currently like,
 It's easiest in the product engineering domain because you have code, which is a source of truth. That is very clear how it changes or when it changes really? And so the way we think, I think there's like two types of I guess documentation and all knowledge. Some of it is is more notes, it's temporal in nature, it's maybe a meeting note, you don't want to update a meeting note because it's a record of what happened. But there's pieces of that that maybe you do want reflected as like canonical a source of Truth. Yeah. And so we're very focused on like the knowledge that you want to be source of Truth.
 How do we keep that up to date? And you know the nice thing is that code is you know another source of truth. That's like very clear what it is. And so as code changes we can compare those to the state of your knowledge base and try to detect inconsistencies. So, you know, you have this in your, you know, source of truth. But your code is now saying this or used to say this, but now says that you should probably update these sets of documents or these facts. So I think making the distinction between what what is information you want to last versus something? That is a kind of a record of the of a point in time. I think it's very important.
 That's so fascinating to
 try and think through that.
 Let's take a second, just talk about our sponsors of today's episode hyperbolic GPU. Cloud delivers, Nvidia h100, that just one dollar and forty nine cents per hour, no sales calls commitments, or surprise fees.
 Spin up, one GPU or scale to thousands in minutes with virtual machines, and bare metal, multi-node clusters featuring high-speed, networking attachable storage and auto top UPS. You only pay for compute when you need. It hyperbolic costs up to 75%. Less than Legacy providers. You need longer runs. Well, get Reserve clusters at predictable rates with instant quotes and fast onboarding. Try hyperbolic h100, power on demand, try it now at app hyperbolic AI in
 just so, you know, the links in the description, let's get back to the show. I've Got a Friend Willem who is working on, like,
 Agents SRE agents, right? It feels like this is
 in some ways.
 The first time that I've thought about, wow, both of these agents.
 Could be best friends.
 Like if the dose of agent is talking with Willems cleric agents and he's trying to root cause and doing more or less the same thing that you're doing, but with root cause analysis and trying to really figure out. Wow, something went wrong or the
 Whatever something is super saturated and servers are failing around the world, or dated August showing this blah blah blah. And here's why
 And then it can sync with those who and get, maybe it uses it for knowledge to help reference different things and and do its job better, or win a root cause analysis is made and then something is committed. It's thrown into those who I wonder if you see that as a way that like the two agents will play together or do you feel like
 it is something that Dosa will eventually start doing is just not there yet.
 So, I know, Willem as well. It's great. Shout out to them. So I actually, I think you're spot on. Like, we want those who to be other agents best friend. So, like, you know, those who can be a knowledge provider for cursor, for cleric because like, you're saying, knowledge is really important for root cause investigations in the SRE world. And then importantly, like, you know, you have run books. How do you make sure run books? Don't go out of sync and go stale because the worst thing you could do is give cleric like a run book, that isn't correct. And then it's going off and doing something that it shouldn't be or that used to be true and it's confused. And so, one of the things, yeah, I have. I was also a question that I had in my mind is, how are you making sure that you're giving the most relevant information? Is it just that you're looking for the most recent? Because sometimes, maybe the most recent isn't the thing that is the most relevant or I guess it's maybe not as
 necessary to say relevant because that's more of a search Problem.
 But if you want to update information, how are you going about updating it and saying this is the source of treatment now?
 Yeah, so I think the way we approach it, so there's a search element to it like kind of but the I think that there's an interesting, you know, comparison between like search and documentation where you can almost think of documentation is like a knowledge cache. And in some ways where someone's done the searches, they have compiled the information and then they've written it down. So you don't have to do those searches again. And so the way we kind of think about it is when we are learning when doses, like learning about a topic or, you know, that from book is associated with, we are trying to keep you know that topic as like a source of Truth documents. So we're you know, as you know, conversations are happening, we are like kind of reflecting those pieces back into the document and so as long as you can find the relevant document, you can trust. You know, it's correct if that makes sense. So you don't have to go through the like of searching through slack or all these other document.
 To try figure out like okay, well, this one happened like three months ago and they said this but then this was one month ago but then they said this again. Yeah.
 Been that feels like the headache but sorry, I cut you off from. Okay, you're now other agents best friends and it makes complete sense like cursor, does much better if it gets given. I imagine these stuff that those who can feed to it.
 Yeah, exactly. And I think there's interesting things where, you know, if you think about how agents are probably gonna become like, the majority consumer of documentation, if they're not there already, they might be in the kind of coding domain.
 Where, you know, again like the format of docs, like, how you think about docs starts to change the apis, you might want to expose also change. So I think like an interesting thing like I think a big challenge at least in like our notion or you know on projects that I've worked on is information. Hierarchy is also like really hard to do and like in traditional documentation knowledge, bases, but maybe don't matter as much in the world where you have like agents being the main consumers of information and maybe there's a better experience that we can build around like how information is organized for the more Discovery kind of modality because that is when it's useful to see everything laid out because you're just trying to, like, figure things out. You're trying to learn, maybe you don't have a question yet, it's fascinating. Yeah, information hierarchy is such a headache, and I can't tell you how many docs I have lost because they were nested inside of like 10, other dogs.
 Just this happened to me just the other day and I was so frustrated. I'm like where the hell is that database? I know it's around here somewhere and I couldn't find it and I used all the key Search terms that I thought I would have called it but since I haven't referenced it for like a year, I had no idea and it was just like going off a feeling of like I think it was somewhere in here and then spent too much time on it and eventually gave up.
 yeah, so I think there's I mean
 I think there's some interesting experience that I mean we haven't built but I think like, you know, I would imagine like where do you think about like, documentation? There's two modalities for it. Really is like how we think. There's the you want an answer. And so you have a question, you want to answer search Chat is actually pretty good for that. The other side is, you are, you know, new to something and you're just trying to explore, you know, what is possible? How should I be thinking about things? And that's where it's really nice to have that direct restructure, you kind of poke through. And so I wonder if there's some more generative experience there on the learning side to be built out of like, hey, maybe you can visualize your knowledge this way and you can be guided through it and a specific way, that's even better than kind of, like our traditional docks hierarchy we have today. Yeah, it just makes me think about those onboarding experiences being so much more custom and so much more tailored to the way that you like to learn. And then,
 You can just be.
 Getting exactly what you're talking about, where I want to see it this way. Or I really want to know what are the main principles that we're working off of and you don't have to go and click through like the start here, read me, or watch this video, or this Loom, this, somebody put together a few months ago or years ago.
 Exactly.
 Well, what else do we want to hit on? Is there anything else that like
 For you specifically.
 Is really?
 Top of mind.
 I think one piece that we I mean we touched a bit on like knowledge maintenance, but I do think it's really like worth.
 I emphasizing that it's like something that humans just aren't it's like a not a job for humans. I think like maintaining knowledge is just the effort to do that to be able to like monitor all the different changes conversations, happening organization reflect that in documentation is near impossible. I mean, some organizations have technical writers that their job is to try to keep up and distill, like, what is important from all the activity and then make sure that's reflected. But even then, it's very, very hard to keep up. But for llms or AI like monitoring all these changes and then, you know, doing an analysis of like how this impacts your kind of current state of knowledge is a much more like straightforward routine operation. So it actually can. And I'm just like, very excited about, you know, what are the implications of that of like when you can actually have knowledge that you trust? And that like things are always up-to-date, I don't know.
 I think it's just like something that is like really built for machines that has only been possible as of recently. Well, it feels like it really low hanging fruit that you probably already do is just giving executive summaries of, here's the changes that happen last week, or on, whatever the Cadence is that the person is looking for, hey, here's everything that you should be in the loop on and you can almost like subscribe to. Okay. I want to know all the stuff that's happening, just so that I can have one eye on that, and make sure if I can be valuable, I can throw in my two cents too.
 Yeah, actually that's an interesting one. We haven't explored Marshalls like the interplay. Like okay, I want to be aware of changes as an expert to know when I should also be involved as well, or as a saboteur and be like, no, we're not doing that. Just total block on everything. But yeah, I think that that is a fascinating piece. Just like, you know, how you have the
 Ability to watch when things happen, right? As there's repos or whatever, like I can just be lurking in the background and recognizing when things are going on. I also, as I was thinking about that,
 I was thinking through how specifically you're going about evals because it feels like you can.
 In a way, get a lot of signal from people. Like if a PR is merged or if there's
 We talked through like all these different.
 Areas that your touching on.
 Which can give you a ton of different signals and ways to evaluate if the agent is correct or it's, it's doing what it needs to do. Have you found that certain
 Signals should be weighted more than others.
 So emails, I mean, always top of mind for us I do think like you're saying we're pretty fortunate in that unlike maybe, you know, Chachi PT where it's very one-on-one interaction because we're operating in public Forums on repositories where PRS are merged. There's usually truth and that helps us get signal as to like how we're doing when we make mistakes something that we've been doing recently that I think is kind of cool and is sort of dog, fooding, our product and how we are doing evals. And so we're sort of calling internally, like, living evals in that. You know, you know, at the end of the day, you know, if we run the agent and someone has a question or an issue, you produce an answer or a document and that question and the answer like is a piece of knowledge. And so, can we like save answers that, you know, from that users have asked
 And then try to text like when our answers on the eval side has gotten out of date and so that way we can kind of keep instead of like having to before what we were doing is we were saving a version of every data set for that specific point in time where someone asked a question and that works, but it's a lot of like operational overhead and wouldn't be nice if that we could actually just save evals as they are today and only when relevant pieces of information changed, do we actually have to update that eval? And so we're very early in this process but I'm like, I'm kind of excited about it because it reduces the friction of us collecting evals like a ton and then we also get signal in the product side as well.
 It's it's a great way to save. It's like it's just reminds me of, you know, like the best Engineers. I know are lazy.
 And this feels like that's like no shade to you. It's like the smart way to do it is just to think like how can I make sure that we don't have to do this all the time?
 How can we just take? It's almost like, are you thinking about it in a way of taking the Delta of? Okay, well, this change. So we need to update our eval set just on that part.
 Yeah exactly. It's like if you think about an answer as like a document or a knowledge artifact, our job should be you know a user saves this question answered pair as like a thing to our knowledge base that is a good eval until you know, that document goes stale. And then can we update that eval or should be prune it from the knowledge base?
 How are you seeing folks want to?
 to deploy this inside of the companies, I imagine you get a lot of data on
 what kind of Integrations they want and
 there's probably like,
 Us the 80/20 principle where? Yeah, we gotta have jira, we gotta have GitHub and we got to have slack. That's probably like the stack that I would imagine majority of folks run with
 but,
 Do you see folks that want to have those who then deployed in their private Cloud because documentation is? It's like the Special Sauce in a way.
 Yes, security is top of mind for us. I mean, we're talking to compliant, but right now, those who is kind of SAS product, you know, go to great things to make sure like hospitals, partition multi-tenant, but and currently don't offer sell posting at least out of the box. I think, you know, we're willing to work with customers on it because of the sensitivity of knowledge.
 On the integration side. I think that's, you know, it is a long tail, the way we think about it is like that. The core of what we want is like, we need code, commits conversations, and tickets, and documentation is like the, the key Integrations. And so Confluence notion jira linear, slack teams Discord, GitHub cover, majority of people. But then there is a really long tail of information that lives in other places that people want access to and something that we've been exploring is, when does it make sense for like a data source to be something that we need a like formal integration with versus something where we can do more like just in time authentication? You know, where, you know, someone like gives do Suey access token on their behalf to go and look at, you know, maybe the logs and this service because or this private internal thing as long as it's
 Like, I was to compliant, so we're trying to figure out, like, maybe there's a way for, you know, first party kind of Integrations but then a long tail, you know, people just authorizing those who to just do just in time access. Yeah, it's funny. How email is not in there at all. Like nobody's figuring out their documentation on email. It's true because email, I mean, maybe it's some companies, but most the companies will work with, like email is, you know, it's a lot more. We really focus on like, the product and Engineering domain, and a lot of those conversations are happening kind of in share channels, or on like PR reviews less. So kind of in more formal back and forth on emails. Yeah, it's it's like email is externally facing and all of the slacks and the discords or teams or whatever it may be are internally facing. And so,
 You wouldn't.
 You wouldn't expect email to be that.
 I'm wanting to ask about.
 it the
 and we don't have to like specifically call out clean, but we can talk about like
 a different ways that
 Knowledge, sprawl happens.
 and we're talking about,
 Just knowledge kind of multiplying.
 And how complex things get, every time you add a new employee, every time you add a new service, every time you add a new feature, that knowledge sprawled, just continues to happen. And it's not like
 Anybody ever comes and says we have less documentation this year than last year, I think that's the same with data nobody ever comes and says, great news last year we only had or we had this data sprawl now, we were managed to get it in order and we don't have as much data this year.
 You've got a vision for
 a flywheel of data that you just talked about.
 I think that there's other folks that are trying to go about it and say, look date,
 Data slash documentation is messy.
 Let's find the best ways to join the mess or give you access to the mess. You're taking a bit of a different approach, so can you walk through that Vision? Yeah. I think you know right now how do we deal with knowledge sprawl or like you know what happens as an organization grows and the answer ends up being something like search which is like we and that's what you see is. People are building better and better tools for sifting through this knowledge and trying to, you know, let people reason about like what is truth or or you know agents to reason about what is truth but I think you know if we kind of look forward into a world where you know you have companies that are started with llms and AI first tools for Knowledge Management. You can imagine where you have a store of knowledge that is like source of truth that grows with you and you don't end up in the same situ
 Vision where, you know, you have like six copies of a similar document, and you're not really sure which one is truth, and you have to go ping. The authors to say, hey, like, did this is this still true? You can actually have a system that is doing that for you in real time like, as you scale. And I think that this is true, you know, at least in our kind of view of the world like this can be true for like product and Engineering knowledge or at least where you have a system of record. So code is like a really really good system of record. And maybe you can extend the analogy to other types of system, records, internally where, you know, they are truth. And you can monitor changes on those and reflect those back into the knowledge base. But unless you have like a digital system of record, it's really hard to reconcile things. So there's still going to be meeting notes and, like other types of documents that might be floating around, but certain types of knowledge, like, you know, how to our product actually work. Like I think there should always be
 A very clear answer.
 Do you?
 all so,
 Think about the difference is between internal and external documentation.
 Yes, it comes up a lot. The I think internal there are some interesting differences today and I don't know how it's going to be in the future. So today, you know, I think the big difference between internal external docs is external. Docs is usually like a representation of your product. It should be very polished as your brand, you know, style tone these little details, really matter versus internal knowledge. It's really about, does it exist? And is it correct? And can I find it? And so, and also like, kind of the topics they cover are different, you know, external docs are very focused on like kind of user-facing. What users want to do with the product internally? You want to know all the messy stuff. Oh, you want to know why? You know, why do we have this hacky code or why can't we handle this integration, you know, or you know, what should I not do to break the product, you know?
 Those things are not going to be pretty publicly facing but are really important for internal collaboration and communication for people to do roadmapping. Effectively
 It in the future. Yeah. Oh, sorry to interrupt but this makes me think about one of those stories that you hear. It's one of the horror stories of
 like an engineer sifting through the documentation is trying to figure out how to optimize or refract or something and gets to this, you know, this this function. It's like, why do we need this function? We do not need this function and there's only one comment above it. That says, do not ever change this function, you will deeply regret it and they're like, whatever they go and they say it actually doesn't do anything. I'm getting rid of it and they delete the whole thing and they come back and they realize they just crash the whole product, whatever. And they recognize that
 it was them that put that comment, five years ago or whatever, and they
 It the real story is even funnier because I think the language they use is very colorful and it's almost like wouldn't it be great if in those comments you can just have a link to the slack conversation or the area that it was decided on in the actual documentation on how that was decided that that happened, or maybe it's a link to the last outage of the person that tried to get rid of that function, whatever. It's just made me think of that. But sorry. I cut you off. No, exactly. I think like, that's the great example of like, you know,
 Machines can have better memory than humans, you know? I, I could totally see myself doing something like that, where, you know, made a change everything broke. And I was like, okay, never do this again. No links in the comments to get planes, buried so far deep, you can't find it and then you just repeat the same thing. And then all the sudden, all the memories come back to you like, oh my God, I can't believe I did that again. And, you know,
 Up with alums like both making the connections for us and helping like think disparate you know conversations or reviews back to code and making that really, really accessible. Hopefully, we can avoid that. I also think there's a piece of interesting like about sort of knowledge continuity there too. Where you know it's that I think examples funny because it's you know, same engineer doing the same thing, like five years later, but more often. It's like that engineer leaves after four years. Yeah, and then there's a lot of Engineers that were looking at that code being like, can we delete it? We don't know like, you know, Sarah left and no one knows what happens if this is deleted and someone's like I'm just gonna try it and then, you know, the cycle continues and I think that as we have better, we make it easier for people to get their knowledge, kind of in the store and that store is kept up to date. I think you can have better sort of continuity as people come and go from companies and communities that can help them like be
 Long or living kind of yeah, over time.