This is what security analysts have always been about or the security team is. They know the truth of the organization, they know what the organization should be doing, what their system should operate. Like, truth is going to be the like commodity of the future in a lot of respects because of how generative AI is attacking and eroding that level of Truth to your point.
 We can start with.
 Give me the rundown on security.
 Right. Oh wow. That's like that's a lot but it's, it's fun. So cybersecurity I guess we look at it linearly of. I like to start off, especially from the AI capacity, it's from signatures. So a signature is, you know, a file hash or a specific Runway. A malware will detonate. And looking at those patterns and signature, for the longest time, you know, is static. But it's been the way and really one of the most efficient ways of identifying vulnerabilities or threats in an, on your computer. So you download a piece of software, it has some small snippet of malware code in and now it's doing something. The fairies, a security researcher would have uncovered that somewhere down the line. Published it to the rest of the security Community, different vendors from like antivirus or firewalls would have picked that up and said, hey anytime, I see this type of signature, this type of identifier
 Flag me. I want to know because this is, this is a representative that this is a real threat, or this could be a threat associated with this. This type of malware, for example,
 And that's really good. Like it's it's the main it's one of the main sources of how we identify threats and clients devices or
 different areas, but
 Where it starts lagging is that you know, they can change malware, they can do a little mutations. So they have this one malware, they change a little bit of code in it and now it's a completely different hash or completely different signature. And so, it's very easy with these
 Static approaches these signature bass approaches to change, how it's happening through like very frequently. So these these signatures are always changing, that's not really good. And so one thing that they did come up with is this concept really early on of like threshold and it's really starting to move from the static approach of data to Dynamic and it's saying, well, you know, let's just use user logins, your user at my company and we work nine-to-five and you always work 95, you're in finance. But if I see anyone logging in like three o'clock,
 What strange to me like that's that's it. So this shows a threshold. So you put a threshold on like the times that people work anytime login happens outside that. You're like, that's strange. It's happening at three o'clock in the morning. I want to know about it, so it's kind of a little bit more Dynamic, right? It's still static. Really in terms of like what we talked about Ai and anomaly detection, but it's starting to move more to that Dynamic way of looking at security data and alerting on threats. That happen in that way. These would be like policies type thing. Exactly. Exactly. These sit certain policies. But based off thresholds, and kind of heuristics that the security individuals know about the organization.
 But say you had an say, in an international organization and people are working all the time, different logins, the Dynamics schedules, well the next Natural, Evolution of that is to like put thresholds and that are Dynamic based on the individual. And that's kind of where we get into profile and anomaly detection.
 So in that, and for a long time, when we talked about AI insecurity it was anomaly detection. Using the Baseline patterns of like traffic authentications user access and profiling it on a system or user basis and anytime we deviated greatly beyond that. So say at 3:00 this user logged in or tried to log in with three that 300 passwords in less than a minute. You're like like that? Yeah, exactly. Maybe I should alert on that. Right? So it has its, it has its like real purpose there because signal you can't really build a signature. I guess, could build a policy saying anything above like, again to that threshold. But this is where anomaly detection has really carved, its strong suit in cybersecurity. And that's what we have for a long time until kind of Jenny I came in. Why did Jenny I destroy that pattern? So like when we're looking at the previous data models coming out of that even only texture just like
 Looking at counts. So I talked about authentication counts. It's a very good data primitive to Baseline, but a lot of the data we deal with is like log data, or these unstructured data points that we're always like, really hard to parse or grab additional context out if and there and there's no normalization scheme. Like, that's another thing that differs that makes it very difficult is that you have one vendor that uses this type of like common log format or this other type that uses elastic on the schema. Or cim from Splunk. They're all using these different normalization, schemes or log formats.
 and so, if you're not having this like strong ETL pipeline,
 To.
 Put them into a normalized thing. It becomes very hard to correlate amongst them, but and then to go around that really cable, we just want to look at IP addresses. So we'll rejects everything out as IP addresses and bring in, but
 You know, it was, it was inconsistent and it was hard to work with and we were really dealing with an NLP problem at some points as we got down the line, like we do the signature bass which was really well, known and robust and it still works really well. We had anomaly detection, but it was only really capturing patterns. And you know, if we if we talk about a large organization, if you wanted a profile, everyone you're trying to profile like thousands of users, hundreds of thousands of systems and the cardinality at that, just kind of explodes and makes it hard to maintain. Plus those systems, create a lot of false positives.
 So I got a circle back a little bit here because you asked me about security and it's like, such a broad question and there's different maturity levels, which you get organizations. So if you're like a new organization and your startup like you're really concerned about putting your product out there, and you're gonna take some risks because you can't afford to have that Enterprise level security. But once you get to a certain level and security, you want to have the best where you want to have a lot of people, you want to have a dedicated security team and at like, the highest material. When you go into any Enterprise, they'll have something called a security operations center, and they'll have a team sitting behind computers ingesting, all that security, while I'm free from firewalls endpoints and they're taking looking at all those alerts anytime alert comes in. They're like, well, I got to investigate this, this could be a real breach, and they risk associate with that, right?
 So once that's coming in and they're seeing that and if they get a lot of false positives, they're just drained if they're stuck down. They're doing a lot of actual work with no real value, it costs the organization and it's not really doing good work. So a lot of time, the fees anomaly, detections that was false positives. It was things that were maybe being too big and not tuned in. And so you had to spend a lot of time, engineering, those thresholds, and those models to make sure they were really tuned in so you're not fatiguing.
 Getting something called alert fatigue and they're fatigue is real man even in yeah in any system right? But I can imagine how in this one.
 Each.
 Whatever. Big.
 Big problem that comes through. You think that this could be the downfall of our system. You you're you're you're exactly right. Like, that's the the attackers only have to be right once.
 To get in the Defenders have to be right every single time. So we take with, it's like that classic.
 A Precision recall F1 score with cancer. It's like we cannot we have that, we have such high tolerances on what these models have to do. And like, how much can be the thresholds there that because if we let in one time and that causes a chain reaction and they've got persistence, they're able to extract data, they're able to enumerate and lateral movement inside the network it. It's it's catastrophic in a lot of capacities especially yeah, the data. So we always got to be looking and investigating but at a certain point like this is where risk comes in an organizational has so much money to dedicate to security or to do these things. So you really have to optimize from an engineering perspective.
 And an operational perspective, these these systems to make sure you're not drowning insecure. Well, of funny thing about this is that my first tech job ever was kind of in the security space and it was a password management tool that we sold to manage service providers. And I remember I was on the sales side of the house and it was so hard to convince someone who really didn't care about security, why they should? Because it's just like, yeah, work kind of flying by the seat of our pants, and we know that we should be doing things better, but we're not and until we get hacked, we don't care.
 Right.
 Right now, that that's where, like, Risk really. It's unfortunate. But that's, and I kind of want to talk about this later when we get into, like, agentic of like, how I think it's gonna transform but you're, you're right, like, the
 The risk. Do you have the crown jewels? Do you are you exposed significantly? Do you carry pii these organizations have to evaluate what their risk level is and you know, maybe they're like, we're willing to risk the entire corporation that we're not going to get hacked, but if we get hacked and we get sued, we're probably pretty much done, but like that's the risk. You took going into that into that, right? So that's like another interesting point on on security that also makes security Ai and ml very difficult is that there is a bias but it's like a personal bias from the organization and their risk. Like you you can develop a model that works for one organization and and their leaders, like yeah, that works. Perfect for me. That means my use cases, but you could go to another leader and they're like, I don't really care about that. That's not my business line, that doesn't matter. So I want to, I want you to tune you this model to be over in this direction. Now, I want you, so it can vary, like there's obviously some core.
 Now is they're like I don't want malware coming into my systems. I don't want, you know, Adidas attempt to happen. So how do I put these controls up? But it isn't, there is a personal. I don't want to say organizational personal kind of perspective that bias is how we deliver security on a especially for me Center because we're delivering security operations for over 2000 customers across the world, right? And they all have different requirements, different businesses, different perspectives on secures, their Security leaders, respect things. So it when we talk about doing them out, it's like yeah we're covering these bases but then we do see these edge cases come in that we go like start tuning in. Like yeah that makes sense. We want to incorporate that you're concerns are legitimate. We can we can accommodate its it ends up becoming to your point.
 How they feel about Security in general? Yeah. Do you find it's a logarithmic curve type of thing where those last
 10% 5% of this risk that you want to take on you need to throw more and more money at it continuously if you want to cover that last little part. So that's the trade-off that folks have to make. It's not just like, okay, let's continue scaling, our security, and continue getting these returns on it. It's more like, ah,
 In order to cover this, we're going to have to really make a commitment and whether that's a commitment in resources or it's money that they have to wait. So Sopranos are really big deal like a big thing that we kind of scope against and insecurity there is this for log analysis or something called long tail analysis where it's one methodology, where, okay, when I look at all my security logs, I want to only look at those events that are like you said very slim at the end because that's typically wearing a tack real hide, right? It's like one off anomalous events like it's called longtail analysis and you just feel like, I want to go over here and look at these ones because those are important ones. But you can, it's very much, a Pareto type of thing. It's like, what do you do to prove security? Well, you know, one big thing is put multi-factor on everything. Let's stop. Like, you don't have, have it tied, a devices, and start doing like, there's some very common base like solid controls that we know about. So if we go into an organization like gets a multi-factor like cape and kind of know where you're maturity value, we're like you're maturity level, so security and we're like
 you're in a game a lot more from doing this, but
 if you have very sensitive data and like a lot of our customers do and need these high levels of assurance and security,
 He got to start investing more because you can't lose once. Basically, all this stuff you have to do when you get sucked, too. Yes. Well, even beyond that though, too. Like talk to you is just like yeah, you want to get to Sock too. And that's like the right now. Everyone's trying to go for sock too but you know, go for those other things. I so peace. PCI. There's so many more compliances and regulations and especially
 In this new market that we're going with like agents, taking a lot more Network traffic up ages. Taking a lot more compute up being a bigger entity with inside. A lot of people's networks going forward as well.
 But there isn't really too much regulation. Coming back from that at the time at the right now, like you know, there's the EU AI act, which is
 Like the best like the the crown jewel of this regulations, but states have kind of Ky Bosch that California for a bit. We don't know where this is going to be. From an AI perspective is actually an agentic for security and when those regulations come in to your point, they'll have to put more controls and start going down that to fortify themselves on that.
 Yeah, like maybe let's talk a bit about that with
 the evolution of agents and how that has changed.
 What security looks like, because I know there's both sides of it, when I think about agenda. Use cases for security. I think about the
 agents and gen AI just being able to
 Get you involved.
 or when I think about agents and Jen, AI, I think about how
 There's now many more threat vectors.
 If we're using agents and my favorite one of these is I read a paper talking about the indirect prompt injection. Yes. Agent goes, and it's scrapes a website and then it gets prompted. So it can be someone is trying to use an agent normally like a good.
 Uphold Citizen and they still get screwed because the agent goes out and it scrapes a toxic website and gets prompted injection. So, we're opening up these new vectors that we can be attacked on. But then we can also use agents to help us on the different things. Like you were mentioning earlier, like inspect logs or go and take action from different logs. If we see some specific or suspicious, we can now use an agent to hopefully try and patch things up quickly.
 Can you just talk a bit about your scope and what you've been seeing with agents out there in the wild in the security field? Absolutely. So
 When this first came out, we kind of understood how much Game Changer change this was going to happen and like, be used and also be used poorly with, inside the Enterprise, like, it came out so fast and everyone was adopting a very quickly and there wasn't much security control around and not much understanding. And we've been seeing the evolution of this field goal so fast. And even from the security in academics, I were like this week just can't keep up. There's not enough time to do.
 Solid, traditional foundational security research out of the Academia Labs. I'm talking about more, not so much of the corporate Labs, but to, to keep up with the rate of which, this is progressing. So we saw this effect coming in and
 To answer your question when to talk about three, essentially different buckets here. So we have the Jenny, I for the Defenders, we have the Jenny, I of How It's been used by the red team and then we have the vulnerabilities that Jenny is introducing to kind of, like point this indirect prompt injection attack. So when we knew this was happening early on, we spun on a lab called the center Labs, that is focused on something called MDR for Jenny. I so manage action response, but focusing on these new threat vectors that are coming out, like you said, indirect proct, injection attack. And when we're doing a research, the reason that's happening is because
 These agents.
 For a lot of time are not being exposed, their logs and their thinking patterns that are changed to Downstream service customers. So they kind of work, you know, inside that black box but even though you're what you can, you can get traces. If they're if these service providers that are leveraging, these services that are prone to prompt injection, tax are not actually keeping traces of what those chains of thoughts are are what those law that Services doing. Then you have no visibility. You can't actually see where this indirect vulnerabilities coming from and addition. These agents are kind of gaining more and more Authority all the time and autonomy so we're like oh the they did this thing and we get this is doing better than this person did previously or this this system proves. That's it. And we're giving it the authority to go off and be more but without the oversight and it's a dangerous situation of kind of criticality. So why what we saw was like Paul if you're gonna implement
 And any agentic or journey. I systems like you need to First have something called an LM Gateway. And we released one early on which is essentially a proxy that exists between your organization or device and these Downstream service providers. Like it could be a model or it could be a service provider that leverages an open source or commercial model that does some type of a gigantic or or manipulation on top of those prompts. But we did a Gateway. We say every prompt that goes through we want to record it and we want to record the responses and there's a lot of power and just seeing that visibility. But what you don't get is what the agent and that agent exist system is actually doing because that's still stuck on the service provider's side. If you're not getting those logs as well brought back
 So to that point that autonomy, and that lack of visibility is the most significant threat that right now, in my opinion, because
 How can you protect against something? You can't see?
 And what's even kind of worse? Is that a lot of Ip is tied up in these agentic systems with how they think. So, if you have a lang chain or something like that, you're like, this is the core IP of my organization. We're doing something unique versus everyone else. But if it's able to be prone to prompt injection attack, you don't have strong guard rails. You have done the significant testing before being released because you've been more focused on production and not so focused on.
 You know, security requirements again going back to that risk and maturity.
 conversation, we have
 It it's prone to happen a lot more and we're seeing a lot of that happen commensal. We're really
 All the development, we've done it. We've, we've been lucky to be at the Forefront. I really am fortunate to work with a lot of smart people. A lot of people that take security, very strictly and we've always been kind of been like, yeah, we need visibility, we need systems. We need rigor, we need to see what's going on, and luckily we've been staying ahead of these threats, so that's like the one bucket there of
 Threats and more attacks at these agenda, systems are getting and I think it's only going to increase because we're constantly giving them the more Authority. And we're using the more and more. It's going to become commonplace. So it's just makes sense that this will
 proliferate into a more larger and larger problem, which isn't is
 Detrimental to those analysts that have to look at the at those security events. So that's why we need that, Jenny AI. That's why we've also been working on the Jenny, I for The Blue Team. Be like they have all this Arc, they have this new attacks service now. They're using Jenny, I just like we're using it. The red team is and coming up and using it all forms of the kill chain. They're developing new males with it, they're using it for fraud a lot more.
 How do you keep up with this? Increasing tsunami of gen, AI attacks and vulnerabilities? Well, we have to start developing systems on the blue side to combat that as well, Empower analysts to, to respond faster in at scale.
 And then there's the Third.
 Bucket. Which is, oh, so that's over top of the first Buckle was the vulnerabilities that the LM systems and the Gionee systems give by themselves. There's that bucket. But I was I briefly talked about the third bucket being the
 threats like how the attackers are using janitor they I
 To inform their attacks. I'm not sure if you saw this paper that came out from and Throop in August, but they're threat intelligence research team.
 put it on an awesome paper because there they captured real case studies of different attackers, leveraging clogged code to, you know, they saw that cloud code was being used in every form of the kill chain, which is amazing because it's able to assess the attackers, you know, previously where they didn't have skills or they need, they got stuck, they could like unblock themselves down
 They found they found a person that was using Cloud code to develop a ransomware as a service that was like completely like beyond their skill level just looking at the props because they're following this prompt change beyond their skill level looking at their md5 and they put out there and be files and show. This is kind of like a boiler plate of what they're doing. So keep an eye out for it and then the saw which I think is the funniest
 the North Korean employee, hire fraud scheme, like
 To get to this point. I talked about this at a high level, I'm glad you say this because
 I I this is kind of been in my opinion, too. Is that generative AI? Its entire objective function, is to be indistinguishable,
 From Human generated content. So the by
 by it's like own latent. It's it's meant to be
 Deceptive. And I'm for and because it's deceptive it lends itself to deceptive practices better.
 And so, when we talk about that and do your point, what your Trump, what we really need to go for in the future and this is what security analysts have always been about or this security team is. They know the truth of the organization, they know what the organization should be doing, what their system should operate. Like, truth is going to be the like commodity of the future and a lot of respects because of how generative AI is attacking and eroding that level of Truth to your point.
 It is so wild, man. So yeah, there's there's very new attack vectors that are coming out whether it is from the agents or it is the social engineering. I know that back when I was at that.
 Security company.
 The biggest threat to security was always humans and it was human mistakes.
 they're making this they're making like, I couldn't believe when I read this because this paper
 From a security because again, a lot of the time I spend, I kind of have two foot roll. I spent time in Academia. I'm supervising graduates in the security and AI space, and and we're academically comes out with a lot of, like, novel vectors. Like, we knew about indirect prompt rejection, we knew about these security vulnerabilities and llms we know about a lot more coming in. But what's really hard is to catch these in the wild,
 when we look at traditional security research,
 Its.
 You know, he found this, we found this in the wild, we found the this signature in the wild, we build a report on a republishing and goes out. But then for this ml stuff, it's not like it appears itself in the wild because the visibility is not there, it's not really well known. So we come up with these vulnerabilities, we come all these attack factors. We publish them, you know, and we say the under this threat model, this is what could happen.
 But they kind of diet they typically died off for the most part because you can't find them. But after seeing, you know, a bunch of new papers and reports of indirect prompt, injection, attacks happening seeing hugging face models poisoned by attackers. So data, scientists are now being attacked more. And then now with Claude finally in these in these large llm community sort of model providers saying this is the attackers and then a various ways we've been seeing our model deployed. You're like okay, yes. So finally everything, we've been saying it is being used is being here. Here's some hard proof, and I love that paper so much because it's like bringing to the Forefront. These really unique attack ideas, we are new. But you don't get this. You don't get a sense of how how they're being used or North Korean won their their, some of the users couldn't even create a git repo, they had to ask how to create a git repo.
 and,
 It's, it's amazing that, that persists, in my opinion, and we have. Yeah, 100%. It's like there's there's a time delay between we see when we learn about something, or when we hypothetically things something is possible. And then, because we're, we have a little bit of that shadow game going on and we don't get the data, the anthropic, or these big Labs have, and we can't see until they publish something. That says, hey, yes, people are doing this, and this is how they're doing it.
 That's right. That's right.
 What do you think?
 the future looks like here for
 Where we're going? How we're going?
 I got to talk about The Blue Team, a little bit here and give some give some context on this because
 There is a new push right now. A gentec has been doing.
 Really interesting stuff we've been seeing all these use cases. They're like okay LMS were you know, monoliths working by themselves able to do things. But you get these specific agents that are working in concert together and they're able to do a lot more power, right. And we're seeing that all. So on the investigation side so going back to the sock because that's where you have these really skilled highly qualified personnel.
 Sitting behind computers investigating this. So you know, this malware detonated or this this Powershell command went off. What does that Powershell command mean? What did it do? Did it infect any other computers? They're looking around the network to to discern this and it's a, you know, a pretty skilled person.
 To do that. There's obviously tier one, tier two and tier three sock analysts. But one thing that they're doing a lot of time, just like with any investigation is collecting evidence building that profile and then at the end they compile a report and send it out to the customer being like hey this happened, but it wasn't a real. It wasn't, it was a false positive and this is why I give you the report or this happened. It was a true positive. This is what we did to kill the connection or quarantine the host and this is, and this is the report and this which need to do next. All that takes considerable amount of time.
 And that's called meantime to detect and meantime to respond and one year, seeing a flood of more signals, come in larger attacks service.
 We're seeing the economies like not scale like Cyprus security. Just like, most wars are a sign of attrition and economy where it's like, we make it too hard for an attacker to attack a system or we don't have valuable data. In the first place, we're going to have a less risk of like being attacked. They either like, that's too much work. I'm trying to different attack vector or they don't have anything, so I'm gonna attack them. That's got no value.
 This. But this economy with this is going is
 Our analysts are drowning.
 All the time. They're getting that alert fatigue they're getting bombarded, we need to scale up and it comes as human generated problem. So what we want to do and what we are doing is leveraging these AI systems to front load some of that work, that evidence collection we can go out and do these automated, lookup this. IP it came from this signal, it came from sorry, this
 Threat came from this IP go look up the reputation as anyone else seen this? Has this ever been seen any other where in the network, so it's able to front load a lot of this. So I want to come to the analyst. We're now giving them a report in their hand being like, this is the work we did up to this point. The agent did up with this point.
 Use your expertise review it, make sure it's good and then help and then if we're if it's missing anything they can go off and look at any other, you know, dots, they need to connect to drug conclusion, but we're greatly trying to reduce that mean time to detect a mean time to respond to account for this massive influx we're seeing from Jenny Ai and the acceleration is doing from the red from the attackers.
 It's fascinating to me how much of a parallel there is between this and what a friend of mine Willem at cleric is doing for root cause analysis, when your systems go out and you all are speaking. Basically the same language just for two separate use cases, you're on the security side saying, hey let's go and figure out what is happening and do as much of that wrote work as possible because you don't want to have to switch between systems, you don't want to have to try and look up, it did this IP, what's going on here? Is it nefarious? Is it not? You would love to have that type of information given to you where something or someone else has done it and then you get to scan it and say, oh, here's what looks fishy. Or oh, no, this seems all right. And in that same vein in the devops fear Willems, trying to do that and he's trying to say hey let's if we see that there is some kind of outage, let's go and scan the data dog logs and let's
 Scan. Let's look at slack for any kind of messages that the sres have been pinging back and forth and maybe there's a new policy or there's a new, something, a new role out that happened. That could be causing this issue and these are like,
 Both ways that agents are helping that kind of menial work that just you have to do but nobody enjoys doing. Yes, exactly. And to it to his use case and similar, it's about reducing that time. So root cause analysis to reduce the time that outage is actually happening because it has a like Direct business value affect right? So vulnerability comes in sorry, threat comes in.
 Let's investigate and figure out. If this thing is actually going to bring down the system, this is actually legit threat and discern on it. So they're I see the parallels are saying and it is exactly right. But using the the agenda systems to bring together correlate this data help draw some, maybe simple conclusions. So to that, to his point, too. He's are they comfortable letting the agentic system or any ice system? Make a decision on a remediation action.
 Like that's a question. I I'd be interested to hear from them because from a security standpoint it's not there yet. There is a big push especially coming off of RSA this year where it was a Genting this and a Genting that. And there's new startups coming out that are just being about pure agent socks like humanist socks and that doesn't sit well, like, you can't put that liability and especially with these complex it systems.
 That requires such deep level expertise.
 To be able to bring this and give that power to an identity. Like, again, giving that Authority. Are you ready to give that authority to change? The system is just not there you. You need human in the loop especially for these high criticality systems and it feels like the agents are already so vulnerable, You're just adding another vulnerability. If you're letting the agent, make the decisions on that. I think for Willems he is but it's more.
 on the
 Event of. Hey, here's what I found. Do you want me to go and fix it?
 And that's when they'll go out. And so you get that human acknowledgment of, here's what I think it is.
 I think I can fix it by doing these things and then, boom, go for it. If I say yeah that seems looks good to me even though I do notice and this is a fascinating like human field of study that with my own use.
 Whenever I get a suggestion, I just kind of blindly accept it and say, well, let's try it, so that's not good. If you're dealing with these very high risk, use cases and it's in the security field or it's in like we're talking about an outage because it's just so cheap for me to say. Yeah. Like click a button and say yeah go do it and cheap as in I don't have to use any mental power. Cheap is the perfect word and I and this is the
 So to the point of like we don't want to give it Authority there, we don't want to get at that point.
 This is going to DeMark these security, or in this point to your point. This this devops approach where say you have a team of one person that has to look after a budding organization. That is like a startup that's trying to do something. I can't afford a massive security sock, but this thing is cheap, it can give me something, it can do. It's like another layer of Defense. It might not be right all the time but it's it's right 80% of the time. It's able to do something. Is it an acceptable level of risk for the cost that you're talking? I want it's it is two side. So I see you know I think for
 The, the security domain. And if you talk to any security professional, that's very diligent and serious about security. They're like, no humans. They need to remain. We own expertise. We've seen the agents fail to many times, but for that person that says, I only have this small security budget. I have nothing to work with. I need to bring data. I need more security. I'm underwater. I'm drowning here.
 That's it, that's it. That's a viable alternative to de-risk, the mass of risk, you have of having nothing kind of thing, right? So to your point cheap was the perfect a point because it's descaled this. But, you know, if can you afford and that kind of goes back to the risk, part risk. Really pays it is, it is the risk worth the reward in this capacity of cost savings.