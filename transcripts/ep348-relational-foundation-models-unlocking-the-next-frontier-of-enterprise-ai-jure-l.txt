The people don't even realize.
 Or we seem to have forgotten how valuable structured data is because it's kind of the ground truth of the business. And it's it, it captures the blueprint, the of the business and we seem to have forgotten all that and, you know, we are now excited about answering questions about documents.
 This is a bold claim being able to create.
 Machine, learning, models, faster, and better. What is this break that down for me a little bit more?
 Um, yes, I can I can I can give you an I can give you an explanation, right? If you think about how we are building, that it is predictive machine learning models today, right? This this models would be, let's say, churn models recommended systems, any kind of risk scoring models fraud models, across all kinds of Industries. You know, from from hospitals to social media, to manufacturing to supply chain optimization. And so, on the way we are doing this is right that fundamentally this models, use the internal data that is usually structured in tabular form, right? It's sits in some database in some data warehouse. It usually has this relational structure, you have multiple tables, interlinked with primary foreign key relationships, right? So it would be a user table, a product catalog and record of all the orders of these users may be the table that includes all the website Behavior. So
 All the cliques that says this user clicked on this webpage at this time. And so on, right? And we want exactly exactly. Right. So this should be the data and the way we build this models, this models today is that we would be joining this tables generate features and then training, you know, our favorite neural network or whatever it is on these data.
 Um, and I can go deeper what what I mean by by all these. But basically takes a side note, several months, to build this models, we need to engineer the features features need to be up-to-date that they must not be stale. There is this time travel issue, we then put this models. Let's say in production to actually run more than just in our private notebook, and that's another kind of forms and so on. But if you look at what's kind of been the trend in AI, let's call it this way, right? The trend in AI has been. Let's learn on all data.
 Okay. So in big revolution in computer vision was, let's not feature engineer from the pixels in the image. Let's train a neural network directly on the pixels, okay? And we get this super great neural networks for computer vision. Totally Revolution. Now we are in the age of large language models, right? In the old days when we did. Let's say natural language processing, natural language understanding. Even if you think, when you know, IBM won Jeopardy, the competition, it took 300 people, two years, and a bunch of very careful feature engineering. So that computer was able to answer Jeopardy questions and be better than humans. So, in some sense, we had AI at that time, but it was just kind of handing engineered, right today. Transformers kind of day, just learn over sequels of tokens. And it's kind of, they learn overall data, so what the interesting thing is to ask. Okay? So it's like the world of machine learning, let's say predictive modeling has been
 Stuck in the past, we don't learn on the road data. We learn on this feature, rise data.
 So what I'm what I was very excited about was to say, okay, how could we change this? How can we develop methods that can learn directly on road data?
 Um, and the hard part here is that your your data in machine learning does not come from a single table, it comes from multiple tables, right? So, and it's usually very messy too, right? It's exactly.
 Pristine data. That gives you a clear story. It's not it's not pristine. It has missing information and all that, right? So what we have developed is a set of neural networks and I can talk about them. They are based on they are basically based on graph representation of the data that allow you to learn directly on the road database data. So they allow you for the first time to learn over multiple tables at once.
 and now, because of that,
 You completely cite step, the featuring Engineering Process.
 Which means you save humongous amount of time.
 And you and compute, maybe not so much actually you know, gpus because this spots are now a bit bigger. So so you know you need to train on gpus but you know the computer works when when you sleep. So it's it's very fine. But there is another benefit which is because now the neurons are learning how to combine the data.
 The Fidelity, the Nuance of those learned signals each much, much higher so your models get more accurate.
 Uh-huh. So now
 I've heard folks talk about recommender systems with llms and just basically the inherent understanding that LMS have you can suggest
 Certain things to an llm and it's going to be able to have a better view of what a user will want.
 Is this kind of what you're talking about or is this a whole separate thing?
 I I would say it's a separate thing, right? Like if you want through llms with for recommendations, it kind of
 Works. Okay, it's not super Teddy Ball, but it's not good either because it's all kind of base on some kind of common sense. It's not really learned for the date of your specific users for your specific understanding and so on and I can give you a simple story. For example, I was Chief scientist with Pinterest for six years, right? Took Pinterest from 100 people to post IPO and I built recommended system platform there. Three generations. And what was for example, a super hard problem with Pinterest is that, as you see these images, maybe you have an image of the Dragon, the floor, and you have an image of a tapestry on the wall.
 They practically look the same, the neural network could not tell the two apart, but humans could, and the behavior on those things were was very, very different, right? So at Pinterest, we very quickly realized, it's not just enough to treat a user as a sequence of, let's say a sequence of images they visit because the neural network would get confused what you need to think of. It is a is a, basically, you need to think of Pinterest as a set of interactions of users with content and think of that as a graph. So if you say, oh if I if I interact with this image and then other people interacted with it as well. And you know, then I kind of know what this image is about, right? This this tapestry can say oh I'm interacted with other tapestries so I'm a tapestry, I'm not I'm not a dragon, the floor and Aragon the floor says oh when people click on me they also explore other things that very much look like rugs. So I know myself, right? I'm not a I'm not a tapestry, I am a rock, right? Like
 Sorry, it's good by association. Yeah.
 Exactly, exactly. And this is what this kind of graph based approaches allow it to do because they allow you to kind of learn from this structured relational. Relational information. And this is what sequence sequence models sequence models will miss
 and,
 if I'm understanding this correctly, for example, when you're doing the
 Traditional machine learning models.
 You're really optimizing for one or two things.
 And with what you're proposing now and what you've built it does it have a more holistic view? Like if we have the two extremes where you have traditional ml models and then the other extreme is a large language model
 Where do you fall on that Spectrum?
 that is a good question, so,
 The approach we developed, it's called the relational deep learning, and it can take any database represented as a graph of relations between the entities in your database. And I will just learn over that graph of relationships. And if you now train a Transformer, like architecture that is called a relational graph Transformer, that does not attend kind of over the tokens, like a typical Transformer, but attends over the tables of that of that, schema of the database, you can, you can do both, you can train small tasks specific models, kind of, you can think of them as, you know, kind of classical machine learning models, but now really learned that directly from the road database data, but you can also learn large pre-trained model. That is good at any task on any database. So you write, like, if you go to change PPT and ask it, hey, do time series forecasting for me or you go and say hey I have this transaction, How likely it
 Is to be fraud.
 You, you will get terrible results, right? How would you know the likelihood of a particular transaction being fraught, it won't right? But the large pre-trained relational graph Transformer, which we call a relational Foundation model allows you now to ask this type of questions without any model training, so maybe the point is the same way as in
 Let's say, in biology, we accept that. We need a DNA Foundation model, we accept that. We need a protein foundation wall because protein is not natural language. I think the key thing we need to accept is that for structured data, we need Foundation models, trained and built for structured data. We cannot testify a database through that into an llm and, and hope it will work because it does not.
 Okay. So, on Stern, understand a bit more of the picture of what you build. And now, can we get into the Nitty Gritty of how this actually is build and what you're doing? Do I just
 Throw in my database and then do, I need a whole ton of data? Like what? What is this look like in practice from how I how I interact with their or how I train a model that is now going to be one of these types of models,
 Yeah, so the way, the way you need to do is you you need to, let's say first select the tables, the schema you want to learn over.
 Usually, you know it's five to fifty tables and what you need to do, you need to specify. What are the primary foreign key relations between this tables, right? That user ID in this table is user ID in that other table and you select kind of the semantic types of columns. So there is a bit of, let's say, data registration, data modeling, even you can call it the data preparation step. But it's it's very small because you just need to say. This is my schema. Here are the relationship.
 And after that is done, you have two options.
 One option is that you use now the large pre-trained model that you just point to this collection of tables, you prompt it with a specific predictive question and half a second later, you will get an answer that is on average as accurate as manually build model that takes I know a month to build.
 what you can also do is, then to say, okay, I am going to
 find my model for a specific task and for a specific database.
 And then you can find kyun and that will get you to superhuman superhuman performance, right? Because now, you have specialized the model for specific database for a specific task. And of course, if you are, for example, doing fraud detection, which is a high value tasks, where you truly care about, you know, the last bit of performance because it's saving you so much. Let's say money, then you want to train a task specific model?
 If you are in a more of a regime where you want to do, basically owns like ad hoc querying.
 Ad hoc predictive querying, you would use a large pre-train model to do that.
 And only tax specific models. Do you see folks creating an ensemble of models just like you would maybe in
 Traditional fraud, detection, use cases.
 Not not really, not really what? What we basically see is people training a single a single model that then learns. How to attend over all these. Let's say different data, from transactions, to locations to time and all that and combine that into the best possible predictive signal, right? That's the difference is, if you think about maybe a fraud model in a fraud model, you say okay I have I have a customer. How many transactions did they do last week and you create a feature and then somebody, you know, some other data scientists wake up and says, it's not number of transactions. Last week is the number of transactions in the last, you know, 10 days. Great, let's add one more feature. Then somebody else wakes up and says, no, no, I I know the answer. It's how much you spend in the morning and I Define morning as I know six to nine a.m. and let's add that counter there. And then somebody else wakes up and says, hey, but there is daylight saving time, we just change the clock in us, okay. So you know,
 In the more in the summer. This is The Mooring in the winter, that's the morning. But you see my point kind of the point is that it gets kind of? We are Computing. This arbitrary Statistics over the data.
 To, to be able to say, okay, how much did this person purchase in the last time, period? But if you have the attention mechanism, the attention mechanism can attend over each individual transaction so it can learn to combine them in ways that
 humans will never never will, and that's why it's able to kind of extract more signal from the raw data, then, you know, a SQL query, or a manually defined feature will
 Because the neurons can do so much more and they are basically trained to combine the data together.
 If that makes sense. Yeah it's basically like move 37 in the go and how, you know in go how was it was moved 37, right. I think it is exactly, exactly. It's maybe something like this, it can be counting intuitive, it can be something that is very fine-grained. It could be some, you know, second order correlation that you as a data scientist. As a machine learning engineer. Never put into a feature and things like that, and because of that,
 You know, it's I would say the same intuition. As in computer vision, writing computer vision. We are like, oh here's the edge. Here's the gradient of color, nobody's doing that anymore. Its just the neural network, figures it out. How to combine all the pixels to say, what's in the image? And and basically, we do the same, right? We take the database.
 Represent it as a graph and then the Transformer architecture, the graph Transformer architecture, learns how to combine all these species of information into an accurate prediction rather than, you know, what is done today, with machine learning, which is you manually, figuring out how to combine transactions into something that allows you to predict something about the user.
 yes, so your
 Wiping away all of that feature engineering.
 What are US data? Scientists machine learning Engineers gonna do now
 That's a great question. I think it's exciting. I think it's good because you're not a feature engineering machine.
 Right. It's almost like saying, oh you brought me a robot that helps me sweep, but I, you know, I want to be sweeping the floor all the time. No. Right. Like you, you are here to, to build models to, to have impact on the business, you are not here to clean data and feature engineer, right? Those are the two most boring parts of your job. So what we see for example is that these three kind of opens the data scientists to be able to build models, faster to refocus on modeling to focus on watch. It will be even predicting right. You never in in let's say, in business, in Enterprise, you never come. There is never a predictive problem that is given to you. It's a business problem that is giving to you. So how do I go from a business problem to a predictive problem? Asking yourself, what is even predictable? What is module from my data, right? Because, you know, garbage in garbage out. So what we see is then because now the let's say the feature engineering step,
 You don't need to worry about, you are much more worrying about. What's the raw data? How is the rotate a structure? What is the information flow over the raw data? And so on, right? And and maybe let me just say this because I get asked many times people say oh, is this Auto ml?
 Uh, right, like remember this old Auto ml promise, right? It's what was Auto ml, right? Auto ml is you run a gazillion of
 SQL queries against your database. You join every table with every table, you aggregate everything, seven different ways. You create this humongous silly feature vectors, and then you train a bunch of models and hope it will work.
 Right. So it's like kind of throwing spaghetti against the wall and hoping that something sticks
 What I'm talking about is fundamentally different.
 It's a single neural network that has the ability to learn to attend over the collection of tables and you are you are now training or tuning, this neural network on that collection of tables so it's faster. It scales to, you know, to the to the largest use cases in the world. I can say more about that later.
 And it gives good performance.
 Well.
 One thing that is very clear after hearing, this is the value of that raw data as if we didn't know that already right? But
 How?
 Much, can you?
 Have messy, ugly, shity data and still get value from this method basically.
 That's a great question. So here is what I would say. Um,
 in something's quality of the data of the data matters, but because this models are learning from the entire relational structure.
 They can implicitly impute, missing labels, they can correct. They can correct for mistyped or misunderstood information, even if the linking information is a bit noisy, sometimes the moles can recover from it because, you know, they're kind of learning holistically across the entire relational structure. So, another place for example, that this is important, if you think about cold start problems, right, calls that in a sense that let's say, maybe in the recommend their systems and so on, or you have a new user, you have a new item, you haven't seen anything about it but then you have an old user or an online item that a lot of others have already interacted with. So you are very
 data Rich information about this item.
 What is the benefit of this models? Is that they can kind of trade off?
 If the item is new, they will focus on the attributes of the item properties of the item. I know description image and things of that, and they latch on that. But as soon as that item starts getting connected into the graph, by users, interacting it, buying it, and so on. Now the model is starting to use this relational structure. You know what? You kind of said earlier like the guild by association to say okay who is interacting with this? What else are these people doing? Let me give an accurate recommendation. So it allows a kind of work, the models, learn how to trade out between learning from attributes to learn and learning from structure, just it leads to very robust models. That's all I wanted to say both, in terms of missing data data ugliness as well as robustness,
 Do we need to have?
 Super clear ontologies and Knowledge Graph setup or is that something that it infers also.
 A good question. I would say somewhere in between
 All we need to know is what table links to what table?
 Many times we can in 30s automatically from column names.
 And that's about all we need. So we don't need. We don't need the model. Does not rely on semantic information.
 Right? The model, the model relies
 On patterns in the data.
 So maybe here is the, the difference, is that the model learns, how to recognize recognize the patterns in the data to make those predictions. So we don't need a super rich semantic model. We don't need to explain what is the true meaning of every column to The Last Detail? Because from the kind of the patterns from the past that predict the future that doesn't matter so much.
 So talk to me about scale.
 Scale, good questions. So this this scale to
 1000 billion billion notes. So for example I can I can tell you maybe to just to illustrate I can tell you some
 Examples of how this technology runs in practice and who's using it.
 Okay. So, um, one interesting story was, you know, a few years back, doordash came to us, right? The doordash during covid was growing great. Alright. Everyone was ordering food, but post-covid kind of, right? So, so they, they came to us and said, hey, can you help can this technology help us? So we looked at restaurant recommendations, right? So recommend restaurant, you are going to order from next. In particular. Recommended problem is order recommend the restaurant, you never ordered from before so it's try something new. And this the way this gets a surface to you is through notifications, right? You get a notification saying, hey would you do you think you want to order from this restaurant today, right? Then and this is one of the core problems, adore this, right? So they've been kind of using traditional technology to build this type of systems with the best possible people and so on,
 we train our
 Our Transformer model over a collection of tables. It was a 30% increase in accuracy 3-0.
 Wow, so me as a user.
 when you suggest a restaurant, for me to try,
 It's much more pointed and it's much more to my liking.
 Exactly. It was much more pointed, it was much more to my liking and that resulted in
 Several hundreds of millions, of more purchases or orders on on doordash, right. So it was humongous business impact.
 Right. So that's just one example right now is a lot of scale of doordash is I know several hundred million users. I know hundreds you know billion orders all the website behavior, all the searches all the geographical location or the cuisine information so it gets quite quite quite interesting as well. That's for example, one example where we saw like this humongous lifting performance over a flagship model right? Something that is not one day does woke up in and said okay he's do better. No, it's like a team. Several years of effort, it can do better. I can give you another example. That's even larger scale and these go this is in advertising. Oh before wait, sorry. Yeah.
 On that doordash one.
 How?
 Long, did it take to go from inception to production with that one benefit of this model is because they run on the road data.
 You putting them in production just means refresh the raw data. Right? So so for people who have put models in production like just having that feature store having up to date features, making sure features are not stale, production them. Babysitting those work workflows is is like, you know babysitting a 2 year old kind of you know tantrum sold the time right like on call all the time. So so it's very hard right? But here it's much easier because you just say, here's my French data make predictions on it, you refresh the data make predictions on it. So we put that in production, I would say quite quickly, you know, maybe a few weeks, and of course, and that was a set of
 its irrespective, if
 The data, schema changes.
 as long as you refresh the data, even if the data, schema changes, you're still able to
 infer the right thing.
 Even the data. So the models themselves allow for data schema changes, so it shouldn't be a problem, but if you, let's say drastically change. The schema change, the composition of tables. You would, let's say, a retrain, the model that takes, maybe a couple of hours and now we have. And now you're back in the game.
 Yeah, right so so that's that's one benefit. Another benefit is, if you think about user Behavior changes, all the time, right? Spare? Or if you think about fraud, right? Fraudsters are always inventing new ways to to commit fraud and you as a or we as data scientists, we are always a step behind because we are like oh my all performance is deteriorating. I need new features but we did approach. You're just let me retrain the model and the model will figure out the new the new signal and and you know and and and and so on. So it's all so much easier to keep up to date. You can automatically retrain and really get the most value out of the data. You have.
 Okay, it makes a lot of sense. So yeah, what's the other scale? The massive scale one that's even bigger than doordash what's bigger than Doris? I can talk about this one. It's advertising models, right? So this is the bread and butter of the internet industry, right? It's like predicting How likely is a user to click on an ad? Because you know every time you visit a website, there is a prediction for this user. What is the most likely that they are going to click on that adds? Get put in front of you if you click it The Advertiser gets paid. So no click no money. Click you get money, right? So 1% lifting accuracy of this predictive model means 1% lift in your Revenue.
 So, the use case here is ready.
 And ready, it has advertising models or running on their website and you know, it's bread and butter of what they do and every you know single or tenth of a percentage Point matters.
 A lot in terms of absolute revenues. So there
 I can say they they usually increase the accuracy of the model by one to two percent year over year.
 Up and with this approach was like five years worth of improvement.
 Inaccuracy within a month or something, right? Again, over these Flagship model that's been tuned and, you know, the latest from research incorporated and so on. And I think this just shows kind of the power of this, just let the neural network, learn over the data, right? The trick is the data is split or sits in multiple tables. So you need now a generalized Transformer, architecture that can attend across the tables and learn how to extract the signal automatically rather than us humans. Doing it manually.
 Yeah, I was talking my friend said it and drop in, he was saying that they have a saying internally which is let the model cook.
 Basically, basically exactly exactly. Exactly. And and I think my point is for
 Natural lake which human like tasks llms are great.
 But llms totally fail on structured data, right? You cannot take a database and Jason, is it? Put it in a Big Blob of Json.
 Put that as a prompt and say you know, now based on these give me a prediction. What will the user like
 Do you see this working for time series? Also,
 I see this working for time series a lot as well. Exactly. There are two ways to think of Time series, right? One is just to say, oh, I have an individual time series. I have a sequence, I and I predict the next token. So people have been I would say quite successful training sequence based Transformers on time series data. Our approach is a bit different because
 The way we think of it is, it's not one time series, it's a time. It's a graph of Time series because this time series are usually connected, you know, so maybe you have sales record for one for one product, but products are related to each other or maybe they are still, I know so different stores and things like that. So, by representing this time series, as a graph, you actually get a further increase in performance because the model does not only learn how to how to make prediction from the single time series, but also letters, how to attend across other related time, series, to better forecast, right? Because something time series might have different legs, some products might be correlated, some stocks might be correlated and things like that, right? So predicting from a single time, series is very hard because you don't have that information. But through this, let's say graph. Based approach. You can borrow learn to borrowing for information from other related time series, which again, leads to the increased performance increase. Feels like you're just giving a much richer picture.
 For the model to understand.
 That is a great way. I think that is a great way to say, right? You give a much richer set of information to the model and then you give the neural network, the freedom to combine this information in in the best possible way for that forecasts that prediction that risk score.
 Another thing that I'm thinking about is,
 does it take a team?
 To.
 Make this happen or is this something that?
 Is like, how many people? How many resources, if I want to try and do this at my company tomorrow? What am I looking at?
 Actually like needing to dedicate towards this.
 That's a great question. Um what we see is that we did this, you need about 20x fewer resources, then traditional approach. So it means you can do much more Vida, much more leaner team. You still lets say they need
 A data scientist or someone who kind of understands predictive modeling but that person can build, you know, 10 models in a single day if they like to do to do so. And then what you also need, I think with predictions It's always important that you are able to plug them in into whatever product, whatever decision making, you are doing, right? So predictions are only useful when you are making decisions when you are making staking some action based on that decision. So, we need some engineering research that says, okay, now that we have prediction here is what we are going to do it.
 yeah, it's still the
 model building and putting the model into production. It's just that the model building piece has been drastically cut.
 Exactly or exactly the the model the time to build. A single model has been cut, which means as a data scientist is to have much more time to explore to, to kind of really think about how am I modeling this business problem. What is the best way to model it? What is the most accurate way to model it? And allows you to kind of to explore that space, much more efficiently and faster than we? Let's say traditional feature engineering, that takes so much time.
 yeah, and I can't stop thinking about how
 To.
 Of the main feature stores have recently gotten bought or acquired for, you know, like tecton got bought by databricks recently. And then I just heard feature form got bought by redis and so
 It feels like that whole paradigm.
 Was it maybe a dead end if we can do things faster this way, why wouldn't we?
 That's a good question. I think this featured engineering approach to things, you know, it got revolutionized first in computer vision, then in natural language, but kind of it felt like that. Let's, you know, machine learning predictive modeling was kind of stuck in the past. We were doing the same thing for 30 years.
 Right? Like we were putting data in a single table and then, you know, we were training decision trees and then we were training support Vector machines and then I know, logistic regression was fancy and then neural networks were fancy but was all trained on the single table and all these features stores were
 Build. Because because of, that need to kind of compress the data or join the data into a single table. I think, with this new wave of kind of bringing AI to machine learning. Yeah, the need for that is drastically reduced. And, and we see it both in terms of productivity of building these models, we see it in building more accurate models and it just simplifies the stack
 Right? Like I it's actually the the statistics from Industries is staggering. You need about two full-time people per model.
 Okay. Alright, so companies, you know, if they have 10 models in production, they have 20 people taking care of them,
 If you want 30 models, you need 60 people. If you want 400 models, you need 800 people. And that's that's the ratio. It's like two people. You need two full-time employees, to babysit a model and 30% of the model cost. Just goes to maintenance of running that model.
 Of course, then what also happens is, you know, people change jobs. So what happens is that in reality, you have all these models running in production, that somebody built who's no longer with the company. Nobody wants to touch those models because as soon as they, you know, even look at them, those models are going to break. Nobody knows what to do, right? It's even worse than that. I remember back in the day, we had somebody come on here and they were working at Yandex and they said, yeah, my
 last six months there.
 I had to go and get rid of a bunch of zombie models that were just out there. Nobody wanted to touch him because nobody knew if they were making the company any money at all, and people were afraid that, well, if we take them offline, and it did turn out to be one of those models that was making the company model or making the company money, that could be a really big problem.
 Exactly exactly. That's that's I think there's maintenance and and tracking of these smallest is a huge problem but with this, let's say a neural network technology with this. Relational deep learning craft Transformers.
 It's it's much. It's much easier to maintain it's much easier to audit. It's much easier to have them to have them there to retrain them automatically, and take care of them. So, the maintenance cost becomes much much less
 well, and speaking of maintenance, if you're talking about
 The feature store feature engineering way of doing things, a lot of the maintenance would just happen with the feature pipelines and that.
 I know so many folks who had headaches because of that and keeping the data fresh is not as simple as it sounds. Oh it's it's super hot, right? Like it's super hot actually. It's super super hard, right? Even if you think about, you know, basically it means that for almost like if you have Discounters this features, like for every event you have to update your features. So whenever you make a transaction, your future needs to be updated. If it's not updated, it's stale, and it's holding information. And if you made them bit of mistake and update it a bit too far in the future, then, you know, you are kind of having informationally, it's time to travel and her models are, are, you know? No good. So it's in reality, a super big problem that I think people in research or Academia, we don't really appreciate that because, you know, the data is always, just give to us, here's a training set. Here's the test said, it's a fixed split, who cares? You can kind of pre compute all but in reality just
 Updating those features is a huge pain.
 Here. Well, this is fascinating. I mean, you've gone from Computing features to just training these models. How many gpus do I need to train a smaller model? Maybe take me through if it was fine-tuned to if I wanted to just build my own foundational model. What am I looking at? As far as compute means costs and then all so
 Time because if you're training a large language model, you need like a month, a month and a half type thing.
 It's actually interesting. Um,
 The big surprise is.
 How little resources you need.
 So, it's actually very cheap.
 If you compare language models, then it's really really cheap.
 Large English models are really humongous, you know, they kind of read the entire internet and memorize it to do predictions. You don't need to do that.
 You, you need to learn how to recognize historical patterns, and how they how those predict the future, and that seems to require much less parameters. So these models you know you can do well with 50 million parameters which is peanuts you can do well with hundred and if you want you can train a billion parameter model but those are those are super tiny when we think of llms. So what does this mean?
 is you need less resources training, times are in hours, not weeks, or months and very importantly when you run things, when you put things in production,
 They are actually sustainable.
 Write llm putting llms in production is very, is kind of many times unsustainable because it's too expensive. Each llm college is just too expensive.
 But with these types of models, because they are smaller.
 Actually they're quite efficient to run. So your compute cost is low and and there is then the benefit it kind of you know the cost benefit analysis. Actually works up, works out well. So you don't need tens of billions of parameters you this models can be actually quite quite small from that point of view and you can train them on a single GPU. Now if you want the foundation model, maybe you need a couple. You need a couple of gpus but you know not 10,000.
 Have we don't need to build the data center to exactly exactly. One thing I want to say is
 the this structure, the data Foundation models as we said are
 Smaller than large language. Models easier to train have less parameters, and it's also means they are cheaper cheaper to operate. One thing that is maybe fascinating here to me is that you can actually build a pre-trained model. That allows you, that allows you to kind of answer tasks at hoc, you don't have to even fine-tune, you don't have to kind of train for a specific prediction task, but you can almost give a set of training examples in context to the model. And the model is going in to a single forward. Pass make you an accurate prediction if that makes sense.
 And you interface with these models through prompts like you do with large language models.
 That's a great question. So if you have a pretend model the way you interface with it is can is through some kind of prompting. It's a domain specific structured language. We call predictive query and it basically has two parts, it has a predict part and four part predicts. As I want to predict this, this quantity for this specific entity. So you say I want to predict your purchases next week. I want to predict some of your transaction values. Next month, I wanna predict probability of fraud for this transaction. So you specify that
 And yeah, although this is basically almost like a prompt and based on this, the model is then making a making a prediction if you are talking about General Foundation model, if you find cute, then the model just takes the data and gives you the prediction you want.
 So it's not like I'm using SQL or python. It's not, it's not like to prompt the model, you are using almost like SQL where you specify, what's the quantity you want to be predicted? Yeah, so let's see prompting this. So these are sure it's a sequel. That selects over the future that hasn't yet happened. So you need to predict that future.
 Nice.
 And you said something in a talk you gave that I wanted to dive into more, which I found fascinating and it was agents need predictive AI tools. Can you break that down for Me? Maybe to start the beginning, right? Like why are we making predictions?
 All right, we are making predictions because we are making decisions based on those predictions, right? Why am I predicting fraud probability? Because based on that fraud probability, I decide whether to stop a transaction or not. You know, why am I predicting churn probabilities because based on that estimate of children and then taking some action to bring that customer back, right? Why in a hospital I'm predicting surgery admission probability because based on that I decide whether to discharge the patient or not, right? So I'm or you know in in if you think in finance you could say okay why am I? Why am I predicting your probability to default on a loan because then I decide. Do I give you the loan or not, right? So prediction is really a base is for decision-making.
 So if now we believe in autonomous agents, autonomous agents, need to make decisions.
 Right? And you don't make decisions based on common sense, right? It's okay, it's not terribly wrong but it's not optimal as well. You want to make decisions that are rooted in the data to make decisions that are rooted in the data. You need to kind of predict the outcome of those decisions.
 All right. And right now, what's the bottleneck in let's say deploying this. Type of autonomous agents is their ability to make decisions.
 And this means that now you have you have your beautiful llm agent super, super intelligent and then you know you have to build manually some Machinery model that's going to say oh what's the probability of children?
 Right? Because imagine a simple agent that says, let's go identify people that are most likely to churn. Let's identify the best offer to to send to each of those people. Let's write a nice personalized email and send it out right in this workflow. There are two big decision problems.
 which people are likely to churn what is the best offer to give, and those are two predictive tools that you need to solve this use case and to end
 I see where you go. It is.
 That is.
 Agents we need predictions. Maybe we have not realized that yet because what agents do today is they query some document database and retrieve a couple of passages.
 But those are not those are not real agents, right? The real agent is actually something that does something to you, right? Like or imagine I'm a customer support agent.
 I want to estimate.
 What's your lifetime value? What's your probability of churn? Because based on this, I will talk to you very differently. I will give you different offers. I may suggest different remedies. Those are predictive problems, right?
 So how can we Leverage? The predictive models that we've been doing but like you said, make sure that
 they're faster.
 Exactly. I think the the benefit is in basically using relational Foundation models that these llm agents can can use a tools to query for this predictions. So, whenever you show up there, like How likely this person to turn? How, you know, what is their lifetime value? Oh, now, that they have this estimates, this is what I'm going to do.
 Right. What's the next best offer to give to this person? I am I'm, you know, I'm making this decision on the Fly based on a predictive model that is a rooted in my data, that gives me an accurate prediction rather than use some kind of Common Sense, hallucination to say, oh, what shall we do here?
 Yeah, or rather than how it's been.
 Done right now, which is the word.
 Creating these predictive models and maybe there's some Advanced teams that are offering up these predictions to the agents. But as we just have been talking about for the last 40 minutes,
 That is a cumbersome process the way that we're doing it right now.
 Exactly. That's the alternative it doesn't scale. It means you have to hire two humans for every predictive problem. You have
 And that's that's a lot of humans to hire.
 It's so obvious. Like, why why don't we have that right now? Or maybe you've seen some teams that are actually doing this because, like you said, you know, the step is Step, B step C is a super common use case and it's a pattern that happens.
 Within companies every day.
 Can we have you seen agents being used like that?
 If not yeah, like what is the blocker?
 yeah, I would say we have seen agents being used like that and we have a couple of
 Clients deep deployments where DC is happening. I think the the blocker right now as I see is the is the is the knowledge.
 Right, it feels like that. Is this AI delirium out there?
 And then there are always kind of quiet, quiet people who are doing real work. And they, you know, they are not being being hurt and that's seems to be kind of a big, a big, a big discount to me, is there anything that I didn't ask that you want to talk about?
 There is, there is actually hope for machine learning as well. That right now is a bit on on a Sidetrack or people. Don't, are they people don't even realize, or we seem to have forgotten how valuable structured data is because it's kind of the ground truth of the business and its it, it captures the blueprint, the of the business and we seem to have forgotten all that and you know, we are now excited about answering questions about documents but I think the, you know, the world is going to come back because
 Enterprises are here to make to make accurate decisions to make business impact, and to drive the business forward. And for that, you need predictions.