You know, so many things in in technology, repeat themselves, and I look back at stuff, whether it was Blockbuster, or in this case, Myspace, right? When we were all trying to find the right song for our Myspace, intro. No one ever imagined that my space could end up in the Dustbin. So my question is, who is, who is it that we're looking at today, that's a major player that's gonna end up in the Dustbin, right? Who is gonna be the Tom?
 Of the industry, right? Who looks the most like, Tom, can we imagine a meme where we've got Tom next? Some other AI personality in that particular organization is now in the Dustbin Justice is unsurprising or just a surprise.
 As Myspace was so I think that would be my spicy take. Because that one of these players, not pointed, anything specific, not even any names but somebody's gonna end up like Tom.
 You said something there, when it comes through, actually transforming this new way of doing software into something that is usable.
 I instantly wanted to ask you, what are some use cases? What are some ways that you've been seeing this actually having success out there with businesses?
 Mmm, most of the time, it's it's around, you know, summarization and expansion of ideas, whether that's looking through spreadsheets or records or, you know, evaluating written text at scale, you know, putting together normalized summarization or expansions that that can then be used whether that's, you know, filling out a lead sheet for a sales team or an RFP or it's a, you know a customer that needs to evaluate all the schools in their District write and evaluate the teachers in them. These are generally spreadsheet focused tasks, right spreadsheet, just an extension, a databases things like that, right? But this idea that we need to run through these things, no evaluate them and transform them. Add color. And I think that's one of the first things that we've seen these LMS, just so impactful at is this idea of semantic. Meaning, you know, it allows us to normalize Concepts down to a set of important criteria allows us to expand right into in.
 Other Concepts in a way that gives us Direction and a way that human beings just are great at at scale and repetition, right? But by having a similar instruction. So we're able to do that. Those are the things that I'm seeing really transform the most. You know, the question and answer aspect of Agents where we're asking a question about some data is pretty important, too.
 We've seen a lot of customers do.
 you know, these interesting
 setups with agents, where they'll evaluate or constantly look across a set of changing information. This can be a set of slack channels or teams channels, or emails or jeera bugs, you name it. And they'll have agents look across this stuff at a regular interval, and organized into some graphs, and a database and whatever works well for them. And then it gives them the ability to not just get a summarized output but to ask follow-up questions. That. So what we see is leaders out there leaders, that normally would get an email and their business. It would say, Hey, you know, this is broken, there's an escalation over here because this data center is offline and they got a call up about four people to get the story right there, grab them, their self and their dialing numbers, they're hitting slack cuddles, they're trying to get answers right, it's tough, those same leaders can go just ask an immediate question of this set of data that's been organized properly and get the exact same answer out. Almost immediately that is incredibly transformative to their business, their ability to respond to issues questions in real time.
 I mean, that's powerful stuff and that's what we see. Now, the challenge to those setups is they take a lot of preparation, right? It takes a lot of figuring out how to get this in place, so that in that moment, you can get that value out of it. And so, there is that investment, right? And I think the whole industry is kind of figuring out. What are these structures? How do we evolve this? But there is this moment of we've got invest a lot and figure out how to organize this to then get the use out of it.
 It's funny. You did say this key phrase, which is organized properly, the data that is organized properly and I was like, oh man,
 Every employee that has ever touched any type of data?
 Is sitting here listening right now thinking, oh, wouldn't that be nice? Yeah.
 Yeah, absolutely. And it's one of those things early on it was when we were having conversations with customers. One of the first things that would always come up is, yeah, my date is just this giant mess or it's this, or it's that. I've got this chain of. I'm thinking of a specific meeting. Now, this chain of 20 hotels, and everybody organizes the client data differently. The customer data. Yeah, everybody looks at the preferences where they ate reservations with the head of their kids differently. And so it becomes this great wealth of information that no one can really use because you've got no way to look across it in an easy way.
 But lo and behold, it turns out that these systems, these llms are really good at that, right? Giving them a structure to Output to asking them to structure. Unstructured data. Generally not in a really mathematical precise way, but in a way that just normalizes some of this stuff so that then you can use that Suite of data to really have unified customer profiles across all those 20 hotels, right? Just by letting the LMS know here's what I need. Output it. Here's what's important for you to look for in this conversation, in this email set in this you know, reservation List. Look for this, put it in this right place and that gives them, that body of information that ultimately then becomes really valuable.
 Yeah, two conversations that I had instantly come to mind one being with the head of AI at Weis Igor. And he was saying, you know,
 it really helps to look at llms as a tool that can transfer unstructured data into structured data and as very much what you're saying right there, like, hey, let's normalize this data. We've got it in, many different shapes and sizes, and forms, and schemas whatever. But if we can, throw it into the llm and get just a gut, check on where we're at. That can be seen as one step in the graph and then you can go and do different things that you normally do with data in the next steps.
 Yeah, even if it's just as simple as going back to that customer, I was just referring to what dinner. Reservations. Every guest makes at the different properties. What time? They like to eat, what? They order things like that. Each one of these customers properties, we're very different store that information differently but it's not too difficult to run through that and say, hey, I need a customer profile on how they eat. You know what restaurants, what types of food, what time they want to eat, how many people do they take the kids stuff like that? You can identify what the important points are then. The elements are great at going through, just a giant ream of unstructured information, and giving you that unified customer profile across all those properties.
 Wow. Because before, what I imagine was happening is that if you had a very important person at the hotel,
 You would have a human go and look at all of the different events that happen. Maybe it's in some kind of CRM, and they get to see that data. But as you said, if it's dispersed across various hotels and different,
 Physical buildings. They might not be privy to see what is in the other Hotel, even if it's under the same brand exactly, and that was exactly the challenge, they had, right? Sometimes it was in spreadsheets, sometimes it was in actual crms, right? Some of his local, it was all different, so it's very almost impossible to get you out of that data.
 Oh God, that's such a great use case. That is one of those ones where you almost are like
 it's
 Seems too easy to. But then you recognize like, how valuable that is and how useful, especially in the hospitality business, right? The hospitality business is known for like, the more hospitable. You can be the better. And so, if you can understand when someone wants to go to dinner and already have that in mind, and maybe you're helping them by preemptively, saying we've got a table ready for you. We know what you liked, or we've got this special, because we know that you really enjoy XYZ since we've seen that you've ordered it every single time in the last 10 Hotel visits. Like that is an incredible experience for a hotel or correct. Correct. So for everybody right? And it's it's something that's done by an llm with some really clean prompting and it described output it just runs 24/7 you know again it that point it just becomes compete. Yeah. So you
 Go ahead. Oh, go ahead. Sorry. Sorry. As you see, hit the nail in the head though with one comment in there. You really you you got it exactly correct in that, you know, it's it's his valuable as you make use of it and while that information can can
 Can you know, feel a little bit like you're looking inside. What I'm doing? In my preference is, I want that right. When I check into the hotel for you to tell me, hey, ya, by the way, Tuesday has the special you like, and there's a reservation at the one that you love, would you like to go ahead and make up? Those are valuable moments, right? Yeah, yeah. But again, yeah, it's about using the LMS for, for, what they're good at. You know, I think when we see some companies struggle, or some implementation struggle is because you're, you're looking for a little bit of magic, right? That maybe isn't quite there, you know, the the rules done that I always use when I approach customers or businesses that want to build some true agents. Some true agents software is, you need to start with the understanding of if you brought in an intern, right, right off the street and intelligent in turn. But fresh to the business and you sat them down, could you describe to them in natural language? What it is? You need and the example we just gave you could write, you could bring an intern in. You could say look here's this giant
 Island information, it's all over the place, you got to log into 15 different hotels but what we need you to do is look through every customer. Look at their dining and pull this information out. If you can do that, if you can articulate, how you would describe it to just an intern that you hire off the street, chances are pretty easy to build an agent out of it. When when folks struggle is when they get to that moment they're like I don't know how to do that. I want, you know, I want this data to talk to that data and if you can't articulate what that is and natural language are probably not going to be successful, building an agent to do it. That's usually the rule of thumb that I will guide people down.
 I instantly think about,
 A conversation, I had with Alexa who works at Bloomberg and she was singing praises for some of her mentors at the company. And, and at her past companies who as she put it,
 Could smell.
 projects that had strong Roi was like they had a nose for it because that's kind of what you get as you progress in your career and you have so much experience you recognize that like, yeah, if I can move the needle on this,
 that is going to be valuable and
 It makes me feel like a lot of projects that I've seen or a lot of ways that we're going about. Architecting, these agentic flows, we're creating something, they're fun. They're cool. But they're completely over-engineered and it isn't this like smelling the ROI that you're talking about and that's why I keep going back to like God, that is such a it's I'm not gonna say it's simple because there's probably the data and actually getting that data to the llm but it's but it's like one llm call. I imagine or maybe a few, it's not like you're creating some sub agents, some multi-agent systems, you're not going and trying to
 Do this over, engineering that sometimes you see with agenda, use cases, 100%, you know, karpathy have listened to, in a particular talk. I think it was maybe at a y Comm orientation, or kickoff, or something. In particular one, where we really articulated. Well,
 You know, software 2.0 versus software 3.0 and understanding the 2.0 is the deterministic component, and 3.0 is where we're leaning on the LMS and this new idea of summarization and semantic, understanding tool calls. But how important it is to understand when to use the deterministic stuff and when to use this new technology and not just hope that, hey, let me throw AI at the entire problem and imagine something's going to come out of it knowing, you know, specifically with these the AI for, and whatnot do is really just key to it. But yeah, absolutely man, that helps you understand the ROI and you can see it coming from a mile away because you can you can see the tasks that are going to require that level of repetition. It's almost the stuff that you don't want to ask somebody to do because it's such a bearer of a project, right? Those are the things and you can see him coming from my way to your point. Yep.
 Oh, what a great call that. It's something that you don't want to do, because it's a bear of a project. It probably involves a whole lot of data or a whole lot of rope work. And by recognizing that
 You can get out in front of it and try to see. Yeah, let's
 Can AI help with this can and knowing where to use it and knowing how to scope. It is really your
 That's where you can add the most amount of value, 100%, we're seeing. Now that initial phase of, you know, what is the the meat on the bone for llm automation or task? We're seeing that kind of starting to be exhausted from a conceptual standpoint. What we're seeing now is the the edge of this is being pushed into
 really more, what we would have considered human moments, right? Things that are real interactions with browsers. Now, I think all the companies are shipping some great browsers. Now some of our fantastic products but just outside of that the problems that we're seeing in business, now that folks are wanting to automate are really, they're just describing to us a website, they go to and they log into and they check this and they check that and they upload it over here. And these are
 These are really very human interactions. And we build all of these systems, all this technology, for human beings, to do this, to look at the web rendering, the click, the buttons. And now we're almost retrofitting this um this whole set of Technologies on to understanding, you know, rendering the browser window looking at it, not that the llm needs that but because we build that for humans in our in our eyeballs that's the way we know instructing yell and it's good to go take the tasks.
 yes, that the GUI that we have for humans Now is really like
 Difficult in so many ways for an llm to be able to navigate and execute on reliably.
 Yeah, I think and you know it's funny we see that in so much, so much stuff in technology just infrastructure that's in place. Whether it's the internet itself or Airlines or roads or something. It's almost as we get the new technology in, we almost have to retrofit onto the infrastructure that we made for us as human beings and initially, we very rarely kind of rip it out. You know, and start over with like add the technology but think about you know, cars or or any of the other modern Technologies they're they're really just kind of retrofitting technology onto the human interaction that we build initially.
 Yeah, 100%. And
 I think there's some
 I used to live in.
 Park City, Utah, and insult Lake. I remember. And somebody will have to fact check me on this because it might just be something that I heard when I was in high school and before, like Google and so I heard that they had such wide streets because you had to be able to do a U-turn with like a horse and buggy
 And so they had all of their streets were just gigantic, it was like four lanes on each side, you know? Because you had to be able to do that U-turn. It never got updated as we stopped using horse and Buggies. It just Ah that's the way it is. All right cool let's put cars on it now and we just have more room for cars.
 Same thing. Yeah, 100%, And it's funny because I often think about the, you know, the bandwidth limitations of we interact with these ai's these llms through interfaces that are grossly, under scope for the amount of information that could pass back and forth to us, but we can't do much more than type or speak and we can't ingest much more than what's printed on the screen or which shown in front of us and that's absolutely the the full like small pipe limitation. I don't think that that changes until we get entirely new paradigms, write these the neural interfaces, right? Or something like that, that truly changes that until then we're stuck shoving, all this AI technology, through these little bitty straws that we, we initially thought were just plenty for us, right? Do. Well, that's funny that you're calling out. Hey, it needs to be a new paradigm. I even just am saying, let's rethink the chat. I've had a few awesome guests on here and seeing a few great talks where
 folks, will talk about how
 Indifferent areas of software. We have different interfaces that we can try and borrow from.
 when we interact with llms or agents, even, and one way that I've heard this
 be proposed is that when we're in Lightroom and any photographer knows that when you take a picture and then you go to edit it in Lightroom, you have a histogram
 and you can bring up certain colors and push down other colors or or bring out the shadows and you have much more control ability.
 when you have that, but with
 Agents, we just have chat. We say something. And then you hope that it understands your intention to the best of its ability and then it then it goes in it. You wait a few seconds or a few minutes and come back and see, like did it, did it get what I wanted, but wouldn't it be cool to have a little bit more of that type of interface, where we have all these knobs and all these ways that we can?
 Express our intent in a way that isn't just through language, because language is very fuzzy.
 100%. But what's interesting is, these systems are built off of language, right? So it's almost that becomes the programming language and becomes the instruction language. I will say though that, you know, chat as a single interface,
 Probably doesn't Encompass everything we need. You know, I see a lot of some of the most successful customers that I interact with building.
 Different uis. That try to meet that person where they are and what they're doing. You know, you can present some information, that's AI, summarized give them some quick buttons. You know, something as simple as, you know, email for instance, right. An AI that just looks through your inbox and then just puts a draft reply or puts a reply you know, draft in your drafts. For each of the emails you have in your inbox, it's one of those things. You didn't go to a chat on your face and say, you know, I'm going to paste in an email here you tell me what the response is. It may or may not be used that draft, but it's right there, right? And so, you're taking the interface that that you work in that makes sense. And you're bringing the AI, or the technology to it, you're kind of trying to merge both of them. Now, the reality is if that draft isn't right? Or you want to change something about it, your point, maybe the the variables are known enough to where you can do levers, but I think that we're never going to get away from
 Either typing text or speaking text, as a, as a way to communicate. Even if 70 or 80% of the interactions can kind of boil down to more, very tactical. Purpose built buttons, interaction points, touch points. I think that, that way of communicating language, probably just, it's probably here to stay with these. AI is just by the way, they've developed from language infrastructures and Nets.
 so,
 do you have any other?
 Examples.
 That are.
 Maybe simple.
 on the surface, but just have been
 super valuable for companies.
 Let's see, simple on the surface super valuable companies. Yeah we're not even simple on the surface. Forget that I even said that just is there anything that comes to mind that you've been seeing has such strong or Roi? Or I don't know where I was going with that acronym but anything you've seen
 In that regard, that has such strong Roi.
 That you have to kind of shout it out right now.
 I would, you know, really it's each industry or each organization in a company. Really gets so much different value. Right HR is really obvious in terms of being able to summarize and expand and look for things, legal legal deals in, so much contract language and needing to go through red lines and understanding intent and evaluating, you know, changes in documentation and comparing negotiations development, right? You know, encoding it's really obvious coding is one of the one of the first things that these these, you know, systems are really excelled at. And I think maybe because the outcome is so provable, I'm not sure that they've, they've been so important there. But I mean, marketing is one of the places that I think we see the most, right? If you think about what you can do, from a design perspective, a branding perspective that used to be some really difficult long, arduous work of a designer, right really working with pixels.
 And what we've seen is those design teams, the ones they're being successful are taking taking hold of this. And instead of right trying to compete with an individual design, they're trying to figure out how to empower as many employees as possible to get that same output, right? So if you're a design team understanding Vibe coding right, understanding how clawed or Chad gbt codex, any of them actually. Builds some Vibe coded UI, if you will. And then if you're a design team, trying to put rules in place so that you can Empower, your workers your employees to get that same output. So I think that's one of the most impactful that we've seen. It just just in terms of human hours taking out of the loop. It doesn't, it doesn't degrade the quality output. It doesn't offset the people needed, but it just makes it so much easier to make more content. I think we're probably seeing that everywhere across the board, you know, this this flood of content but it does help, you know, to have
 That really personalized content that you can really speak to individual demographics groups purposes. We're seeing that being one of the most transformative I think
 Oh interesting, so you're not saying like a design firm that will turn out 200 different ideas and then choose three of them. You're saying personalizing
 In a way that wasn't necessarily possible beforehand. Absolutely, I think everything, you know, social media was already kind of becoming this short form content thing, you know, I think NRF earlier this year. It was all over. It was, you know, everyone needs to move to short form content. It's all about the Gen Y. It's all about the experience in person, but the short form easy to consume content in the the reason for that I think is because it is more personal, right? These feeds these algorithmic feeds, everybody gets, you know, they're really, they're really customized to you and your needs and wants and the more content, the more granular, the content is,
 the more targeted, those things can be, you know, there's arguments, you can make well whether that's targeting good or bad, but the end of it, you know, that ability to have
 Material that speaks to individual people, more specifically, I think is valuable, we seem to be valuable. We've seen the customers like it and it just matches the the flood of this, you know. So social media driven demographic, hyper-personalized experience that that we're seeing of all over the world.
 So what are some ways?
 That.
 you have encountered challenges when building with AI and
 Overcome them. What are some things that just were hard?
 You know, um, tools and Integrations are one of the toughest, you know, initially, I think everybody was impressed with, you know, I ask a question of a PDF and I can give an answer, which was pretty important. Don't get me wrong, all 18 months ago, or whatever, you know, as fast as we're moving. But now, what we're seeing is, is as we passed that initial, hey, chat with PDF and with past this phase of rag, where we need to take everything and adjust it, and turn it in, you know, vectors and then search across it. We're seeing this ability to power elements and agents with tools in the way that makes these these calls optional right? You know, he's huge just tools and function calls in general.
 And that's where really, you know, and as much as a shied away from the term magic. That's what a lot of magic happening right now, because what we see is, um, you know, a certain set of instructions and prompting giving the llm kind of some really agency about. I'm going to look something up. I'm going to read what I got. I'm going to decide if I need to look up more. I'm gonna meet him making action or taking action, because of that. And so that,
 There's two parts of that. That's really difficult.
 Number one, you know, once you've given an agent the ability to just use a tool or call an API, it's it's really important that they know exactly how when why, maybe, what not to call, right? Because it's very easy for an agent to go call, you know, an API that returns back, you know, just you know 100,000 words or something. Immediately, you've got an issue, right? So that's one thing, but the second thing is the security because you've given some, you've given some sharp scissors to, to a very, you know, toddler esque entity in terms of its ability to run around and make some make some, it's an issues or cause some Havoc. So, you know what, we're seeing one of the biggest challenges is in that in giving that agency but with some constraints, you know, I think that's where we get back to this idea of governance and how do we make these agents software systems? Something that can be used safely in business because they are non-deterministic.
 And just by the virtue of the definition, it's incredibly difficult to put a discrete rule set around something that's non-deterministic. But what you can do is get a general sense about what you want to empower these things to do, and then maybe zoom out and start taking a look at. Well, if it does, this does that mean I need to change the rules for the next couple of things, right? So you start getting this idea of understanding how to constrain some of this Behavior, even though you're trying to give a blank slate at the same time, that tension that balance is really where we see The Sweet Spot in building these successful repetitive trustworthy systems right now.
 it's funny you mention that one because I've been meaning to create like a fun series or just
 set of tiktok videos where I play a character that is an agent Persona 5 called like Johnny drop tables and it's like this agent keeps deleting our fucking database. Let's go on here.
 And Johnny drop tables doesn't even realize that they're doing things wrong, they're just doing it wrong, you know. But the other thing is the question.
 That I I feel like every agent Builder is going through right now is exactly what you had mentioned. How much harness is too much harness and where is that? Sweet spot? Have you found any tricks? When it comes to that? Wow. Um,
 It's not necessarily a trick, you know, so many of these things, I'll go back again to the human analogy. It's like that intern right? If you just give them that set of tasks on Monday and then check in with them on Friday to see if they got there. I mean, maybe right. But, but the real answer is you're, you're gonna be on top of them, you know, once and now or a couple of hours, you get a check in and, and that's what we see with agents and so much of this is really just, you know, so many human analogs but for this specific one, it's that it's checking in until you get a level of trust. You know, we have, we have some really interesting tricks that we do that help anytime and agent hits an error. Making sure that agent understands the air and in a separate stream of evaluating help. It could potentially prevent itself from hitting that are again and going ahead and recommending that change. So there's there's things like that that you can do but there's still there's still no substitute for right now, just a little bit of oversight and guidance because the vectors of you know, how these things can go wrong.
 Longer just so massive, right? It's just this little 360 degree ball of. You could go any direction and you've just got to stay on top of it. Like a like an intern. Like a new employee until you get a level of trust.
 Yeah, it makes you feel though and I don't know if you've had this realization, it's like, wow, this is really cool, but this is unscalable if I have to be checking in continuously, all right. If I have one agent, that's great, but if I've got thousands that I want to be doing all of the stuff that I normally do throughout the day, then me checking in with them constantly is not really going to work. Correct. 100%. And, you know, I kind of often will hold the the one person billion dollar company up as a kind of a North Star, right? You know, he's sort of a mean we all had at the beginning of this. How long is it gonna take before? There's the one person billion dollar company when we get there, what will that look like? I mean, because you get Envision that you can get there. And he's sort of work backwards. He started thinking through this and you that's where this human analogs that keep coming into really coming into play. Maybe not because they're perfect, but because again, we're talking about
 Out replacing systems that were already human infrastructure, right? So it's back to that that conversation we're having earlier about, you know, the infrastructures made for humans. So if we're going to replace them or we're gonna we're gonna we're gonna attach llms to these tasks. That's probably how we're going to look at it. And if you work backwards from that, you come up with the idea that
 The important tasks, the big important tasks are probably going to be held by things that you trust more for one reason or another. Those are probably going to be more expensive. They can be more capable, they're going to have more experience, just like the people that you would trust in the organization. So you get this idea of, there's probably going to be this this tier this pyramid, this hierarchy if you will of the types of agents and the resources they have and what they cost you versus, you know, how much you can trust in them and how tall is they'd be
 Oh, I hadn't thought about that? Yeah, it's actually
 because,
 We kind of just hit the API now and we pay for input tokens and output tokens. We don't think
 that and I guess in a way different models could be seen as that you're paying for the more expensive model, but
 yeah, I could see a world where
 You're willing to allocate a certain budget and I think in Tropic, kind of did this for a while. I don't know if they're still doing it with the
 Idea of saying.
 Allocate x amount.
 To trying to solve this task and I'm okay with you, spending up to that much. Yeah 100%. We see this. One of the first questions we get from customers. When they start talking about these things in business is wait, how can I, how can I put out some constraints on the spending, right? How can I budget what this is? Whether that's a budget for the entire project or, you know, X tokens per month. Or I don't want you to go over more than this per day. That you usually is, is one of the first things that we get. That's actually not that complicated of a system put in, we have it in like a lot of people have that in and it's one of the one of the things that's needed right out of the gate just to kind of control that. But again it is that same human analog, right? If you think about I'm going to put a new business unit together. I'm going to hire some people on the first things in your head is what's the budget. And you know, the funny thing is the no matter what the budget is, we as humans still find ways to mess it up and you
 I just think, like,
 to a conversation that I was having with a friend about Snowflake and how
 You can set a budget for, let's say that, like my monthly budget on snowflake is 40 Grand, but that doesn't mean that you can't eat up that 40 Grand in one.
 Lookup, you know, or one equation and then your whole monthly budget is done because of one thing that someone put. And so having much more scoped, budgets is almost what we're lacking in a way and I feel like there's a very clear parallel with the open AI or enthalpy apis because you can put a budget for your company.
 But I can also use that budget or I can have my one engineer use that budget in one day and then the rest of the companies like what happened, I thought we had 40 Grand to spend. Yeah, rolling windows, are the best way, right? You know, if you think, you know, x amount per day or per 24 hour rolling window or per week or stuff like that. So there's this decent metrics that you can do that. You can put in place, I'd say, the more important thing is really kind of understanding when you hit that. Where's it coming from right? You know, what caused that hiccup. Usually, it's some sort of Rogue look up or something like that. You know, an API call it, you know, that it was supposed to return 30 pages of PDF to humans. Not something that went straight to a context when they
 exactly. I like that rolling windows. That seems like something that is again. Fairly obvious for somebody who has experience you instantly call that out. You're like, no, no. We dealt with that before. I know how to not blow a bunch of money up in the make that whole money truck, go, boom. You know, so that feels like a good one. The
 the other piece though of it, like,
 Do you see ways to?
 look at budgeting per person, or is it per
 team like how do most people like to structure it? Because if we're looking at it again like humans, you kind of have budgets per team or per
 category or activities, right?
 Yeah, so we see all the above, we see, budgets assigned to individual people users, we see budgets assigned to projects, we see budgets assigned to you know individual tasks per day. We see all that it really kind of depends on what is the setup tasks, right. One thing it is interesting is there's never a question about was the output worth it. Alright. And let's there was some mistake where you've called something and you grab a bunch of dating. You shouldn't have and it just blow up the token window for the day and you hit your limit for the most part.
 as much as we talk about how expensive these tokens are, the reality is compared to
 Right. They're astronomically cheap if you really think about and you're using it properly when development standpoint, I'm marking standpoint HR legal all the things we talked about the actual cost that you spend on the tokens is is just minuscule. Compared to generally speaking. What it would cost to have a human being do it now, that's got to change, right? There's these companies have these, massive Investments, anthropic open AI. Actually, I massive investments in these data centers. You know, everybody that's running the open source stuff from from Asia, as well as huge investment. And so there's going to be a natural Arbitrage, right? If if I'm getting massive value out of some tokens that aren't costing me a ton and somebody's going to Johnny data. So they're looking to monetize that's gonna Arbitrage out. Right? So I think we're in this weird golden age of. You know it's it's the juices absolutely worth the squeeze every time if you will. Yeah but it'll be interesting to see how this plays out. We're going to get to a place pretty.
 Quick, where there's Rich painful, right? Where the value is, much more associated with with what's what the output is. You, people have to make different decisions. That's where the budgeting is really going to come into play. I think we start getting closer to that that cost from the, for the value.
 Yeah, enjoy the subsidies while they last. Exactly. Exactly.
 I have heard.
 Different companies going about this with outcome-based pricing. Have you tried to experiment with anything like that? We haven't um you know what we do is the software that we offer enable someone to do all this stuff you know it's organized that they can hand it for carrying feeding that has all the security governance stuff. We've talked about
 We offer, you know, our own keys for instance, for customers. He could, he could charge me with us, you know? We burn that down, or they can use their own Keys, bring their own keys. We're really just agnostic to which approach the customer takes. It's not something that that is, you know, important. We really try to focus on the value of our software provides. So we're all we're, we're sort of, you know, observers in this, the this price Wars as well at the same time.
 We're watching it unfold at the same time. We haven't seen you know, many people that are
 That are pricing outcome, right? It's really just time and materials and that's not deliverables base. If you will if you go back like a PS comparison
 Yeah, well I this is probably a good moment to just jump into what you guys are doing.
 oh yeah, so area is um is an orchestration platform, you know, and we began by
 Looking into some applications that we wanted to build and some of them were health and wellness. Some other areas and we build some AI applications, they became our reference applications and then we step back and said, what's the lowest common denominator? The problem. If we wanted to solve was, there's an enormous amount of complexity in these new technologies. They're brand new. There's not a huge knowledge set out there in the market. There's lots of organizations out there that have really competent Developers app developers. It teams but they don't have ai experience.
 Means that they are probably capable of doing all of this. If we just give them the right tools and put it together in the right way. So that's where we started. We said, hey, there's this need for everybody to adopt AI, find ways to bring this into the organization and get value out of it. How can we make it simple for the teams that are in place today to be able to do that? It's, that's the system. We started put together, we put together system, that's agnostic of the model, provider of the tool of the token of whatever it is. We just wanted to put together canvas. If you will to assemble agentic business software to test it, to manage it, to monitor it, to have ways to have it, be reliable. And then we've added in a lot of other feature sets, the evaluation is the red teaming, the security guard rails. A lot of the constraints stuff they're talking about earlier. That ability to let your agents go call tools, but then all so not do some of the more destructive actions or change those actions out as the agents go through their execution, based on what they looked up. You know, if they've looked at some sense of information
 They shouldn't be emailing outside the company, the same time, some of these things that are, you know, just really simple to understand but have an existed in this new paradigm of, of AI technology. So that's what we do at area. And that's really what we put together and all of the security and guardrails stuff that we we Market that we provide value with is available, whether you build your agents inside area or not. But we did begin as a true agent building platform.
 tell me more about the red teaming because that always fascinates me and it always feels like
 It's gotta be the most fun job. Just the professional red teamer. Oh, he he does. We do have we do a PhD on staff and he does love what he does. In that aspect, our red teaming, we, we tried to build it from an agentic standpoint almost from day one, we really acknowledge that boy. As you build these agents QA is just really tough standard integration tests just don't work, right? So you have to come up with these ideas, these schemes of adversarial testing because really even if you're testing a conversation or you're testing an agent for a particular output, you can't just ask it one question.
 Right? You have to have a conversation. You have to have a conversation with the almost different personalities. You have to come. What is, you know, how does an angry person react to this conversation? How does a person who doesn't speak English as their first language, a person who is too verbose. Oh right, all of those things. You've done was come up with these ideas of who are these personalities and then let that be an adversarial challenge to the agent. You're looking to test.
 Well, once you've done that that's actually not the hard part is you get all these answers? You get all these conversations. How do you evaluate them?
 So you've generated all the synthetic data about how your agents perform and then what's important about that, right? How accurate were the eight answers? That came out. How closely the compared with what you expect it.
 How well can you trust that this agent is going to behave the right ways in these environments.
 Sounds like that's a very expensive task and that's why. I mean, you've undoubtedly heard llm is a judge, just kind of what most people throw at that but I
 Am not convinced that you can just as a judge it, this for something like red, teaming, where it is a little bit more.
 High-risk.
 Yeah.
 Help me understand, what do you worry about with Ellen is a judge?
 We go to pros and cons. I'm curious. Yeah.
 yeah, I feel like the main thing on there is that you can
 not know what you don't know. If you're throwing elements a judge at it, then you feel like things are going well because
 The llm has said, yeah, this is all good, we're golden here. But in reality, it's not
 among a large Corpus of data, you may let some things slide under the cracks. And if it is
 A red teaming use case. It just feels like you want to have someone be.
 Really getting into it with the those multi-turn conversations that you've created to know. And to
 Understand.
 The way that.
 The agents are communicating.
 Yeah, I'd say that's accurate. Um you know, we generally encourage a combination spot test and I was a judge because you can get some value of scale. Out of llm is a judge across multiple operations that you can't do in true. Human eyeball spot checking. But your point is just as important because it's probably one of the things where we, you know, encourage both approaches but I don't know that there's a
 I don't know, there's a solution on other side, you know, the scale of being able to run these types of evaluations without needing 100 people in a room, reading reams of conversations is valuable,
 You know.
 It is, yeah, I I see what you're saying. It's like you use both so that you can
 Almost like sanity check have one sanity, check the other.
 Absolutely, absolutely. An interesting Paradigm. I don't know if you've seen this or it's come up. We found the in times when the evaluation is against different base models.
 I don't know if you've seen this and set up just evaluating based on personality. If you're evaluating a different model that you might use in a few different agents or you know, a few different models. You might use one agent. We've seen that the judge of evaluating it, the models tend to prefer themselves so much that you have to make sure you get a diverse set of judges because it's undeniable. If you look at the statistics and you do this and you just measure multiple models and you evaluate their efficacy on the answers and you use models to evaluate that efficacy, you undoubtedly see over time the models prefer their own answers. It's really interesting.
 I mean.
 Sounds a lot like some humans. I know.
 Again, right. There's the there's that the theme like, yes, right. As much as we might not want to think that we are, as you know, deterministic or as as understandable as we might be, it might be that well like these things.
 And are you evaluating?
 The conversations for a certain set of important features I imagine and then evaluating or just looking back on how well. They
 Are tool calling or how well the agents are actually like executing. What was asked of them? Can you break down that? Because I've heard some folks talk about how they'll go as far as to create heat maps for the agents and really see where the agents fail.
 Over many different simulations. You can get like a clear picture because of the heat maps that you've created. Yeah, and I've seen I've seen this really compelling generally what we encounter from our customers is, they know very specifically what they want in an outcome. They know very specifically how they want an agent to react. They have an idea of what they would prefer. So it's less about a, you know, a true evaluation of kind of Choose Your Own Adventure. And let's see what we end up and and where we can get and it's more about, you know, here is the exact answer that I would expect or would want in that case. And so mostly evaluations that we serve customers run, are how close am I to the true north of what I said? I want to the agent to enter in this particular situation.
 Oh, that makes it so much easier. I imagine.
 simpler, you know, but then you still just only have this linear approach of, you know, how how like,
 Was this answer which um you know sometimes it just a percentage can be a little bit misleading but generally gives our customers what they're looking for you know.
 Amazing know, is there anything else that you want to talk about that? We didn't hit on?
 You know, we're seeing, I know, I know we talked a lot about Integrations and and that the tool calling being where agents really start breaking out of that and start start exhibiting behaviors, that look really cool that look like to figuring stuff out. We're spending a lot of time there. That's to me, one of the more important areas and, you know,
 The idea that, you know, we all were settled on rag for a little while and we just had a retrieval and a generation as a way to get information out LMS is, you know, I think it was a flash in the pan. And what we're seeing now is to say idea that it makes more sense to empower agents to go look up the data and do what. We just described Choose Your Own Adventure, right? Decide if you need to look up more, the idea of rag feeding matching chunks in is probably going to really diminish its in his efficacy. Its applicability going forward and so
 I think.
 What we're seeing is just that important and that importance of understanding, how do you equip an agent with tools? And it kind of comes down to apps, right? We all have apps things that we interact with and now we have agents. How do we make our agents securely? Talk to our apps, you know, we've seen mCP, just just proliferate, just everywhere. Everybody's running these GitHub repos, you know, you want an mCP server for something you search and we're just, we're grabbing things out of GitHub. Like, it's hugging face, right? We don't even know the guy that wrote this mCP server, but it says it's going to go check that API and give me the price of, you know,
 Yeah, gold and Zimbabwe. Who knows? Right. And so, we're just running this just GitHub repo, we're just running it, right? We're just eating around API Keys, it's not. It's almost as if we've taken Decades of security practice and just throwing out the window. You know, the amount of API Keys you can find and get a repost now or the things they're just being thrown around an email says notes. Now, there's some real truth to why everybody's doing that. There's insane value to these tools. The the value of the is about letting the llms go get this information and interact with these agents or interact with these apps.
 In business. That's incredibly dangerous, right? Because essentially now we're now saying hey I'm going to give this agent an API key to go interact with my atlassian products at or my Salesforce products at that's dangerous and we don't, we don't then have identity. We have trouble with data governance and access. So, the real next Frontier I think, is in trying to figure out how authentication fits in all this
 Oh, we need to give these agents identities, or we need to let them use the identity of the person. Executing right? Which starts to come into this idea of oath of user base? Authentication of, you know, dynamic client registration. Lots of really detailed stuff about how can we make sure that when we go execute an agent and has five tools that when it calls us tools, it's doing it with the credentials. The user, that initiated the call.
 And I think that's going to be one of the biggest unlocks for us. As we as we turn this corner into securely empowering, he's agents interact with their business systems and figuring out. How do we handle this authentication throughout the system throughout the pipeline of this agent? Taking a query going to get some tools, doing some things, but doing it secure way, with identity, that matches, our existing practices to get out of this whole, like, you know, let's email Rose and API keys and run some random GitHub repos. But,
 So that's that's what we're going and we're putting a lot of energy information. Go ahead.
 Specially when it comes to dynamically provisioning permit permissions, and taking them away because you don't always want the agent to be able to do what you want it to do right now.
 100%. And that's where that idea. That is constraints comes in. Right. That idea that, you know,
 Simple basic rules that say, as I described earlier, if an agent goes and looks up something in my last you know, my jira, right? And then it goes to send an email, I don't want that email to be sent outside the company because it's just looked up since of information that's in the context window. It might, you know, in a, in a bad scenario, might find some reference in a jira, bug to a customer and a problem there, they're expecting an answer to and that customers email address might be in there. And look, I've had agents do some crazy stuff when you go back to access to do it.
 but, you know, you can see a scenario where an agent's going through and it's looking up at Jerry taking the finds that there was a customer and that customer's contact information is right there and it wanted to know about that bug and the agent just says
 Let me do this, right?
 Those are the scenarios we really want to try to avoid, right? We can understand what that scenario is and the risks of it before, that even agent even executes. A lot of times you can just look at a gently, the prompting and instructions of an agent and know what it should and shouldn't be doing. But it's simple idea of this. If this then that statements around Asian execution around tool calling and security dynamically adjusts in real time based on what that agent has done based on where it's going to your point, right? All this happens based on a rule set because it's got to happen faster than we can keep up with. So we have to establish that rule set ahead of time and just know their agents are going to work inside it, even if there's some intelligence Dynamics stuff in the meantime, but but that's a, that's what we're seeing is the the way that we can actually finally cross this Chasm of businesses, have these cool prototype agents but they're scaredly actually using the software.