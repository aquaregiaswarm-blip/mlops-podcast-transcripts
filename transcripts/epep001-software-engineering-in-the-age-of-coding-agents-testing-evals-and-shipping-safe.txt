The language itself is very sensitive and you need to be able to to test different versions very quickly and see if a change of one phrase trickles downwards into a different conclusion. At the end of investigation and you need to show where where you started thinking in certain way. It's quite complex.
 I was recently talking to a friend and he mentioned to me.
 Never have. I paid so much for a tool.
 And he was referencing Claude code.
 Never have. I paid so much for a tool.
 And felt like I'm still the one that is coming out on top. Like I'm getting more value than I'm actually paying for it was like I you know every time the Spotify Bill comes through and it's 10 bucks a month, I sit there and I debate. Should I cancel it? I don't know if it's actually worth it. I'm paying 100 200 bucks for Claude code and I'm like I would probably pay five times that because I'm getting so much value from it.
 Okay, I think that's that's what we all have. If it's been experienced in right now, I think it's not only called we have like you know let us set up of clothes code and then we have, you know, people have been trying to enter gravity. People have been trying, you know, obviously cursors been around before that we were vs Code Pilot Shop. So I think we're it's very
 He we're switching between them but it's definitely.
 The velocity impact is huge and it's still funny to see, sometimes called will give you.
 Like an estimate of work and says, oh, this is gonna be three weeks of work. You say just do it. Trust me. It's not going to be three weeks we can start with the first step and then you're like, go for it and you know, an hour later, it's all done.
 Yeah, that's amazing.
 It is crazy. Now the context here is that your doing a agentic work at a security startup and I wanted to talk to you just because
 we've both been seeing that.
 Software engineering in a way is changing, but you're coming at it from the traditional machine, learning, engineering space, and you're understanding what we used to call, you know, ml, We Now call AI. And now we're leveraging Ai. And just about every workflow possible, so break down,
 Your journey a little bit.
 Yes, I think I don't have a traditional data science background in the sense that I joined the data science Industry. When it was Peak hype, maybe 2012, I got hooked on this being the next, you know, the best career. And I kind of self-taught and got like online courses to, to go into this industry and at the time it was very much around like Predictive Analytics and statistics and machine learning. Some of the algorithms. We were kind of being taught were from the 80s or even from the 60s, right? Like but they were very useful at solving business problems.
 When Big Data was the biggest, you know, high do this? Yeah. Do was one of the things that I started with, right. And I think at the time, there was a kind of changing in software development methodology because if you were doing traditional software, like someone would write requirements back and you, you just build it. And hopefully, the customers would love it. And people try to do more agile as they were saying, we don't know exactly what customers want and on the did his signs side, you had these teams that would hire a scientist and say, okay, sprinkle some the designs on onto this project and they would say, oh but we don't have data so we don't have a can really do anything so our. So a lot of teams were kind of struggling and that is science. Is that a scientist at the worst successful where those who were able to
 Like hold on to date or find data to solve their problem, right? So if you were a good like getting data sets from other teams in organization you were you're you know successful and you had to go at barter at lunchtime. You had to you had to have to make friends. Yeah to do to get the job done and they did a sign dances without data is really. You can go to, you could be the best you know, in Kegel competitions, but it's not gonna be like you productive at work. If you don't have a data that you can use for, you know, to turn your kind of idea into a business problem that can be solved through data. And I think part of that a scientist or just in Predictive Analytics, you have to use some sort of proof that to show that your thing works and that proof comes from having a data and having some of these methodologies of innovation that are kind of core to this industry, if you train a model and you didn't have it a graph to show it's working. How do you know
 It working. I think that's part was never part of software. Engineering stack. People build software and just read the source code to set. It'll work and fluted the moon. So yes, I was in the tested it but it was not
 As methodological sound as what we would see with.
 The data science. And I think now we're kind of experiencing another shift in the sense that when we approach agentic system, it's a hybrid of both data science and traditional software, engineering practices in the sense that
 Agenda system are just software in the end. It's but you use prompts to program something and the problems prompts are like predictive models. They're not deterministic in any way and even within one vendor quite one provider. If you're using off the shelf commercials, you don't get this stability that you would get with software this software crashes in predictable mostly predictable race. But with llm prompts you might have, you know, latency deviation, but all so behavioral deviation. So I think that that forces you to kind of think differently than
 It was a software engineering.
 And that's part of what we're explaining both the stop Fringe as people, that practice engineering is then people would build a system that have to deliver something.
 let me see if I play that back for you because I
 There's a point you hit on.
 That I'm not quite sure.
 You were trying to make but it instantly made me. Think about how
 since you're Outsourcing the brain of the llm to an API and usually to one of these big research labs
 and they can be a little bit.
 Unstable.
 We all know folks who used in frop over the fall of 2025, probably.
 recognized, how
 It's like, is it my thing that's not working? Is it because anthropics not working, you got to go dig through the logs and recognize. Oh wow. All right, so what do I got to do to make sure that the uptime on my API calls is higher and
 They I don't think could have done any more. They were just getting inundated because of the demand being so high. So there's that inherently that is unreliable. But then you're saying there's also the two sides of the coin where you're riding the prompts and you're doing more data sciency work, which
 It's not like, does it compiled?
 But you also have to create the software because you're creating the agent and that is very software, engineering work.
 so you've got like these three pieces, the reliability side, you've got the data science side or the
 Stochastic side and then you've got this very deterministic side.
 What?
 Is.
 Part of what people forget and then they kind of realize is that the creativity of the other lamp? Is this still got excite, is it in order to have a creative element, it has to have this this random effect of variation in the output and that is beautiful when you're trying to generate poetry and it fails miserably when or, you know, write jokes, but it's very miserable when you're trying to have
 To gain someone's trust in in about a piece of software behaving in a particular way. Think in the end when we write software software is, you know, high level software that we write, we don't write in assembly, we write in very high level language is in writing, prompt is the highest of them right now.
 We're trying to tell the computer to do certain things and in the end, it needs to do what we want in a deterministic. Predictable way most of the time, and if it doesn't do it, we have a problem. So we have to kind of build systems around this variation so the creative side of elements, which sometimes very entertaining is a challenge in writing, agentic systems that kind of follow orders.
 So, how do you harness that?
 You can, you can basically, you know, build guard rails into your system and, and test it. But you should also have to test not only doing the authoring time which would, which was the normal way of writing software you just write it test it and then you had you could you know, the same in The Sims sense that people did.
 Whiteboard interviews. You could write an algorithm on the white on a whiteboard and people had. And then understanding that this is a shared language. Programming language, high level one and you could, you know, any advanced user of that language beloved to read it and know how it will be compiled and run with prompts. I could write a prompt on the wall but you can't guarantee. No one can guarantee how it will be interpreted by any other even the authors of the llm say, well, it's a good prompt, but no one knows, right? So that thing is challenging for everyone. Doesn't matter how, you know, Advanced you are, if you didn't work in the best, you know, research groups, you don't know how
 You know how it will run. So that's the part of software where you have to basically really build a garden around it.
 You ever pray to the software gods and hope that they hear you, or the llm gods, right? It's just like, I'm gonna throw this up. It is an absolute crapshoot. What's gonna come back, but you can't
 Build a business, a stable business off of that type of thinking.
 No, but but I think it's the same. When you want predictable systems you you have, you can really kind of narrow down your problems into small building blocks. A lot of the the challenge here in I didn't system is deciding
 How much responsibility to give to each agent? Like, you can like with the expense, we have, as users with Cloud code, you can give it the very first, tell tasks, like one sentence and say we Factor this thing at a feature.
 and you might have mixed results depending on
 You know, your luck. Like sometimes it will just find right filing a repository and understand what needs to do, but sometimes it goes, all right? And you have to start over and say, no, listen, here's the file. This is how we do things around here, have to do that for a while. You might start putting that, you know, into the context and then you contacts, because this is a set of instructions that never do this. Never, you know, change framework made flight when you're implementing a front-end feature, but
 You don't want to put everything in the context all the time. And I think that's, that's the challenge of
 You know, being both software engineer and like, in the genetic engineer is that you have to, you can't, you're not allowed to put all the instructions that you want to be enforced. All the time, you have to kind of
 use them sparingly and in the right context to get the results you want. But you can't simply just say here's everything that needs to be followed, you know?
 I hope I wish it would be possible, but we know from like studies on context window limitations. I did. Even if the contacts we know, is 200,000 tokens. You can't really use them. You probably shouldn't be using more than 30 40% to get anything.
 Decent out of it.
 Which means the ear, you're always, you're paying for capacity for, you know, for skill. But, you know, you can't really use.
 Old mcps that are out there or all the instructions are that you can write down.
 Yeah, there was that blog post by Manus, that talked about this, and how they got around it, right? I can't remember the term that they came up with, it was something like Progressive disclosure, or something like that. And so,
 I also have heard.
 From a friend of the Pod Brooke who runs Koval and she does a lot of things with voice agents. How
 A lot of times since voice agents are these multi-turn conversations and they're very high stakes and you want to be low as low latency as possible. You just can't have these gigantic prompts
 continuously being there for every turn and what they'll do is they'll build graphs and then dynamically inject different pieces of the prompt in at different points because more or less they have ideas of how the conversation should flow. If somebody's calling up a customer support agent, you kind of know what they're calling about. And so you can in different points of that conversation inject different prompts and there. And it reminds me of that Menace Progressive disclosure right idea too.
 Yeah, I think many people are trying to find the right way to do this, because we couldn't obviously compact the context window and compress it using various summarization techniques. But it's, it's not deterministic.
 So, we don't know what exactly will be lost if we compress too harshly. And I think the tricky part here is that
 Every conversation is different.
 But I think if you understand your domain, it comes back to, you know, data scientists, I worked with a lot of smart people in the past that were kind of converts people had phds in Neuroscience, or biology and they came into the science and software, right? And, like, some people have this allergy to kind of study the domain because they want to stay on the algorithmic site, they want to be, you know, different type of person. But if you understand, you're the domain and your users, then you can have more opinionated view in these questions. What is relevant? So, in the question of customer in a support, you would know.
 You know, there are some scenarios that are not reasonable. If someone asks for a chatbot is start coding in Python, it's okay to say, no, this is not what I've been trained to do and you don't have to use a one at 11 with all the problems. Like you can have one and do the guardrails and then another limb, only do the business logic with the limited set of functionalities and if it can't solve the problem, it cannot solve the problem. Having
 Like less Advanced llm with limited skills is actually preferable in. Most of these cards is both in terms of velocity in cost and we can even see it, like, with Cloud code like, it doesn't tell us exactly. When is it switching to Haiku? And when is it switching to like sonnet or Opus? But you can tell that some tasks are better with cheap fast model. Like let's say we're searching for source code. The task is to find which file we're going to modify you don't need
 The most expensive model to run some grip commands or Ripley. If you're
 If you have that installed, right? And for those
 Tasks, it's nice to have.
 Different sub-agents that can do. You know, they don't need the entire context to operate successfully. They feed back to kind of a more orchestrator pattern.
 Yeah, the orchard basically, the constellation of models is becoming a very common pattern that I'm seeing, and I was literally editing a podcast just before we hopped on the talk about this with my friend Paulo. And he said, we tried so hard to replace all of our traditional machine learning models, with new llms, but
 Oh, there's a few scikit-learn models in our workflow that they do much better in pound for pound against any llm that you give it. Just because of the nature of the Beast of what you're trying to accomplish. And so in their workflows, they'll have that psychic learn model there and they it comes with a bunch of inherent benefits, too because you're not, you don't have this gigantic model that you're trying to now serve and the infrastructure around that you've got something. That is a lot more common and people have been doing with it for a lot longer time.
 I mean now that you've mentioned like that, I've been thinking about
 So in my previous work, we did work on you know many years ago on Friday dictionary, right? And I can't imagine someone replacing a front detection model within. You know, if you can say a transaction is Farland because someone has a new laptop and they're buying a Rolex, watch on a shop where they just signed up for, with a new email address that that information.
 Um, you could obviously have an Olympic, you know, analyze that transcript of an information and make a prediction. But in terms of like velocity and cost, we had to kind of get the response in like you know, less than 50 milliseconds and and we had to be correct, you know, all the time practically, right? And and you can't have that with an alarm, you can have a length to explain what the system did, which is probably where I would, you know, use it today. And I think people are a bit hesitant to kind of say that because it's, it's trendy to use our lives for everything, but you can definitely, you know, build use graph method, users are still recommend the recommender system methods and when in hybrid approach with your llm, and I think that if you have the know-how of how to turn some section of your problem into
 A kind of predictive problem.
 You'll get a better results and even with your contacts management, you have these opportunities. Like if you have
 a kind of knowledge base of
 em contacts that you may need to include.
 Like the retrieval of that particular piece of context is a small machine learning problem, how to do efficient information and Revival.
 In measuring that the information that you retrieve is relevant because you can retrieve information. But who says it's relevant like that test of relevancy is a small data science problem that you can pick up a few have you know, the appetite and the domain knowledge to say I can I can say what's relevant
 Yeah, it's funny that you mention this hybrid approach is for fraud, because I was literally just reading an article and I think it was Pinterest that took a hybrid approach on their fraud. They're like trying to figure out ways to bring Ellen's into their workflow and specifically around the fraud use case I want to say was that, but
 I'll bring up the article and try and
 figure it out a little bit better so that I don't misquote anything. But that is I I like wholeheartedly agree with you if it ain't broke don't fix or you know like that old saying
 it still Rings true. If it ain't broke, don't fix it and
 the other thing that I was thinking about as you're saying that is how
 Different. It is when you are just working for yourself and trying to boost your own productivity or playing around with your own context versus, you're trying to make a product that can then be used by many people and like mass production. We could call it right? Because if I'm just doing it for myself, I think about how oh there's these tricks that I know when I'm looking at the code base and there's an error that's happening and I'll say to clogged code. Okay, explain this whole
 File.
 Explain it to me in as much detail as possible, and it will explain exactly what's going on. And then I use that as the context, with either clogged code or throwing in another model and saying, now find the bug, here's what's happening. Here's the flow. Here's the documentation. That is how it should be happening, where is the difference? And that's a great trick for me as an individual. But how do you make that so that now, when you have this AI product that's out there,
 It is operationalized. So I think the tricky part is that there's more than one other. Like, if you look at our traditional kind of organization in software, engineering companies, you might have, you know, kind of a business and go to marketside, then you have software engineering and then you'd have kind of Specialties within it and one of the Specialties would be ml engineering or mlops or data science or software engineering or, you know, devops people, right? We have this kind of skill set, novices security practitioners are kind of carrying
 Security is like a method domain because security is like a mindset. So you need to understand it and software and, but it's and also think like an adversary. So we have a lot of people that have like, the security expertise is not is that they understand how attackers work and think. But they also understand how operating systems and networks work, and they have very deep intimate knowledge about, you know, how to spot, the behavior of those. You know, malicious actors.
 As it witnessed through, you know, Telemetry that we have in the security industry, we have EDR and we have network monitoring Telemetry. So you have these people that are experts
 And you they can't tell like write the same software. So, I think, at least for when we build the agenda, existence, we kind of tried to say, which part is agentic business logic, which part is kind of instructions, that relate to the security domain. Like, how do we investigate security incident? How do we know
 To, to demonstrate to a human that we did, you know, everything that the human would do in the same manner that they would do it. And then, how do we take input from the users? Who say, you didn't do what I want? You should have done this extra step or in our organization. This is okay somewhere else, it's not okay but we allow it. So those type of contributions are all like separate. We kind of build three different parts of our system.
 To allow these interactions. But they all in the end they all like run in one runtime. But we have three people contributing code and prompts to the same kind of Investigation in the in the context of arrays.
 And the tricky part is it, they all need to be to have like, a feedback loop, right? So, you need. So, for a software engineering building in an investigation, it's very difficult to run an investigation without
 actual data like the first thing we did, when we started the company, you know, I joined this, you know, when the early Engineers we
 Created the lab it just so we can have like a real investigation before we had the new customers. We set up, you know, some some machines running malware in the cloud provider. And then we would see the Telemetry from some of the security vendors and through the Telemetry, we could tell the agent. You know what would you do in and see how it would investigate and through that build the process without actual data, it wouldn't have been possible and
 We see that, you know, obviously customers want to contribute and part of it is very much a product question. How do let people contribute with without
 Letting them kind of ruin the product. It says like they could make a mistake and like you know write something bad in a project wouldn't work and then who's to play, right? So we have to build some guardrails and what contributions each user can do. So the system still works and give them a feedback on what they on the actions, right? It's the most credible part is
 Is a user. When you have written a piece of code in a high level, programming language, you can compile it, you can run it. You can run tests on it. Yes, that's your confidence. You know what, you know, through that with elements investigation in or an agent, how do you know that it's going to work?
 It's sadly, you mostly have to try it. I think that's like the that you can obviously test.
 Incrementally different parts of the system. But end-to-end is the most powerful proof point that we have.
 Yeah. And and speaking about building four separate users,
 It then becomes much harder too when you are debugging.
 if the user is,
 in some way, shape or form, not
 Testing it or not. Looking at the logs as to why things are going wrong.
 There's not a clear feedback. It's not like the agent says oh yeah like I did all of this. I just got stuck in the last step. It's more like I wonder if it's not working because this problem is too hard or if and it's not capable or it just like
 Got stuck on one of the loops and it wasn't able to complete it. So I think there's a lot of that investigation were too. That becomes a little bit of a nuance and a headache. Yeah, it's this certainly, if you, if you're using one of the popular Frameworks like lunch chain, you have
 Limited visibility into what's happening unless you start instrumenting, kind of the state of of your agent. So, in lung, shape and they, they support these build language graph is the framework where you allowing to create these graphs,
 but the graphs mutate after each tool called they can accumulate State and accumulate information,
 You need to build tooling to kind of see it. Obviously you can, you can get a kind of a commercial.
 Observability framework in place and you could have a page, but sometimes your agents will have 100s of actions. So scrolling through a thread of 100s of actions is quite limiting. Even for, very technical users. So I think breaking the your questions into. What am I trying to look at is, is important and we see in the product itself, like when we show the Agents output, people really want to know what is it do. So if everything went well we need to have like a very nice audit trail of actions. We don't have to show every thought the agent has but we need to know we need to show to the users.
 What, what was the reasoning behind taking every action? And it needs to mapping what is normal human would do? So that part is really a kind of a ux question, how do we show enough and in the hide the information, what? It's too much
 and we also have sometimes investigations at least when we develop new content. New, kind of prompt, we need to show how things are different. So having a view that shows you, this is my version a this is my version B and showing you the difference in rerun of the same investigation is very powerful idea. Just being able to see the difference between them because the difference might be hidden if you have a thread of 100s of actions, right? So highlighting, what is different could be quite useful for the users that are trying to
 To understand their own changes.
 Isn't that fascinating?
 How the ux design patterns are really the most crucial part in building the trust.
 In my ability to know that this agent did the things that I wanted it to do or that I would have done.
 Yeah, I think a lot of the nice part of it is that it's all human language like in the end like it's not. It's so I think in the most in other domains like in video art or an image generation if you're trying to understand why did the image get, you know, Cloud misclassified, there's a lot of start then trying to say, oh, this pixel here is red and that's why it was, it was a cat and not, you know, a car. But it's, there's some noise in those algorithms that people are trying to understand. But in, in the domain of LMS, at least, you can really see how one word could throw their name off if you use the word.
 Suspicious, right? We have a in human languages. We use these words kind of freely but then LMS as soon as you tell us something is suspicious, it's going to start thinking in those terms. So at least in the security space, you cannot say.
 This melt this file is suspicious.
 On on what grounds. Why is it suspicious under? What contacts are scenario? Is it suspicious? Because malware is modern day. One malware has been so Advanced that they they use what we call living off the land binaries. So, instead of writing a file and you know, compiling malware into one file, they break their functionality of their malware into you know, function files that already exist on your operating system.
 So now you have, you're running Windows and you have a Powershell command and Powershell could be used by legitimate administrators to, you know, to do administrative work install and remove software.
 But also used by malware authors to kind of, you know, gain persistence or do anything. So you have this Duality and if you use language like the word suspicious in one of your prompts, or if the vendor said this file is suspicious, now the element is already triggered. The thing that this is suspicious and we needed to think like a scientific Explorer and say, why would this be suspicious and what context is this? And so, the language itself is very sensitive and you need to be able to to test different versions very quickly and see if a change of one phrase trickles downwards into a different conclusion, at the end of the investigation and you need to show
 where where you started thinking and certain way, it's quite complex and
 And this suspicious pieces because it is basically just leading the LM in to saying like oh yeah it is suspicious. Yeah. So what are the features that
 I don't know. My wife Ortho Works, in this domain of AI these days and she tells me, you have to give prompt that. The AI is going to have this notion of agreeableness. It's good. Try to agree with you, so, so because it's been trained to agree with you. You have to kind of tell it. Don't agree with me. And, and I think this is again, a counterintuitive thing, because we think it's intelligent based, but it's not. It's very nice parrot. So if you
 if you tell it, if you give it, these words are starting to
 If you, if you give it words that are triggering a particular line of thinking, you will see that it will try to agree with you and we wanted to be a scientific Explorer and, and really answer questions in almost like a scientific way. So we, so we can create proof points that are resonate with humans. So if something is suspicious in its starts with vendor security, vendor says you have a suspicious firing or computer.
 We need to corroborate that with additional external information. We can use the vendors word of suspicious to say,
 It's suspicious because if we trusted the vendors you would have a million alerts per day, like our our problem in the security space is that vendors have been optimizing for never getting it wrong and and the alert on everything that could be potentially malicious as suspicious. And we have a kind of alert fatigue and volume problem in the security space.
 So, we can't.
 Blindly trust the vendors. We always need the secondary evidence and that's part of the fun part. Here is like trying to build a system that collects secondary information to corroborate would
 you know, one system says
 tangentially related to what you're talking about back in.
 2023. When LMS first came out,
 we had the creator of airflow on here, Maxine.
 And he said, he also created Apache, super set. And at that time,
 He?
 Was on.
 A trip about how we need to treat prompts like code and less like something that we do and just kind of like throw at the wall, we need to really have the abilities to version or prompts and to understand them and then also run unit tests against them and all these things that you're talking about like the champion Challenger of the proms that we want to go out. And then, to be able to visualize how they are affecting the output in different ways.
 And at that moment in time.
 For.
 what we were the maturity that we were at and then in time was so
 Far behind this idea. But it's still is like so true to today, when you really want to make sure that you're doing everything you can so that your AI product is
 reliable.
 And it is useful to that user. Well, what do you know? Like it would be great if all of our prompts were versioned and we could figure out the lineage and we could figure out how introducing a new prompt or a new word into a prompt to fix that final product.
 I think that's very
 it's it's it's it's funny that it came from exam, you know, I think it worked on superset because I think it's it's not dissimilar from what you'd see in the kind of data analytics space. So if you look at the products that allow people to write write SQL, they all start with you. Having a kind of UI where you can post paste your SQL and hit, you know, run and get some table of results. Many of these products become analytics dashboards and maybe they support notebooks, maybe they support dashboarding. But then there's a question of why do you store the sequel? And some of them end up with
 Different methods of persisting, the queries? And eventually, someone says, could you put it in Source control? So at least the mature bi and analytics platforms will allow you to have some sort of revision or Source control functionality because you build on top of data sets and you share dashboards and you need to have some sort of source control and that pattern I think is the same with prompt management this with us. We have been storing them in you know. So it's control from day one but we've build
 Even kind of better separation between source and prompts, you know treating them as content putting them in a separate directory and having multiple layers of validation against your prompts is very useful.
 And I get those. The separation is not only for you know, having you know clean Source control trees but also because you have different personas writing. Those prompts if you're a security engineer and you're touching a prompt, it's probably nicer that. It's not embedded into a python very long variable, right? It's very it's nice if they're files, are in yaml right having your problems in a separate place and having the tools to test the prompts before you kind of commit them is
 is a nice kind of their Vex experience. I think they've
 Is not, you know, it's it's the it's what gives us velocity. If you look at, I think there was a Twitter thread the other day about you know,
 Claude called productivity. And, you know, there's someone from a topic say they do five releases per developer per day. So if you, if you think if that's real how do you
 How do you whatever definition of release is? How do you know that the releases are not breaking anything? You need to have a very solid process for testing those small prom changes. So I think that's the key part is saying okay, anyone can commit to Source control and make a problem change, but we need to have a couple of guardrails. One is like, you know, have a suite of unit tests and integration tests don't have kind of a staging environment. We can see the problems in action, you know, running and working against real life data, and once you have that, you can deploy it and give it to customers. But you have to monitor that it's still doing what you expect to do in the distribution of outcomes or what you expect them to be. And that that's like that's that's traditional observability and that's like you have your we have we have this ability stack that could have been around open source. Observability tools are very popular. You can build
 you know, tooling to kind of look for outcomes with
 with the same of his ability to use for tracking your web service availability, right? And but you need to have the business. The main knowledge in the business interests to say, I care,
 That this problem change doesn't break this outcome for this customer.
 so, part of it is not
 Because it's it's if you're a security engineer, you might not be you know expert in devops having the skill set of combining someone with the devops skill set with prompt engineering AI engineer and security engineer works together on a feature then allows them to be more independent long term. Where are you having the evals fit into all of this in this pipeline that you're talking about? So you have evolves into levels one you can evaluate on the unit tests so we can basically have unit tests, they're more like integration tests and they actually make llm calls and then we have evals on a kind of a staging environment where we can see things run against customer data and then thirdly, we have
 These llm as a judge. So let me just a very nice feature, but we don't.
 Believe that it could run.
 In.
 In line with traffic, I think my observation is that like the business needs listing our domain is that we have a very strict SLA to listen to security space. People expect you to have an investigation and then minutes later, you need to know the answer because if it's real it has a real impact on a business. And you know, we're completing with humans, the humans take, you know, 10 to, you know, 10 minutes to an hour to look at things that we have to be faster. So our evals often run asynchronously as kind of a scheduled task and they pick items from same cue and they kind of revisit them and say
 Are we happy with this conclusion? Are we happy with their set of tools that we use? Are we happy with the, you know, the hallucination level that we see here, we don't see want to see any but we were we, we can run more deeper evals out of the cycle and we can run, you know, shorter evals before you release the software or before you give it to the customers.
 And that makes sense to have yeah, these three gates. I think it's it's also there's a cost element to it like in the ideal sense.
 We we would run more but we have a limitation of both in a performance in costs. Like we can't have unlimited gating to get confidence and we there's a cost element to to having full real runs of something like our
 You know, we are limited by LM cost because we want to use the same Alum. We can't use a cheaper airline to do the eval investigations, right? Yeah. And are you having security Engineers go through and craft some type of golden data set just to have as?
 Something that the LM is a judge can reference, or is it full-on? Just give it to the llm and oh, so it's not a golden data set because we don't have we cannot use historical investigations because some of it will age out. For example if you're working on a security incident and you have data that is aged out, you can always store a none of my version and kind of and keep it as an integration unit tests. But anything that requires kind of dynamic nature will require.
 Live and fresh data because there's a timeline element to all these investigations. If you have malware today, I can't go back and, and run it on data from six months ago. It's it has to be all like fresh and and recent and available in the downstream systems or you can, or you can mock half of this system. So in our case, we try to use kind of recent date and I think there's two reasons to do. It one, is it? We live in an adversarial space as well. So attacks change all the time and part of it is because
 The security vendors change all the time, and attackers are changing the way they attack due to secure the ventures. I'll give you one example. We are very good at spotting email that has phishing text in it. If you write, you know, a text that says, you know, please it use my new bank account or here's whatever it is, the text and this is very cheap and efficient. So what attackers do is, they embed an image and they know. Now we have to bring in OCR to read the content of image. And now that your cost is more, it's more expensive for you, right? So they now, if you can read images, they will bring a PDF, so now you have to scan PDFs. So, this is an adversarial space and because of that, we can't like, you know, rest on our Laurels and say, oh, we have a test running for this. It's been, it's fine. We have to kind of keep being good at what's currently being investigated and that's where we use this. Kind of sampling approach of fresh data.
 And and try to use as real as possible attacks. And if we need to run expensive jobs, we do it on a kind of async fashion and we do tap the shoulder of security engineers and say this one wasn't good and we and we have a slack integration for to make sure that there's a human in the process. Because, you know, as this is a company we're trying to
 A tell people that we are still humans behind the product, they're not just agents and like one person a shop, you know, it's we have to create confidence, the confidence comes from like having a person that actually has the ability to to help you.
 So it's there's definitely a lot of human in the loop shepherding, these agents.
 Are you okay talking about?
 memory and shared memory because it feels like
 that would
 Make the product much better if there are.
 Certain patterns, that you're seeing, or a certain type of attack on one surface area or one client, and then now this becomes common, you can bring that back, learn it. And then the agents can reference it later, or is that just like a naive way of thinking of about it? I think so. I worked for companies that have exactly this. So previous employers,
 Where we're pitching their product. As exactly this, like you, if you, we see an attack on Against One customer, we can say the, you know, information about the attack would become threat intelligence, that we can share throughout intelligence, it's behind. And that is very powerful for yesteryears attacks. So, if you could attacks that used hashes of malware, you can see the hash of the binary. And if you see that hash, you know, it's game over and sadly for us. It's never like that. It's never that easy because it actually diverse sophisticated and they have polymorphic malware and they have living off the line binaries. So we don't
 you cannot make just threat intelligence, your only strategy to combat attacks. And what we see is that even with our customers have the best of best security policy, you can buy and they still get attacked every day. And not everything is stopped by the, you know, email security, the EDR, security and network security products that they bought and even if they bought a Sim where they aggregate all the information to one's product, that is still not.
 Successful in aggregating everything they need. So having an agent that can go to systems. That are not integrated into the same. It's very powerful.
 And in terms of, like memory, we have built mechanism for us for them to tell us about false positives, because that's more interesting than, I mean. So, I'll say, you can always have false negatives as well to our system. But it's often false positives that make the most noise, because if you have once a month, backup software, and once in a month backup software, generates an alert every time it runs. So, every month, you have a spike and of noise, and it's just nicer to have like, this Collective memory. So we have a way of saying, you know, if you see this it's okay. And that kind of is injected in an intelligent way. So they look up into the memory has to be aware of what are we looking at? So, we have a notion of artifacts in the security space so some people call them observables, but you might have an IP address or domain or file, hash, and we have a way of saying, when you see certain
 Elements in Telemetry you know look it up and see if we have something to say about them and that allows us to kind of inject this Collective knowledge that we have and we don't need to do that for attacks necessarily because attacks are by Nature suspicious and cannot be explained naively if I show you a command line.
 That is running from from a malware. You wouldn't be able to understand what it's trying to do because the authors of the malware have tried to obfuscate it. So it's already
 it's the fact that you see a physician is making you suspicious because the normal developer would not try to obfuscate their code. So we have
 multiple levels of
 You know knowledge, but we don't need to tell the AI. What is malicious.
 because,
 That is, it's almost like a common knowledge kind of, you know, it when you see it. So it's a thing, I kind of want to switch gears and just get your opinion on where you think or where you've seen.
 The most common failure mode when it comes to building agents that you're putting out into production.
 Wait. I mean we started with writing the tools in and like the wrong language. I think one of the things that we we first did is say, you know, what's a good language to write? Back-end in and we chose go which is a very nice language. I love go but it is not the best language for both coding agents to write code in and and when you're trying to build tools, so this was very two years ago, right? So today it seems funny. We have mCP, but this was before mCP. So when we tried to give tools to our first version of the LM, we tried like the wrong language and the go creates a language safety, which is very nice for software engineers.
 But what you want with their LM is really give it the maximum amount of tools. So I think one failure mode is is giving it not enough tools so it can really do the job. Like you can't expect an agent to do with a human does. If you don't give it the right set of tools. So we were kind of a little bit behind when we were trying to hand the right, all the tools and go.
 the second thing is like when you have too many tools, so,
 when you give the agent,
 You know, 1000 tools. It's not going to be able to work like you have a limitation on the memory. And as we talked about context, the format in which the LM vendors, want to know about tools is very verbose, this Json, schema that describes. The tools is not an efficient format for declaring the tools, so you basically cannot give all the tools. All the agents, all the time. You have to have an intelligent way of saying,
 What tools are relevant for the context in which you find and that's like 10 in specifics. So each customer has a different set of security software installed and in each investigation, has, you know, relevancy? So you have to apply this relevancy test. So you don't pollute all your your, you know, contacts and that's something we've all seen with even, you know, software engineering agents and I think the trend now is to use the skills just because of that the skills is kind of is like,
 It's coming from a topic, right? But it's it's something we have similar notion of filtering and they dated, you can
 Only pull into context.
 A subset of this of the functionality at the time is very appealing so that is differently. One thing and
 I think also, you know, if you look at the other Frameworks like we use length chain when it was one of their early Frameworks I think it's it's a good idea to evaluate kind of Open Source Frameworks up election when we started with not very mature and I still feel like they're doing as best as they can as fast. Moving startup, you know, popular in open source Community, but you, you have to kind of
 not get a suede by GitHub Stars, I think you have to kind of decide for yourself.
 What is the business logic you want? And how to build it and not say, I'm gonna build it using this tool just because that's easy because it's not necessarily easy. And if you look at the the pedantic agent dick framework, that the Pinette folks have released one, they basically say you probably don't need a graph. They have like in the documentation assignments. Like do you really need the graph? Maybe you do, maybe you need an orchestrator or stop agent and and agents of calls that are also agents, but maybe you don't like you have
 A minimalistic version of your app is really a good way to start. I think a lot of people they have
 They want to go to the advanced mode immediately and in our in our case we have we're like on a higher version of our system right now because we've kind of adapted multiple times.
 And it's constantly changing. So.
 Try to prototype and evaluate before you kind of commit to framework or tool.
 And it's funny about that graph piece.
 Because it feels like more and more.
 You are just getting that one agent. The sub agent architecture idea is becoming less relevant when the main agent.
 Is more powerful and can do more things.
 I I think I mean, I agree that it we wouldn't need some agents if we would be able to manage the context of the main of having one agent. So it's it's really about
 Keeping the contacts not polluted long enough to complete long-running tasks. And if you can do that, then you have to break it because that's basically our way of hacking, the memory, and the contents. But if you can, if you can have
 Good visibility into utilization of your context. You can solve a lot of issues. A lot of problems with a single agent, it's just very tricky because you your Hammer writing more prompts is, you know, destroying your agent. So you have to really find a toothpick version, not a hammer to slightly nudge it towards the outcomes you want without.
 Over-engineering, you know, a multi multi-state multi-agent, graph necessarily in the graphs know, I'm not against using graphs. I mean, if you come from the tenant's background from using airflow, and using any kind of Sparks system, you understand that graphs have a place in building detailing jobs, but
 We have seen.
 This in the security space, like, two things that don't necessarily always work. We have sore platforms where you can build like automations using, you know code. And we have seen
 platforms like, you know, orcas and temporal that are, you know, baking open source workflow engines and I think
 Those require software engineering skill set.
 But thinking about how to test those workflows.
 In the agenda context is a lot of human context to consider. So when you, when you, when you pick your your solution, you have to consider do I need just to workflow engine and write some business logic in in a general, DSL for temporal orcas or do I need an identity framework in the graph of, you know, complexity and deciding. Where do you need, like, why does it need to be? An is like, a is like, a is like a tough question because I was excited about the limbs, but I think it's it's a good question to always, ask. Does it need to be in? Can it be your business logic and in who's the person to write the business logic? Because it need to be me? Or can it be user? If you ask those questions, you will find out the many, many cases. You don't need everything to be a gentle, get a lamp. Like, you can get quite a lot of value from
 You know, very limited set of llm usage in your application.
 And then it's obviously going to make it more testable and cheaper to run. If you don't overuse agent existence. And if you have to package skills, the skills are not a necessarily, you can write skills as regular software and write, you know, same way, you write to mCP for whatever you that functionality.
 Is traditional software and it's fun to write because, you know that part of this works.