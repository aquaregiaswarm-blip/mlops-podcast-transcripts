Um, so you kind of flip the problem on its head, instead of constantly asking the questions, the model and are giving back an answer. You basically set up this lens and then you basically stream, your real-time physical AI data through the lens. And now, you get this very rich structured output that you can then build into a bigger, a bigger system, which we typically call a physical agent,
 That's right. I am. Yeah, Palo Alto. Yeah, but I would take you for more of a Celsius kind of guy. You're right. You're right. I grew up in Northern Ireland. So that's where my slightly interesting accent is coming from. Yeah, it is one that I couldn't play right off the bat.
 Yeah, I've been I've been here since 2011 is when I moved to the US and do my postdoc so yeah, I mean I've been in the US for a while. So yeah it's a bit. Yeah at this point and you've been doing cool stuff with
 Physical Ai. And I know that you were at Google you
 Did a lot of incredible things there. Have, since spun out a company, you were leading the charge with physical AI but
 I recognize that. My first thing, when I heard the word, physical AI, I thought instantly about robots.
 I imagine you. Get that a lot. Can we just clarify terms? What are you talking about? When you say that?
 Sure, sure. This is a great place to start. So I mean, robots are definitely physically. I mean, they're a key part of it, but when others, I think, in this field talk about physically AI, they may really just think about robots and maybe autonomous driving and then maybe stop there right at archetype
 We are thinking much bigger than that, and we're really thinking of any use case in the physical world in the real world where you have some form of sensor, and you need to make sense of it, right? So, just think from owner abroad that is, I mean, of course, that includes robots, it includes self-driving. But, you know, think of like a large Factory big machines. In those factories, they have hundreds of different types of sensors in them, typically to measure the, the presser pressure, and the voltage and, you know, gear, rotations per second, and all of these type of things, and they're great, indicators of what's going to happen with that machine, right? Think of all the cameras, you might have around that factory to kind of help monitor the safety of workers and you know High efficient the logistics are getting shipped over and everything else, right? So think of all the kind of the electrical grid that's physical. Yeah. Right. VR are you know like all in a thing where you have a sensory in your trying to be do projection on the world and to projection digitally on top of that, right? So it's very broad. It's a really
 Big area. And really we have a, you know, an archetype, we're trying to build this horizontal plop form the understands the physical world that customers can use to build on top of for their vertical and the breadth of customers. We have that come to us, is kind of wild.
 everything from you know, autonomous driving to you know people in factories to you know folks that want to put sensors, you know two kilometres underground in a big drill drill head or something and
 Each of these users is very different, the requirements are very different, but really the thing that unites them all is they all have some sensor in the world and they struggle to make sense of it with, you know, traditional machine, learning techniques or traditional signals that processing techniques. And they, they want, they're busy missing that intelligence layer to map sensors into reasoning and then to be able to take some action on top of that, right? So for us, physically, AI is really not intelligence layer. That's able to kind of perceive reason and act on the physical world in any form. So it's very Broad
 and you're not talking about,
 A model that will be multi-modal by.
 Default that can understand things as like higher level abstraction, you throw everything at it, whether it is Cameron data, or it sensor data that comes tabular form. It's more like specialized models in that case, for the specialized use case,
 Well, our goal is to build one big foundation model, the customers can plug lots of different sensors into and then they use natural language to love that focus it on their use case. When it comes to actually deploying the model, they may need to take that really big model and then break off a slice of it and, you know, compress it down for their use case to, you know, for example, to deploy it literally inside a drill, that goes, many kilometers on the ground. I'm not sure if you need to know every single possible fact in the world to put it underground, right? There's probably Bob Dylan song. Yes every Bob Dylan's on, right? Or you know you know all of the presidents of the US like there's a lot of knowledge there that is required initially for the foundation model to understand what's happening. But then once you know the use case you can break off a subset of that model compressive, very efficiently. And then that's the thing. You actually deploy in the real world, right? But kind of the best model or like the mothership model that the customer start with is a much
 Much bigger model that you can't plug in many many different types of sensors and then you can apply natural language to focus. What that model is paying attention to. So you know
 Typical use cases could be.
 You plug in, you know, four different cameras, for example that are overlooking the Factory and then all of the time series sensors from a big large machine inside the factory. And you focus on, like, what is the health of that machine right now, right? So imagine if a technician comes up to it and like starts to take it apart to do maintenance on it versus the gearbox and the Machine and start slowly getting out of sync without taking in all of those different sensors. Like the model has no idea. Why the gearbox rotation is changing, right? It could be because it's getting out of sync or it could be because actually know there's a person about to repair this right now and they're taking off all of the, you know, other brackets and that's causing the machine to start to vibrate more, for instance, right? So, being able to plug in all of these different modalities and use natural language to steer them all for real-time, safety, alerts, or for being able to predict when a machine, might need to be repaired or being able to actually help that technician as they're walking through the repair manual, the model can read the manual, it can walk what the
 User is doing in the world, it can look at the sensors that come out of the machine. It confuses all that together I think and I can help the technician actually move through the steps to prepare the machine safely on the right order for instance, right? So we one big model that you can essentially use natural language to control and then focus on the task in the real world and add some wild stuff. Talk to me more about building that model because that's where the fun part comes in. I imagine,
 Yeah, yeah. So when
 When when when so let me let me go back and give some context here. So I mean I've been working on sensors since the, you know, started my career even before I started my PhD it was doing a lot of stuff with real-time sensors trying to understand them initially with the signal processing techniques and heuristics and everything else. And I was I've been working on machine learning since about 2006. So, you know, long before it was cool for deep learning was even a thing. So I've been able to go through all these trams and you know it's being been a wild ride. It's been it's been a lot of fun. And before I started archetype I was a Google for almost 10 years where about a lot of my co-founders
 And you know, we've all been working on some form of sensing in the physical world for different parts of our career, right either or on the ml stock for myself for more of the signal processing stock for for Jamie my co-founder or chief science is on the Leo, who's our designers be looking at it more from the interactions tomboy and so on so forth, right? So
 when we left Google in 2023, we saw what was happening in the foundation model space, and we were able to kind of extrapolate of, like, this is gonna go well, beyond just text, all right? This is gonna go well, beyond Vision. This is gonna go potentially to every single sensor, there is in the world and like this, this is the missing link that we've all been looking for in our careers to like, actually understand, you know, what's happening in the world, right? So when we left Google,
 A chart. CBT had kind of just exploded text models were very big thing and we knew that we wanted to build a foundation model fund, you know, four sensing for the physical world that could take in any form of sensor data, right? But to actually build that model. The architecture of it is extremely difficult because the techniques you use to build a large language model or a vision. Language model are very different than the techniques that you would need to build. You know, something where you can plug in lidar, for example, right or models where you can take hundreds of different time series sensors across all of these, you know, like a like a large wind farm, for example, right? Every turbine might have hundreds of sensors, and you may be want to plug in all of those sensors across the entire winforms. So now you can see the kind of microbial patterns between each
 each of the turbines on, like the whole holistic field, right? Like just the techniques to build this, you know, didn't exist, right? So, we had a really go back to First principles and look at, like, how do we build a model where you can plug in any form of sensor and
 In most cases, it won't have nice internet sized data sets with very nice Truman. Labels annotated for them for, you know, 10 or 20 years of, you know, things you can go on script and it's like, oh, there's a big technical challenge there of how to do that. So there's no common crawl. Yeah, exactly. Um, so the
 We're able to leverage things like common crawl, you know, to start the model, but you know, that's, that's the start, that's the tip of the iceberg, right? There's just so much more that has to happen there. So, we had a big single Back to Basics on. Look at how do we take in all of these different types of sensors? All of this data that's actually not on the internet. And look at fundamental techniques that we can leverage that.
 And then use that to build very large, you know, technique to do, leverage the CM kind of recipe that you would see in an llm, right? So there's there's a big self-supervised pre-training fears. There's post training, there's RL fine tuning, and so forth. But the strategies of how you build those data sets of how you do the alignment of, how are you synchronized? Different sensors. It's a very different approach to actually make this. So, you know, if you squint out, it looks right like an llm. But um, do you actually go and build it and build a data sets? And think about how to construct this really requires to go back to basics, right?
 But our our fundamental kind of thesis is similar to what's happened in language, right? Or if you look at the original GPT you know one paper there's a very interesting
 phenomenon was picked up their work simply by learning to predict the next token.
 Over over these text sequences and there we are using, I think early, it was a script, a lot of data from, you know, Amazon reviews, right? So they were able to actually have the model predict on its own the sentiment of the review, right? Whether it was a positive sentiment or a negative sentiment, right, simply by predicting the next token in the sequence, right? Which is, which is kind of wild if you step back and think about it, that by being able to predict the next token. You know, in this arbitrary sequence. You actually start to like, pull out these very rich, sentiment models that other people in the NLP, NLP space for struggling to build, right? And suddenly this model, like, out of nowhere, is able to do it really well. So we have a similar, a similar thesis on sensor data, which is
 Simply by observing the physical world. These really large parcel models can can fundamentally start to learn the like the intrinsic principles, the governor world through these sensors and then be able to model and leverage that knowledge in the same way that nlm can, you know, read the internet and then suddenly understands, you know, Truman history, and it can, you know, generate beautiful essays and all these type of things. We could do the same thing with sensor data. So that's kind of, you know, the fundamental thesis is the same how you go about, actually building it is, you know, requires a little bit of a little bit of innovation.
 There's so many avenues that I want to go down. But the one that I think will probably be the most valuable right now for myself.
 Is the idea of the world models?
 And how that plays into what you're doing? Is there any crossover again? I'm trying to like, clarify the terms because I see things pop up on social media for a blip.
 And I kind of am following it. I understand what it is a little bit but I don't necessarily do a deep dive and then I'm graced with someone. Like your presence on this podcast and I get to go like, okay, what's going on here? Yeah, yeah, okay. This is, this is a great thing to die on. So there's definitely some overlap, but I think car, we look at World models is very different than most of the community, right? So and that's fundamentally that a lot of folks who are working on world models, um they're they're very Vision based right? There's an intrinsic bias there to take a bunch of vision
 And spatially aligned data and use that to build a almost like a digital twin. A projection of the physical world which can then actually be, it doesn't have to be the the digital twin it can be not completely simulated world but it's extremely anchored on
 You can think about as, as 4D dimensional data, where you have the 3D dimensions of the World Plus time, right? So, you can basically move around, you know, these Virtual Worlds that you can generate by observing, you know, huge amounts of video data, for example, right. And while while that's definitely, you know, a world model, I would not argue against it. The problem is it's extremely biased on Vision, right? So if you want to take another
 Sensor modality.
 And capture things in the world like CO2, for example. I mean how do you project that into this kind of very Vision dominant system, right?
 How do you, how do you take a temperature sensor on project that into into this type of system, right? And then kind of Leverage that I'm bootstrap that, right? So the way we think about world models is that we're trying to allow the core of our of our foundation models called Newton. So we're trying to learn Newton to learn this world model itself.
 From the sensor data. But actually to learn the representation of that model directly from the sensor data and not have this kind of human bias of having a very structured kind of, you know, 3D. You know, 3D form that very much bias is the model towards vision-based systems, which is, you know, right? Very useful. But to actually capture the the subtle details that are happening in the real world. You need a well beyond Vision, right? There's a lot of things in the world that like, you can't see, you can't touch you. Can't, you can't sense, right? And that a camera cannot pick up but, you know, you need, you know, and the electromagnetic spectrum, you know, all of the chemical kind of processes that might be happening, and all the things that are happening in the electrical grid. I mean, you can't see that with a camera, right? So, even even take a teapot right on the stove, unless they're steam coming out of it. You can't really tell what the temperature of the teapot is, right. You know, there's like, fundamental things that are that are missing and sure if you go into other spectrums, like infrared, for instance, then you might be
 To start to get these things. But the fundamental Point here is We Believe like these World models need to go beyond vision and you need to bring in a lots of other modalities to be able to fundamentally understand that, you know, the subtleties of what's happening in the world. And then to be able to reason about the world or to be able to act on top of it, or be able to predict what might happen in the future. For example, which is, you know, what, a lot of world models are being used for
 You need to be able to capture all of these modalities as kind of. So there's definitely an overlap of how others in the field. Look at World models. But I think we're trying to go beyond these kind of very vision-based models. That essentially are being used to build, you know, fancy computer games at this point. It's, it's cool but a lot of our customers are coming to us and they're not trying to build a computer game rendering or of their Factory. They're trying to optimize the throughput of the factory for example, right. Or they're not trying to, they're not trying to build you know, a nice digital twin of the of the Wilma Farm. They want to actually understand why you know machine number 62 has dropped 10% and efficiency today, for example, right? Things like this. You yeah.
 Is there certain data that you throw at these models that you should be waiting differently?
 Well, I guess one of the one of the the biggest ones is
 The multimodal nature of just looking at the same phenomenal in the, in the physical world. But through different modalities, I guess. That's one of the, you know, one of the things we are betting on. In terms of we're trying to build a model which is natively multimodal in multimodal art, right? So I can talk a little bit about that, but I'm multimodal in here, means you can either take multiple instances of the same sensor so it could be multiple cameras, for example, that are overlapping or non-overlapping, you know, in fact, Factory or in a townhouse car or it could be hundreds of different time series sensors and you know, one large, big industrial machine, right? So if you do to be multi instance, or it could be multi-modal, right? So you may have a camera on a pressure sensor or you have lidar on a radar and the, you know, the gearbox RPMs for example, on each of those sensors, even the very simple ones, they capture details that other types of sensors just fundamentally.
 Count right there. Are there are some to each other so by combining these different modalities into one model.
 And to be able to do deep Fusion on those sensor modalities. It allows the model to understand these fundamental relationships. That you really can't solved with just one sensor, right. And there's, there's a lot of, there's a lot of problems in the world where you need more than one sensor modality, to be able to solve it. The teapot example is like a very simple, a very simple case of that, right? The thing I mentioned before of the large machine and you start to see these anomalies in the signal, well is that because there is actually a malfunction in the machine or it's actually know someone is, the technician is actually just working on the machine or the machine next to it. And that's kind of a known anomaly. It's like, it's explained. You don't need to stop the factory today to go and fix it. It's like, yeah, that's scheduled maintenance, it's kind of in the normal thing, right? That's Scott. Yeah, it's come again. Yeah, yeah, no. If you extrapolate this out, it feels
 Like, you can.
 Have infinite amount of data sources, where do you stop?
 Well, this is it. Um, this is one of the big, you know, challenges of working on the physical world. Right. There's a very long teal required to make these system works. And, you know, folks that are working on autonomous driving know, this, right? That you have to, you know, after you've driven a few, a few hundred miles, like the system, you know, already it starts to like, in some intelligence but you have to drive billions of miles, to be able to capture, like, all the possible things that happen because, you know, frankly in the physical world. It's, it's on 24/7, right? And never stops, and most of the time,
 Not things. Things are not that interesting, right? Most intersections, if you have a bunch of sensors there, most of the time there are not accidents, thankfully, right? But like you have to look at me, you know, six months, seven months, eight months of data before you actually see, you know, that rare event happening right by Nature. It's a rare event or an anomaly. So I'm going back to your question. How much did you need? Well, you need a small amount of high quality data that captures all of these you know, different parts of the distribution. But unfortunately tough like sample. Those, you know, important moments. You do need you need to look up most of the distribution. I'm there for you. Need to look at a lot of data so a big part of what we're building an archetype. There's a lot of
 The ability to take in these huge data streams that are essentially continuous streams from the physical world and be able to very quickly triage. Those be able to catalog them, to be able to understand. You know what parts of that data is useful versus, you know what, the data you need to store and Cold Storage versus walk. You can just throw away because you know it has very little value in it, for example. So this is one of the big challenges of working when in the physical world but just like you need to. It's very hard to capture those little moments that really helped push the performance of the model forward or from an evil perspective. It takes a lot of sampling in the world to capture those hard moments that really show where your model is bricking. So that you know them, what to do next to go and approve it. Right? Yeah. By definition. There are anomalies and that's literally, you know, you know, a rare event is rare. So you have you
 Two years, three years of data before you see that event, right? So these are some of the challenges and, you know, again going back to that first principles thing. We talked about. I'm like this is not a new thing with, you know, Foundation models and everything else. Like this, this is, this is a problem with the physical world. I've had this problem in my whole career. I've been trying to solve this problem many many times and unfortunately, to solve it, you know, in the, in the previous kind of deep learning ways where everything was focused on mostly supervised methods, as opposed to, you know, self-supervised and everything else, you would have to go and get, you know, a very high-quality labeled samples for every instance, right? You'd have to go and get millions of labeled pears to be able to build these you know models you could ship when we were building radar models back at Google. Like there is no image net for radar. We have to go and build those data sets ourselves on it. We would have to go on, you know do these massive data collection camp.
 Ends and, you know, collect millions of examples of people doing motions and Gestures and picking up mobile phones. And, you know, holding them in weird ways, just to capture, like, all these nuances, and then give it to the intern good. Yeah, exactly. And like, we have these huge teams and it was, it was most of most of the effort of those big production pro products honestly are. And this is the thing. Yeah, I think I've heard about these say, this is well, when he went to Tesla, which is, you know, when you're a researcher or not Academia, you do all the fun things and, you know, you're doing all the research and you're building the papers in your working on the algorithms and, you know, you're drinking coffee and sitting and thinking deeply. And in most cases, you're lucky enough to be able to kind of, download some reference data set and work on that, and then publish a paper on it, right? When you go to Industry, it, inverts, and most of your time is built on building the protocols. Like, what's the data collection plan? You know, how are you gonna work with legal to kind of get this out? You know, what are the product requirements of the features? And the actual
 research part of our part of the system gets much smaller, you know, in terms of like what you're actually spending your time on, it's still equally as fun but you know it's a very different kind of setup problems. You have to solve like really build production models. Yeah. Well I appreciate that your mind win to the idea of the anomalies and the massive amount of data that you need in order to accomplish what you're doing in the in the form of the size of data.
 What I was thinking is the diversity of data sources and data streams. So as you're mentioning, you've got lidar, you've got temperature, you've got whatever, it's there's time series data, there is words. So, yeah, you have cameras,
 if you again,
 extrapolate that out.
 When is enough enough? Because you could say, well, maybe we need to get some kind of infrared sensor in there. Maybe we need to test the humidity of the air. Maybe we, how can you know that you have enough data points? Or can you ever are, you should you just be trying to get as many as possible? Yeah, this is a great question. Um,
 I know it's always, you know, the the dirty secret of machine learning that no one really wants to talk about particularly when they look at log log plots, is that you get these diminishing returns right in terms of how much data you put in versus, you know, how much improvement you're going to get out of the system. So I always like to think about your question, there's kind of two axes to think about it, right? One is more from a product driven axis and one is more from kind of an evaluation performance, you know, reliability, trust perspective, right? So on the product perspective, it's well, I mean, what sensors are customers coming to us with that they want to plug into this model, right? Because, you know, it's, it's all cool us, you know, being you know, very, very academic about it being like, we want to support infinite sensors but like at what point do we have a customer? Come to us and say like, oh yeah, like great, you support all my sensors, cool, you know, so we definitely have not got there yet and I think it will take as many many more years before we get there. But there is this, you know,
 Also kind of Proto distribution, where most of the customers are coming with a small subset of common sensors, right? Most of our customers come to us, they have some form of of camera Fleet and they have some form of Time series. Sensor Fleet, right? And in most cases, they, they have both and they want to have a model where they can plug both of these sensors into, right? But the classic example, I've already given, you know, you have a big Factory, you have a bunch of machines there. They have hundreds of Time series sensors in the machine, measuring pressure and RPMs. And, and, you know, oil temperature and electrical consumption and so forth.
 And they want to take the
 systems measuring those large industrial machines and they want to then fuse that with me, all cameras that are around the building that are actually looking at what, you know, higher those machines being operated by humans are hires that being connected to the kind of the machine next to it. For example or you know what was all the logistics of things moving around the factory and so forth, right? So one aspect of this is
 How rich and diverse are those sensors that customers are bringing to us, right? For this, this is where, you know, again going back to that first principles concept. This is where we're trying to build fundamental techniques. Where if a customer does come to us with a completely new type of
 A Time series sensor or camera. Ideally, we're building General and often coders that they can plug that into our model without us having to retrain it from scratch, right? Because the structure of the data is close enough to everything else. We've trained it on and the fundamental
 Processes that Those sensors are measuring or similar enough to what we've seen before. So the model already understand some of the key components and maybe you can either use are the box or you can you can tune it with a little bit of data that kind of recalibrate the model to understand your special new type of, you know, if Bosch comes out with the brand new temperature sensor tomorrow, right? We should be able to plug it into our model and it should just work, right. Um, but then there are some customers that come to us with very, very rich unique sensors like, in the RF to me. And for instance, where the word literal structure of the data itself looks nothing. Like, what we, what, we give them the model before and that's where we are trying to build general recipes where we can take a large amount of unlabeled data for that sensor, and be able to use it to build these very powerful and coders, that can then be plugged into Newton. And then you can kind of retrain Newton to pull in that new, that new model ID type, right?
 So you don't have to train them all from scratch but you will need to build a new custom encoder for, you know, this very you know, exotic type of RF sensor for example. But then if they come with another type of RF sensor, you should be able to plug that 9 into the same encoder and reuse it and bootstrap from there. So part of this is coming up with those General recipes and then building the right software, abstractions and platform Tools around it so that our customers can actually do that themselves. They can come with their own data, they can run our platform on their infrastructure, so they didn't never leave their networks, etc. Etc.
 And they can use that to build their own custom encoder without having to know any of the kind of deep science that's going on under the hood to kind of build those things. So, yeah, so that's on the that's something that kind of sensor side to quickly talk about the other aspect, the other access, which is
 when is enough data enough, it's more on the evaluation
 Access, right. So
 There. Um, this is where I've, you know, learned a bunch of hard lessons. In building production, production systems were it's very different from how Academia typically thinks about building production models, right? So you know if you open any machine learning textbook, you know chapter one, it'll be take your data. Everything is IID. Everything can be, you know, you can take a random split, 80% of it, use it for training, 20% user for testing cult grit. You know you do that and you get these great numbers. You you know publish your paper, ship them, all Etc. So fully or so easy. Yeah. Everything was that easy? So um
 When?
 When building production models, this is a really bad plan.
 Fundamentally right.
 The main reason being that the distribution that you typically capture in that data set is not fully representative of the product and the real world, right? And I've made a few mistakes in my early career where I would go out and I would have teams collect the training data first for a product and you'd be randomly splitting and, you know, keeping back subset of this, that you would be testing on and then later on you'd realize, like, you know, we're getting like 99% performance, you know, on our held our test set, you know, in our evil. But like when we try the product, its kind of crop. It's like it's the best. It's 80%, maybe it's 60% right? Oh, and I laugh, because I think everybody's been there as well. Yes. Yeah. And you know, it took me a long time to work out that. I'm the way to build a test set is not to build the training set and make the model work and then start to build a test set. It's like from day Zero, you build the test set.
 An ideally you build the test set, using a different protocol, and a different team, maybe even a different part of the country. So you really start to see the difference is between your training set and your test set and the way to build the test on this, gets to your question of how much did it is enough? Is that before you build the test set, you need to sit on and you you basically um I like this framework, you know, it's called bucketed analysis, right? Which is
 So instead of just looking at some proxy number, let's take the most obvious one like accuracy for example, right? Having accuracy over your test set is, you know, useful for exact presentations and you know, simple graphs and papers and things. But there's a lot of nuances in there in terms of like if you have a model, which is 92% accurate and you have another model, which is 92.5% accurate. Well, is the second model actually better than the first model.
 I mean you would maybe think it is, it's five you know, 0.5% better but
 In production, it may be fundamentally worse, right? And you don't know because you agree with everything into a single single number. So I really good framework that I use a lot. Is this bucketed analysis technique? Where essentially, before you start any data collection, you sit down and you break apart your problem and you try to understand like what are the key variables in the system you're building, right? That are gonna impact performance, right? And some of those are human variables, you know, height wit gender skin tone, you know, if it's if it's a wrist device versus something you're going to put on your eyes versus something, you're you know, you're holding your hand, there's going to be a bunch of things that are going to impact the performance of that system, right? Then you have to think about, you know, other aggressors. You have to think of other, you know, other key variables in the system and typically you come up with maybe six, seven eight critical variables for your system and now you can do the mathematics of. Well that's that's not a seven or eight dimensional system.
 and you can basically say, for each of those key variables,
 There is a bucket.
 Right. The captures. We need to find enough participants that are Sam fruit tall that have darker skin that have blue and green eyes. And you know really like you know cucumber sandwiches or something right? Right so now you got this bucket with represents that slice of the distribution. And then you say, okay well, what is our confidence level on that bucket? And then you can actually say well you know, if we want to be 95% confident, for example, in that performance of that bucket, that tells us how many samples we need to get of, you know, seven food people that like cucumbers. Oh no, I see. Yeah, okay. I know you can measure accuracy and F1 and you know, all your typical metrics. I'm not specific bucket. But more importantly,
 As you're because again, you should do this like day, minus one. Before you even start to project, I was you start the project. You can already start to build your test set and initially you're going to start an all those books are going to be empty, right? So now as you get data, you can basically see where which parts of the distribution, you're actually feeling up. You'll never fill that full distribution, right? It's, it's with six or seven variables. You're already a millions of of buckets. I'm for each bucket. You may need tens. Hundreds, thousands of users to be more statistically.
 Certain, right? So like that's already like everyone's data collection budget is already blown up just by this, right? So you have to use very sophisticated sampling techniques about which buckets you pick and etc. Etc. But essentially what you can do is as you are building out this test set, you can look at the performance of those buckets and now you can actually say for a specific bucket is the performance. Good enough, right?
 And once it's good enough, ideally in your training data, you tell your training team, stock collecting data for that type of thing. We're already good enough in that area, go over here, you have a torch not you can kind of shine in these kind of this analysis framework and it will help guide you and walk data you should collect, right? So this is a very powerful framework to help like structure projects and like build production, AI models because it allows you to start to see. Well, first of all, where are your gaps and testing, right? Because when you actually do that analysis, you'll be like shocked and like production systems when you do this bucketed, slicing Harden little of the distribution, you've captured, you know, in most cases you realize, oh wow, we're 95% accurate because most of our data samples on in this bucket. But actually, if we, if we normalize by her book at performance, and we basically cop, you know, once we hit a certain threshold of samples in the bucket, we can just copy the performance and not not continue to like improve.
 Performance in that bucket. And then you normalize that by all the other buckets, you realize you're not in 95 percent, you're at 60% right? Because you've also holds yes, everywhere. It's such a more clear picture of what you're looking at in as far as performance. And as you mentioned in the beginning, it's really nuanced when you're looking at performance or accuracy as just one number, as a posed to breaking it up into these buckets. Yeah. And then of course, you cannot get all those together to get one nice metric that you can track over time. So you can like for a bigger team you know you can't go into that level of detail and you can actually like is your is your performance improving over time is your coverage in those tests book, it's improving over time. Normally, what you see is this kind of this kind of Saul tooth way of like Trend where the performance increases and then you basically expand to a whole new set of variables that either. You, you couldn't collect in the early days or maybe you're not even know about when you started the company. You
 Just discovered like, oh, actually this, you know, like I'm whether the person exercises are not before they use the product, actually impacts the performance of the product, okay? That's another variable. We have to add seriously, performance will increase, and then you'll hit this neck and performance will drop. And then we'll increase again and then it will draw up and, you know, over over 6 12, you know, 18 months, you'll build, the actual performance, you need to really launch these things. So to go back, this is a long way of saying to go back of how much data is enough I think.
 It's either when the customer tell you, you have covered all the sensors or it's when you've actually looked like you have these very rich, you know, evolved slices and you can actually say for for the product, we're trying to ship this is the performance, we need to hit. And now, we're really confident, like, for 95% of all these buckets that we do have, the True Performance numbers on, you know, where the system is going, right? And typically, you never, you never reached that, because you always add in more complexity, there's always in a new feature, you want to add or, you know, there's always something that will have one more Dimension that, you know, keeps growing the system, right? Exactly. It's goes back to that whole thing of your
 doing a ton of R&D, I'm sure.
 And your also creating a product that folks are are using on a daily basis.
 Yeah, I don't know, honestly, that's the type of team or company or, you know, that's, that's where I really loved. I loved to be on those edges of, you know, going from zero to one and then really taking that and building a product around it. And then, you know, building like a rich set of features and things that people love on top, on top of that core R&D. But yeah, it really starts from like a core like piece of technology. And and that requires, you know, normally some significant Innovation to work out, how you're going to solve that. Yeah. And this is where I should mention that. I'm, you know, one of one of our tricks of how we do that with my co-founders is so I mentioned, you know, Jimmy, who's you know, my long-term collaborator and our chief scientist and we're kind of both on the, on the technical side of the team, right? But then we have
 We have Leo who's our chief designer. So he's coming in and really looking at things from an interaction standpoint and then we have Brandon, who's our CEO and really, he's like
 He's just like the best operator I've ever met and he's just amazing at like smashing through walls and just getting things done. And, you know, being able to like really understand more product needs to be built and like just been building teams and executing and, you know, changing laws, when they need to be changed to allow things to get chipped and all this stuff, right? And then we have a van who's our CEO, who is rce, who is, you know, kind of the glue of he's able to look at all these things and kind of bring them together and like our big secret is while we're building fundamental technology. And we're always building it from the growing up, from a technology standpoint from the very start, we bring in design and interaction and we really think about how people will use the technology and then we use that both to kind of guide where the technology is going. But also to put some strong constraints on the technology itself to allow us to actually ship it, right? Because if you don't put those constraints on it, you're just
 Trying to, you're trying to boil the ocean and solve every problem at once and it's it's it's it's it's too much right? You just you can't you know you there's always this like volley of death. That R&D goes to die in, right? Or like you try to take an R&D project and you typically pass it to a production team and they'll take it and ship it. And you know that doesn't go well for like very new technology. You really have to, you know, make sure that that's successful. I'm one of the tricks of making this happen is making sure that you know we use interaction as a way to kind of put some strong constraints on this and really think about how people will use the technology and how to kind of guide them and put you know make it easier for the model itself. That's all these problems Etc. Well, that's a great segue into what we were talking about before we hit record on the construction site and the use case around that.
 Can you?
 Connect those dots of the interaction, how you ship something, so that you had those constraints with this use case of you've, now got sensors and all of this multimodal data for a construction site. Yeah, that's a great. That's a great. That's a great.
 A segue. So construction is a, is a really good example of why doing things in the physical world is hard, right? So we worked with the Japanese construction, you know, this Mammoth company. They're fundamental, they're, they're kajima. If you've ever been to Japan, they're the company that built the island that the airport. Sits on top of, they didn't they didn't build the airport. They built the island that the airport is, you know, is resting on top of in Japan phenomenal. So I'm they build, you know, Bridges and tunnels and islands and you know, buildings and everything right? And they we work with them on a five-year project and we come in at the end where they were basically moving a river, right? As one does on a normal day, right? And to avoid flooding, and they essentially had four or five years worth of data,
 Of all of these cameras placed around different parts of the river. As the construction teams, put these large excavators on barges floor to the mode in the river. And basically, we're dredging the river to, to busy widen and deepen, the river to, you know, avoid flooding, right? And there are big goal was to understand from that data. How to estimate the the throughput of each of the the dredging teams, right? Like you know the team how to schedule they were supposed to dredge 6 hours that day. Did they actually do six hours or did the two or did they do zero because of maintenance issues or weather or etc? Etc. And this is a great example of why doing AI in the real world is hard because like a construction site is literally different every day. Right? The thing that was safe yesterday is no longer shift today, right? Because they've moved the bridge a little bit further, right? You know, over the over the river or you know, that system is now. You know, that wall is not three meters higher than it was before. So if you fell off it yesterday, you'd be okay. If you fall off of today, you're gonna break your
 Right, like these type of things. So you have whether you have, you have very large sites that are 25 square kilometers and you know, the the excavator is not just barely visible, you know, in the in the farthest Realms of the camera and you have to get the model to be able to understand this. So a few things we did to really use interaction to solve this. I'm one of the big Concepts we have in our model is called a lens. And you can think of this as the ability to instead of like a chop ball where you ask a question and give you back on answer you ask a question and give you my kind of answer. You can actually set up a lens and connect a real-time data stream to it. And in the lands will constantly analyze the data with whatever Focus you've put on that lands, right? So, take this simple example, you want to work out if you're packages arrived at your home.
 You don't want to keep having to ask your doorbell, like, did my package, it right? Did my package, it right? Did my package arrive, right? You want to just say, like notify me when my package arrives today? So, you set up a lens that will continue on, and then when something happens, it can actually output an answer, right? So we use this interaction technique for construction, where and stead of constantly having to ask the model, like, what's the efficiency of the work team today? You set up this lands before you before you get the data.
 And now you actually run that lens on either pre-recorded videos or live data streams. Another model. As it's running will actually basically output. This Rich structured response of you know, is the work team resting, you know, is the barge on the river, is it actually dredging or is it being moved? You know. Etc, etc. Um, so you kind of flip the problem on its head instead of constantly asking the questions, the model and a giving back an answer, you basically set up this lens and then you basically stream, your real-time physical, AI data, through the lands. And now you get this very rich structured output that you can then build into a big old, a bigger system which we typically call a physical agent and that's the system, you cannot actually deploy where you basically, deploy. These physical agents in the real world that will. In real time measure the productivity of the construction teams for example, right? And it will give you back notifications and responses, but you don't have to keep asking a question of like, you know, is the team operating? Well,
 Today for example. So lens is a way for you to tell the model. What you care about? Yes beforehand. Exactly. And then it's it's interactive, right? So it's real time. So you're going to adjust the focus in real time. You can ask a questions if you want to and it can give you back the answers. But it will, it'll essentially you give the you give the the model some explicit goal of like what you're interested in and the model. Now I can on its own autonomously analyze that and then work with you to actually like give you back a report or like send you a notification, or you can ask it if follow-up question and it can then give you the answer, but it just have to do it from scratch, right? And then of course, you can have dozens of lands as running on the same sensors, train to look at different aspects of the data.
 That's pretty wild. So I can get a daily report of how my construction site is going exactly or, or so, you can get a daily report or you can have if a worker happens to go up and more on the rigs and is in it, and it'll immediately notify you on your pager and say like, hey, you know, you know, Claire is not climbed up on like a danger, just like pay more attention to her to make sure that she's safe, for example, right? Yeah. Or you can say, like, here's here's the work. This section of the river,
 Two months later. Here is none of the productivity of this section of the river. Why is one section 20 percent lower productivity like, help me analyze this and understand this big? Because it's Scott Scott's over there, which is a little side gig over there, and it happens to be winter, which means it was a lot more rainfall, which means the river is much higher, which means there's a lot more dredge kind of, you know, dirt, in it, etc. Etc, But ultimately, it's got we should move Scott. Wait and so it will be able to extrapolate that kind of information of. Hey, there's all this stuff that's happening. Yeah. And this we have a confidence level. I'm sure that we think it's because of these things. Exactly. So the I'm joking about whether but actually that's a that's a that's a real thing on my project where we fused him from these multimodal camera rigs where they're all capturing different aspects of the work we fused in the local weather station report. So we got Rim full, we get temperature. All those things we
 Refused. Then the, the, the, the, the, the hydrometer sensors that are on the river. That actually tell you the height of the river, right? And things like water flow and things like this. We fuse these all into Newton and now you can actually ask questions like, you know, show me the top, you know, 10, top 10% of teams that are working, you know, this month, show me the bottom 10%, Like, what's, what's the difference between these two teams? You know, I can actually go and tell you, oh, because of the weather, there was a storm, three days before and it takes several days for that storm, to kind of impact Downstream, right? But because of that, even though the team was able to go and work that day, it was a bit wet, but it wasn't that bad. But because of the storm two days before, it's actually not decreased the productivity of the of the system, all right? So in a great example, again of that multimodal data where you're bringing in the weather data plus the Commerce, and now you can combine it together and look at more than just five minutes of data, you can look at many days of data, and
 because of that, you can start to see these bigger patterns, that even the engineers on the site, were able to pick up. Yeah. So what do they do with these insights?
 Well, mostly mostly, they're using this for to help them to help them the way they structure their teams. A lot of the work has been done by subcontractors, right? So a lot of, this is from a compliance standpoint to make sure that the team their subcontracting and they're they're getting the work, you know, for example, a six-star shift to go on dredge, like, are they actually going and doing six hours of dredging and if they're not, is it because of the weather, or is it because the drill bit broke that day and they had to spend two hours, repairing it or is it because you know Sam didn't turn up the work that day and the team wasn't, you know, it didn't have the barge driver and therefore it couldn't do the work and then based on that part of you, then go and start to optimize the site so that. Oh, next time we see a storm coming, that's actually just have the team go and work on this section of the site instead, because they're going to be a lot more. They're going to be a lot more productive for instance. Yeah. And we'll go back to that area of the river where, we know, four days after the storm, it's okay to dredge there. For instance, right?
 Wow, dude, this is so fascinating.
 I think we'll end it there. Unless is there anything that I didn't ask you?
 That you wanted to talk about.
 Ah, well, I think that it's been a fun conversation. I love to talk about this stuff. I guess the, the final thing I can end on is, you know, we have a huge amount of customers coming in. You can probably tell that the team is growing quite a bit. So we are hiring. If folks, you know, want to come and work in physical AI, you know, hit me up on LinkedIn or maybe we can we can attach our careers page to see, you know, because yeah, there's a lot of work to be done and, you know, a lot of very cool things happening on the physical world and I think in the next few years, this area, this, this is gonna is gonna explode. It's gonna be huge. So, yeah, you know, come and join us. Come and work on very cool. Technology in the real world. Is it hiring in the Bay Area?
 Hiring across its in the bay on in Europe. So yeah, we're, we're hiring a team across where we're just distributed company. So yeah, we have folks, we have folks all around the world. The two major axis is, you know, in the US on, and in Europe, but yes, and Engineering sales, all yes, everything engineering seals research,