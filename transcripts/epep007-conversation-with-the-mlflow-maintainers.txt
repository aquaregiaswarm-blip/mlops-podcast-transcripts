All right, folks, I'm super excited for this conversation that I just had with jewels, Danny and Corey. They are on the mlflow team. They are veterans in the space. Jewels literally wrote the book on spark. Now he is leading the developer relations side of mlflow and Danny and Corey are both lead Engineers. Crafting what we know and love about this classic tool and they talked about how mlflow is leading with open source. They really want to go all in, on the open source mlflow and bring it to this new agent Paradigm that were in enough. Amy Chen, let's get into the conversation with it.
 On every day that I get to hang out with so many incredible people. I am Grace by, not one, not two, but three of the mlflow team, how y'all doing today? I'm super stoked to chat about agents in production. You guys have been doing a lot with it and I think each one of you individually has been on this podcast before just in different phases of your lives. It's cool to see you all working on mlflow and doing things in this new agent era.
 Absolutely no. It I think all of us have come a long way and at the same time some of us are continuing to focus on developer experience for AI in a radically different domain. Models to agents feature stores to agents, as it's kind of exciting. Thanks for having us back.
 Yes, we will. We will, we will come a long way from from spark to mlflow.
 And back to spark and back to him a flows. I'm delighted to be working again with Danny. And with Corey, and mostly talking to you. We have had many interactions. I just love you me. Serenading on, on the stage with hero, killer that was that was the highlight. Absolutely. That was the highlight.
 Yeah, well, I feel like a lot of this is reflected just with, it's not just like with like what the three of us I've been doing is also, like apple Ops Community is like been shifting as well. Like we've we kind of all working in various versions of the classic ml world before and then kind of all shifted over to this like, Danny, I Asians kind of opiates space, right? So yeah, it involved real quick, and
 So, speaking of evolving,
 Like mlflow has evolved a ton. You guys have been seeing folks that are putting their agents in a production. I would love to start out with
 Just, what have you been seeing out there? Each one of you comes at it, from a different perspective, so maybe we can get your takes on the conversations, you're having each day.
 Yeah. 100%, I think as we talk about agents in production, the sort of First Natural question to ask is, what are people trying to put in production? What are they building? And what are they deploying? And
 After talking to a lot of our customers open source, users can pretty confidently declare that the age of the chatbots is is still here. It ain't going anywhere that the bulk of our customers. The bulk of our open source users. They're building chatbots. Yes, you know, some of them that started as text only bots a couple years ago, have become multimodal. We do get the occasional like voice application, folks are doing speech to text and then they get the text back and they go to text to speech, some are processing images but the end of the day like that chat interface remains pretty ubiquitous like well over 50 or 60% of use cases which is is massive. The other thing that I'd say about chatbots and there are a bunch of other use cases, all of Danny and others talk about is the pretty much all tool calling chatbots. Now we have the age of rag up until about a year ago. Everybody was really excited about
 Document retrieval specifically. And now, yeah, folks are continuing to retrieve documents. They're gathering information from knowledge bases, their indexing them. But they're doing so much more around that as well. You know, they're, they're making real-time API calls to other services, other services to fetch information. They're even having their chatbots take various actions. I'm gonna go ahead and save some information about this user to a knowledge base. So, there's not just read, there's a little bit of Rights coming in as well. So this chatbot interface is remain somewhat consistent, maybe he's become multimodal, but then under the hood, the architecture is as evolved quite a bit. So thanks to the powerful models. That seem to be being released every week or two. Now, I think that's what you can probably attribute to a lot of the upgrades behind the scenes. As so that's my pitch on on chatbot, use cases and Danny Jewels. What else are we seeing? Yeah, well, just so elaborate on that. I think chatbots of what I've seen.
 Is chop also getting more and more sophisticated. So maybe an initial is just like text based type Bots. Now we're seeing like they're there's a lot more like Rich kind of return types. Like maybe like we're HTML, that's being returned or like more human in the loops or within this agent. Now, you're adding like, kind of multi-agent systems or but it still look at chat interface to the user or there's a basically. Yeah, the infrastructure underneath people are building more complex, like, guardrails within the Asians, you know, just to as they get closer and flips for production or working with higher Stakes, use cases. I think people are now just beginning to get some of those like, oh, you can't just wanna learn something production, kind of use case and a production out. So that's been cold to see.
 and,
 yeah, I've seen over let me jump in there Danny because
 When you talk about the advanced, the chatbot is a little bit more advanced. I don't know if you guys saw this website called AI, ux design patterns, it is brilliant. It gives you all these different ways that
 folks are leveling up chat and
 right now, a lot of them are just kind of Concepts but you have things like you can highlight certain parts of the response in a chat and then expand on only that part, you know, which is so nice as a user experience because a lot of times you want to say like oh yeah, I like all of this but I don't like that part or I've also seen
 Designs where they'll come back with a carousel of items for you. That you can scroll through like we're natively used to using as a posed, to only chatting, chat out that type of thing. But yeah, you were gonna say something else Danny so hit me with what else, you know, well, that kind of reminds me of different things that will touch on as well. But I was going to say,
 even though all the chatbots are getting more more complex, it's surprising to me how
 A nascent, the observability, and eval space has been for for these chatbots. Like, you'll notice that the current, this course is all about like multi-turn evaluation assimilation. And how like basically, every vendor or solution still is like, really, just kind of began to get the free wet but like multi trends like conversations and chatbots are naturally multi-turn. So like you know, why isn't there been kind of more robust Tooling in that space? And really a lot of customers had just work around that and build their own tooling that's like super bespoke so yeah, that's been interesting. Yeah. I mean I'm in told me to but Corey and Danny say they actually begin to see when you my conversations every little Brazil. There, the chatbot is to be the favorite one and in is Danny pointed out more of these evaluation judges being written.
 For example, deep eval and ragas do provide very specific board built-in as well as custom judges, you can actually do multi-tone evaluations. So we begin to see a more of that in that particular area where chatbots are being written. But they begin to fill the confident that they can actually write these judges to evaluate the multi-toned sessions to evaluate the entire conversation. And that somehow seems to be the, the trend, at least from what I've seen from developers trying to ask questions or or hit me with, you know, how are you actually evaluating your chatbot in production? I'm using my, I'm using either the mlflow. I'm using some framework that actually has a host of built-in judges. That allow me to
 Gauge a particular metric or I can write my custom judges along with agent as a judge. So we begin to see quite a bit of that trend.
 Yeah. But yeah, I think it people are being talking about turn judges now but not because not because like they didn't need it. Like I think as soon as we launch multi-turn judges always started talking about people like yeah, I need that. That's like the first thing I want to start with. It's like, you know, I want to know is my tap is our humans, like escalating to get like, you know, people trying to get escalated human support, are they as the agent repeating it over and over again? Like all this, like common, like Asian problems, you really start seeing in the context of a conversation rather than a single turn and really as soon as we lost people were like cool. Yeah. We're gonna use that immediately. We needed that yesterday.
 Yeah, an example of this, I like to use a fair bit, is determining, whether an agent is being repetitive. I can't figure that out. Just by looking at one, you know, input output, no clue. But if I can see, you know, 20, turn conversation. I got this agent, keeps saying the same thing. It's giving me the same answer. I asked for more information and it continues to respond with basically the same stuff, rephrased, it becomes a lot more obvious and so that's where I can't figure that out. If I have this little function, those thing called a Judge is looking turn by turn. I have to have the bigger picture. Same thing comes with human feedback though that, you know, a lot of times folks, rightfully, so are using their end users or internal testers and whatnot to determine whether a chapped butt meets their needs and they need that conversational feedback. It was indicating that this particular message didn't make a ton of sense is that kind of helpful but it's
 Much more useful. If I can say this entire interaction should have been half as long or you know I I got this is what I was trying to achieve with this interaction and this is where I got and what was missing. So I think there's an element of human feedback. There's there are these questions about how I automate the process of finding issues in these multi-turn interactions and bugs in my agent and it's funny. It's taken as long this long to get here and that everything started single turn input output because this chatbot interface is especially this multi-turn thing was the first thing launched. You know, November of three years ago. Now, you know, Chad GT was a multi-turn chat interface. And so, it's kind of funny that we've finally started to catch up. And in terms of the evaluation and the quality measurement side,
 Cut you touched on two very sale in point one is the repetition, right? How do I know that if I had, if I had a 20 turned conversation, something was not being repeated, and that had was a logical consistency. Is there a logical flow between each and every steps? Or am I being consistent or I've been trying to hurry. I think those are the important elements where the judges can I should help to detect in the multitude and conversation. One the coherence is there, a logical flow B is a context. Be retention. I referred to something, you know, 20 turns before
 Is my agent or is my conversation board has has the ability to remember that in the context. So I think those are the important thing. And that actually collectively, those meaty make the the chatbot, more robust and and and, and, and more cohesive. Yeah, so you touched on that important Point. Yeah, because you kind of take it for granted Us in this field that we understand. Oh.
 potentially a lot of times to get the best result, we should just start a new chat and clear the context window but that's not
 Like, I would guess that your average user off the street doesn't know that.
 And so how can you make sure that what you're talking about jewels that stays true? Like if somebody wants to reference back and you're not
 doing it properly, you know that and you have a way to either be in the janitor of that context or
 Encourage them to start a new chat or do something because that just leads to poor user experience 100%. And I think it dovetails nicely with this whole topic of memory management for agents. That everybody seems very rightfully excited about the idea that a very long message. History gives an agent a ton of Rich context, if I've been talking with a chatbot for 15 minutes and then I asked it to do something that follows naturally from that 15 minutes conversation, I much more likely. I would guess to, to get the result that I'm looking for at the same time there, is that trapdoor Pitfall? That? I think you rightly point out, where, if I'm asking for something totally different, you know, that 15 minutes of contacts can hurt more than it helps. And so to me, I have you would as a problem of memory and State Management in an agent. So across all the conversations that I've had with this,
 particular chatbot when I go and I asked for something whether it's in the middle of an active conversation or a brand new one,
 what information about me or about my Intense or previous interactions is this agent drawing from and that's something that it were taking a pretty close look at at databricks, and from a research perspective, as well. And, and I think it's an important topic for the industry at large agent memory. Yeah. Yeah. And how do you wait? Memories. Exactly core. You referring to the short term memory and long-term memory where the 15-minute conversation has has just passed and nothing has been diluted with other conversation that 15 minute turn can actually put in a long-term memory and can be rude On Demand. Right. And the short term memory is just like, okay, I'm going to flush this out and I'm going to start a new session, but I'll keep that in short to memory. If there is a reference to sudden thing that I talked about, previously that doesn't exist in my short-term memory. I'm gonna go and refer to the long term memory to see if the context is there. And that way, is you say it? It maintains
 A good flow and you as a user has a better experience to say, okay you know this guy remembers everything for the last half an hour that actually had. So it's like a human really be paying attention to you. And and referring to something that you had asked about 15 to 20 minutes. Half an hour ago that is and that is like a moment. Oh wow, this is a brilliant. I can't I can't believe half an hour half an hour ago I asked you to do something and I refer to it. Now we actually comes back and gives me a full answer. That is definitely
 Aha moment, magical.
 I'll tell you what though, when I encounter chatbots in the wild and they don't work that well
 I am very adamant caps locks and everything saying how bad this damn bought is, and I just like, I'm trying to fly it, so whoever is doing evals on the other side. They see that like, oh, maybe we should try and make it better in this area. Yeah, it's funny because one customer that we worked with, they were kind of like they lost a production without really doing too much for us though. And like, at some point they were looking at all. They're like responses that's just like
 Like, I think most of their chats were just people, swearing out the chatbot, and so really, it was like trying to serve a different purpose, but ended up just being a therapy chatbot for all their users.
 But, but interesting Danny when he, when you become abusive to the chat bone, if you have a judge that actually did that, that has the frustration. Like, for example, in the middle floor, we have the built-in evaluation call user frustration, right? And so you detects it, detects your language and tonality and based on that it would actually respond sometimes it surprises. You when, you know, you were frustrated about this topic and you would come back with a nice polite answer to mitigate the frustration. I think that's an interesting way. How chat boards can somehow appear, like human interaction because of the fact that we actually have this old both in as well as custom judges, you can actually write and you can specify in tailored to your to your, to your chatbot, right? They have the chatbot is talking about insects then in everything dealing with insects would be sort of mortality. So,
 The other piece of this Beyond yelling at the chatbot is the degree to, which folks are collecting like, thumbs up thumbs down, categorical and freeform feedback from their chatbots. I, I notice that a lot of folks. The first time they deploy to production
 I haven't really thought about collecting those signals, as much as you might expect. As a result, you know, Demetrios you resort to caps locking and yelling at an agent in the chat window but it's often High signal for developers. If they can get a more structured set of feedback if they can get thumbs up, thumbs down, or they can get a one-to-five rating and they can they can carve out a separate text window or field for an explanation about why things are going well or where that they or where they can improve. And I think a lot of folks treat that as an afterthought and it's a little bit surprising at least to me because it's pretty fundamental to a figuring out if you deployed something that working reasonably well and then be automating that process of fixing and identifying quality issues in the future. It's hard to sort of automate the detection of a real quality problem without having some early signal from your users about what the quality problems are. So,
 The the upshot here would be feedback, collection is massively important and to some extent under invested in in the chat body ecosystem. And the interesting thing is it's very easy to build all the stuff that folks are doing around stitching together. You know, tool calling chatbots with knowledge databases and external apis and putting that an awesome web, front, end, storing data securely, all that stuff is hard. Yeah, collecting a little bit of end user feedback is pretty straightforward and I'll shamelessly plug the mlflow feedback collection apis as being a particularly low friction and easy products surface to use. But the point is like spend that extra hour that's that's really all it should take to wire up some and user feedback to understand the quality of your agents.
 Who you touch them, very important Point, how do we actually take the human feedback and not and and operationalize it from your, you know, from your experience talking to customers even though it is, how do you think they actually doing it? So when they have the ammo flow deployed and they're going through the UI to additional feedback, how does it get injected into?
 Into the into the traces so they can improve that. Yeah, it's a great question. So what you have with most deployed agents where you've integrated, some amount of tracing is effectively this massive log statements, whether you collected them with mlflow or a variety of other Solutions you you have thousands hundreds of thousands even millions of these time. Stamped logs you can open them up and you can see this statement by statement. Stack Trace what feedback helps you do is filter those. So I want to find the needles in this Haystack. You know, most of my hundreds of thousands of interactions aren't necessarily going to be labeled. The quality might be reasonably good. If folks didn't take the time to express their frustration or, you know, provide some feedback. And so, what I would like to quickly do is ignore all the stuff that I know is working. Well, I don't have to look at closely. I want to click in this UI. Apply a filter for certain types of
 Negative feedback. For example, user frustration is is high, okay? I'm gonna apply that filter or the completeness of whatever task the agent was being asked to perform is low, it's only providing partial information or not fully meeting the users objective that condenses this log of thousands and hundreds of thousands of of Stack traces into a few things that I can, then analyze more deeply and start to root. Cause it's great to understand that the user was frustrated, why? Well, now I have to actually take a look at those particular log statements, then in the future, once I've uncovered that root cause and I've started Implement a fix, I don't have to go back to my end user and say, is this better now. I should be able to like take their their feedback and turn it into some sort of unit tests and that's where these like evals, you know, automated techniques, really help. So that's sort of the pipeline that I see it starts at
 The feedback collection and proceeds to filtering. There is some human debugging and then you're fixing with these automated unit tests that you can build.
 And those are unit tests, somehow help us to improve the prompt as well, right? Not to not, not to get repeat the same rut again, not to get into the same frustration Loop and all that. Yeah, I think there's a variety of solutions out there. So prompt optimization is certainly one prompts and still remain. I think a major determining factor of agent quality two or three years ago everybody was talking about hiring professional prompt Engineers a year after that. It was well you know this prompt engineering thing is brittle, we should look at fine-tuning. We you know there are other techniques that that may serve us better and get us further. It's like prompt engineering though is sort of Romaine front and center for a lot of agent developers. So you're absolutely right and the process of fixing an issue. I may want to do a fair bit of prompt engineering to get the agents to make better decisions. Retrieve information from a knowledge Source more, proactively issue, queries against the web service,
 Somewhat differently and Beyond, or take on a different. Persona, these are all valid use cases for prompt engineering and so something that we've been investing in quite a bit in mlflow is making that quick and effective. And I've personally spent days prompt, engineering Voice Assistant that I build at home and it can be agonizing, wouldn't it be great if there was some automated technique that I could use to make that process a little more efficient and that's where, you know the research
 Folks a databricks, but all just more broadly in Academia. I have been pushing forward. This concept of prompt optimization, so given the collection of negative feedback giving some information about what isn't working in an agent and give us a set of prompts. How can I come up with better? The versions of those problems that that mitigate and fix those issues? And can I treat this basically like a optimization job and an automated automated solution rather than this human in the Loop English instruction modifications thing? Where you know, I don't know if I'm making deterministic progress and I might be spending like hours or days coming up with better versions of my prompt.
 He took another this way, the Gap help to decision comes into play, where you can actually, optimize your prompt engineering using the, the get power optimization is part of the mlflow library. So,
 One of the things, as you mentioned that when you actually want to automate that, the problem engineering part would be okay. Here's a, here's a set of prompts running through the Gap. Run it through the Gap optimization to get the best bomb to see how how you can actually do that. One thing that I find interesting, when it comes to,
 Trying to optimize your prompts.
 From.
 When you're out there and you're trying to add all these edge cases, you can quickly get into a place where you've got this massive prompt because you've added all these edge cases in and then you end up destroying your usability of the agent since you tried to cover for all this. And I know I've seen folks that will
 focus on, like dynamically adding in or injecting in the context, and that's why we
 Have all heard, you know, the rise of the context engineer Because You Gotta Throw the right context in at the right time. And you really want to have
 not more than you need. It's if it's anything, it's like just enough but not too much.
 It certainly and if you're not careful like you said, you end up rewriting a rule-based system, except this time you're not writing it in python or typescript or Java or whatever you're writing it in English and you know, that is effectively becomes lower quality and more brittle almost than just running it in code. So, you're 100% right? And that's where some of these optimization techniques and Optimizer like a Frameworks, like DS, Pi that we try to integrate closely with. And mlflow can help you avoid that, because they're, you have to force them to generalize across a whole bunch of samples at once. So you can't necessarily overfit sample by sample by sample. Ultimately, you know, you're that Optimizer is not going to be able to produce good quality, that way across thousands of of test cases. So that's certainly one way to arrive at a prompt that a fixes your problem, but B avoids, that overfitting and sort of rule-based prompting the quickly.
 Really get out of hands. I that really resonates, absolutely.
 Yeah, I don't know if we've seen our own kind of like customers hit the case where they're just like adding so much stuff into the prompt that it's like really polluting the conversation. I think they, they're still more in the state of we've thrown something in there and we're still trying to figure out how good is the, the Asian in general. But, yeah, also I feel like people have stopped short of that because they are cost conscious as well. I didn't want to just blow their L and kind of token budget and that is something they're tracking very closely. So, you know, a lot of construct, this like massive prompts that just continuously sucks all your budget every single turn and you were, you were talking about something before gory that I think is fascinating when it comes to the feedback collection and how
 like you normally need to bring in the subject matter experts for that type of a thing. When you get that feedback, it's great from users. If they raise their hand and they're kind enough to take the time and tell you what's wrong with it. But a lot of times when you're looking at these multi-turn conversations, you're just gonna bring in a subject matter expert and say like is it doing what we want it to be doing. And I've heard a huge complaint from folks about how subject matter experts don't want to be looking at like Jason they in fact that can't really be doing that. So you have to figure out how can I give these folks the data in a way that they can properly annotate it and
 Add value to it. That isn't just like logs dump.
 100%. Absolutely. And that's something that we've spent quite a bit of time at databricks and in the mlflow open source Community, the investing in attempting to address that challenge of as a domain expert. How do I quickly like, recreate that same view that that end user had overlay. Some extra information that helps the domain expert, get to the bottom of what went wrong and then very quickly provide their expertise. And so that's a matter of, if I have a chatbot out there and users said, it is really isn't going well, you know, dimitrios, I'm yelling at the thing, I'm frustrated, whatever. I as I then call in my domain expert, Danny, or jewels, and their reviewing this interaction. They're trying to figure out whether this agents that's designed to answer questions about like Telecom customers or something. Is what is it? That is making Demetrios so frustrated, they should be able to
 Replay that exact chat. They should have access to the turn-by-turn chat, first and foremost, but then oftentimes, that isn't 100% sufficient, they should also be able to see, okay, this particular turn of the chat, the agent went and accessed information, where did it access? Okay, it accessed information about the user's account, rather than about the line of products dimitrios was trying to get a new phone. He's not asking about his account or billing related questions. So if I can give Donnie and jewels that information in a UI and they can look through it and very quickly go out, okay, this thing is fetching, the wrong info. Then that's very, it's very easy for them to come back and say, all right, I found the source of dimitrios frustration. It was, you know, the sort of knowledge retrieval problem and so in databricks, we've invested quite a bit of time building this review application where I can collect a bunch of these chat sessions and I can send them over to domain.
 It's in my organization they get back that full chat view with the overlay of what information was retrieved that way. They can dive deeper and they get this awesome feedback form where they can provide root cause analysis which then helps the developer go back and fix their agent. They can then send that review app to any other member of their organization and we've been really invested and actively working on bringing this into the open source Community as well. The vision is the anybody running open source on all flow, should be able to spin up exactly the same UI. Send it to any other member of their organization and start collecting these labels. And so, that's a problem that hits home is deeply. I think personal to all of us on this call at this point. What? Maybe it's interesting to talk for a second about how mlflow you traditionally were building for data scientists. Right. And now do you feel like the Persona has changed?
 And if so who are you building for? There's more stakeholders involved. What does it look like now?
 100%. We we are focused on continuing to serve that community of data scientists who want to build. Awesome, high-quality models, and to make clear, that's not going anywhere and is super important to us. At the same time, what we found a couple of years ago is there's so much Synergy between agent developments and model developments. You're going through this process of building an initial version of in AI application, you're measuring, its quality. You're iterating on your managing the deployments of that thing. You're collecting signals from your end users. Once you've deployed it. Yeah, that's true. Whether you're building an agent or a model and so we felt like wow we can we can take on this problem, we can start to addressing the needs of agent of developers but as you dive in the next order of detail around okay how do I measure quality? How do I deploy this thing and understand whether its operating well and and production as we've talked about looks a fair
 Different. And so that's where you know you have to serve your agent developer who's building that initial version of an agent by giving them logs and observability and stack traces so they can debug the same way that you serve the model developer by giving them a way to track train tests and Val loss during the training process. It's you you're meeting similar needs but you're doing it with different tooling and then at the point of I'm trying to evaluate and improve quality. Okay well the the for the model developer again, they're going to take a look at that test loss and they're going to go and do feature engineering and things like that. But for the agent developer to evaluate quality, they need to go talk to a domain expert within their organization, or they need to have these awesome out of the box, llm judges that can look at natural language response and determine whether it needs the needs of the user who sent the query or not.
 That. So again, similar objectives, very different sort of sets of tools underneath. So we're still serving AI developers model developers and agent developers but it's required us to tap these brand new spaces of agent evaluations and things like prompt optimization and build higher Fidelity feedback collection into the platform. So what you'll see now when you go to ml float.org you install them all flow and open it up. Is you get this sort of structured experimentation and deployment platform for ML and Jai. But you you pick the the objective of my building and AI agents or my building, an ml model, we tailor The Experience based on that selection.
 I think Corey is absolutely right in terms of this is sort of a natural progression that we actually serving in two different audiences, right? As, as he pointed out the data scientists deal with models, which are sort of deterministic and evaluation criteria is more quantitative. You know, you have, you have a certain position that you actually referring to your certain recall. Whereas if you look at the experimentation, they go through the same process but the evaluation process is quite different, right? You have more a non-deterministic model where you actually now have this set of built-in judges and a set of Customs judges that evaluate based on the criteria. And I see I see a similarity between prompt optimization and fine, tuning a traditional model. They are sort of similar tools and similar techniques they use. But it's an actual progression and thing we emblem for currently serves both needs. So, with your traditional ml engineer, and data scientist, you can use them before local, that particular need. And if you have the recent more engineers in Janine developers with building agents is Corey pointed.
 Out. We have a set of techniques and tools and judges that allow you to measure that in a way that meets your media demands and media needs, right? But also very often it's the same person doing both, right? So, I think, while back were like, you know, who is actually building agents, I'm actually curious to like whether you've seen this as well.
 Um, we're like oh maybe it's like all these like, new class of Engineers who are building agents and like, we're not talking to them at all. And then we went on. I just talked to like, a bunch a bunch of people, including a lot of nanamo users and it seems like actually
 A lot of people just shifted from being like, okay, I used to be a data scientist or data engineer collaborating on the classic Model and now I've just rebranded into like, Jenny, I Engineer, hey here. And so that's, that's still probably like, you know, like, half the market is like these, like former data scientists or data, Engineers, or Emily's converted into these Gene AI people. So, that that's where they often, then we'll come to us and be like, hey, we've already used them all in the past. Like, we would love to continue using a more flow. But so only there are these kind of like, new, like, Engineers who haven't really gone to this hospital for and then they're like, what is an eval was. So then just kind of an education component there of like,
 You know, hey, we've already built all this expertise over like Decades of how to like, how do you iterate quickly to get high quality, you know, outputs, right? Let us tip teach you kind of one of the best practices there. So,
 well, that answers my question that was going through my head, as you guys were talking, which is like, why not make a different product? Why not separate the two?
 but it feels like if it is for almost this very similar Persona, then it makes sense because they're already familiar with mlflow, they're just going to extend the capabilities
 Absolutely. The case I think that there are from what I've seen,
 Two temptations that product.
 Developers competitors out there have had to Grapple with the first like you say. Is that temptation to? I'm gonna build a brand new product. Throw everything out, you know, it's a brand new world of Asian development. We don't need all that ml stuff. People can install a totally separate package, hope host, a separates service managed. The work entirely separately are agent developers, who was Danny points out of the same. Folks need a different platform. They're going to install something else that we build and we're gonna have to maintain two different ecosystems. That's sort of the first Temptation. Second temptation is, there's nothing different here. It's all just kind of, you know, training jobs and and, you know, quality measurements for agents and, and for models, I'll just comes down to metrics anyway. You know what, what's the difference and that has its own problems? At the Asian developer needs traces? The model developer? Yeah, I mean, they might want them like way in the future after they deploy something, but it's not even something they're thinking about,
 A, as they start to build a new use case, the Asian developer needs llm as a judge. I don't know why the heck, I would use that if I was building like a traditional machine learning classifier. So you definitely have to like understand. The workflows are different and the tools are different. But the, the objectives and the Persona, the person, that's that's trying to meet those objectives Remains the Same. They don't want to, you know, they have to learn install, a separate platform, the the business stakeholders or folks that are making purchasing and decisions within an organization. They want to pay for multiple platforms and set them up and figure out how to make them available. So we believe we found a pretty good. Happy middle here. You know, the one platform with the right tooling for those Asian devs and model developers who are often just data scientists. So fair bit of software Engineers as well.
 yeah, the Persona is really had to meet in the middle in a way like data scientists had to learn how to become better software engineers and then software Engineers had to learn how to become better data scientists and so you have this nice
 like Venn diagram of of both of those skill sets that you need and you want to take advantage of and I do see like the agent building very much as
 Lots of software.
 Skills that we're talking about. But being able to fall back on fundamental data science, type stuff is crucial.
 It really is, especially as the actual software development, part of it is increasingly automated, the coding assistant's, eight basically, agents. Building agents are kind of becoming the defacto pattern for a new use case. We talked to customers all the time where you know, despite the fact that Claude code and codecs and other tools, really only Rose to prominence within the last year. Or so, everybody has subscriptions all throughout a lot of our customers. The entire Dev team is using these things to build their agents and so it's easier than ever for them. To integrate new tools into their agent or connect a new data source or or even ask, you know, something like clotter kodaks to prompted engineer. And so then yeah because the software development life cycle so accelerated, they have to fall back even more quickly in their workflow to, okay, like what does the quality look like how? Well is this going like those data science skills to your point become increasingly
 Important earlier. I'm not thinking about it three or four weeks from now, after I've hand-coded an entire agent, I'm thinking about it three or four hours from now because the the code is already written because I, you know, I asked a coding assistant to do it and so where this is all leading I believe is a ensuring that folks can do the data science properly themselves but the B
 Why can't we enable coding systems to do some of this too? So there is I think an appetite out there for, you know, if I'm asking clot or codex to build a an awesome agent, can I ask it to evaluate it as well? Can I give it some example inputs or tell it more about what I'm trying to build and let it come up with those inputs, can I then asked it to assess quality and iterate? And so as a result we've tried to make all of them. All flows, judges evaluation capabilities, feedback, functionality available to coding assistance. Through mCP, I give you drop mlflow into my coating assistant and have it act like the the agent developer, which includes the eval. And the more data science he work and that's something that folks seem to appreciate as well.
 Chose your offer me. I get the feeling you got something to say.
 No, I mean you know Corey just articulated very well that the Persona is now a sort of merging along and it's hard to say I'm a data scientist or I'm a data engineer, or I am an architect, given the ability and the extensibility of code. Assistant to say, go ahead and create a traditional model for me. I'm not even a data scientist, but I'm just saying the developer might say, I'm not a data signed. By the heard about you can actually create it or classical traditional model and they will do it for me. So I don't have to actually have the skilled. The skills comes in. How do you actually measure the metrics in which metrics do you actually want to use for a classical model or for an agent? I think that's where the skills can be. That's where the background knowledge comes in but the coding assistance can do one thing or the other is as Corey pointed out. You drop in an MCB server with code. And you can say, go ahead and go ahead and write and write me an agent to to evaluate this particular chat.
 So I think we entering we entering this new front here where where the personas are very gray area but you just need the background to be able to say how I'm going to assess and what are the things that I need to know to understand to get the best out of this particular piece of software that the agent was actually created.
 Yeah I think one kind of cool thing that's I'm at least. I'm also seeing is that
 Yes, these persons are merging but also a lot of the animation seems to be coming from the like more data science kind of persona or like people who are well versed in that space. A couple examples of this would be I think we've seen on our so like very good success with if you want to switch between different. So like you know you're using like a Gemini model. You want to switch to gp2 or vice versa. How do you actually like
 how do you make that switch? Your prompts are tied to your models. I think there's kind of the data science will kind of was like well this kind of just looks like model distillation. How do I like teach him to act like a different model? And I think we've seen that actually if you combine if you try to do that and use DS pi and the mix to do that, you get like really really good results. In fact, you know, we've we've even seen results where you can use like a smaller cheaper model and I can really accurately reflective outputs of like a much more sense of model. So it's almost a cost optimization. Right? So that's kind of like one Innovation. I think that's like coming out of like more traditional like researcher data, science kind of spaces.
 But I just want to read heard something. What Cory said, which was I think the amount of team has done a fabulous job of even though having two distinct platform, one for the agent developers and Gina developed, another one for the for the mlflow, traditional machine learning scientists, the concepts are not very dissimilar, right? Both need experimentation Board needs a way to to fine-tune as I said earlier, which one won, does the prompt and realization? The other does the fine-tuning Board needs a set of data sets that you're going to actually provision over. You know you training data set you need a evaluation data said you need a test set. We do the same thing in Jane Eyre. We actually hit the data sets so I think the concepts are very similar and the underlying tools and techniques used are our percussion across. Both this particular platforms. So somebody who's familiar with with ML and is coming into geni, can actually do their or somebody who's familiar with jni and is going into ml, can easily transport those political. So I think conceptually
 And Visually and and through workflow, they're not very dissimilar. I think the immortal team has done a very concious job in making sure that the efforts of flawless from one for the other is not like you're in a completely different world. There is some Concepts that I should actually come from. There's a when diagram as Dimitri's kind of settle tools and Concepts the merged together.
 Absolutely. The case I also wanted to dive deeper into that sort of model distillation piece that Danny was bringing up, because I think it's a great example of a capability, that folks resonate with whether they're coming from a data science, background doing model development, or they're building agents. Everybody wants as high quality and ai-powered applications possible and they want it for 0 dollars and like obviously you trade off cost and quality a fair bit, but the real upshot, I think a lot of this work and model distillation is I can build a great agents using Frontier models. And then over time, once I understand the quality of that age and the factors that determine quality, I can I can ramp that choice of model down to a less intelligent. Less expensive Frontier without sacrificing quality. And what I need is a structured way to measure it. And what I need is a structured way to adjust my agents.
 Actually, in particular, if I'm going to change models. So the task of moving from like a GPT 5/2 down to a GPT 5. Mini, for example, or a Gemini Pro to flash that process requires me to understand what are the examples of great agent quality? What are the examples of negative quality that I'm trying to avoid and how do I tailor my agents instructions? So that many can understand those instructions just as well as as GPT 52 Etc. So if I have both of those pieces which I'm also provides I can get to a point of cutting my costs by a factor of 10 even 20x just by virtue of switching the model. And so that's pretty powerful as well.
 One other area I want to hit on Danny. We talked about a little bit. Last time we chatted was the idea of governance and how
 Folks are really trying to figure that out right now. It's specially with agents, being able to read and write and you're like
 So who are we gonna blame when they write the wrong thing? How are we going to keep tabs on this especially if we're at an organization with
 100,000 people and they all know are leveraging agents.
 Yeah, I think governments is a super hot topic for us right now. I did everything and Corey's been actually in a lot of these conversations more recently than I have, I guess databases, well, we're known for governance. We think about governance a lot. I think I governance kind of breaks down to a couple of components here. Probably the first thing that people talk to us about is like, well, I have
 Like pi, and this and all my different, like, Asians and traces, like, what do I do with that? How do I kind of insurance? Only the right? People kind of nice to that and then there's all. So I guess if fumbles out into well, what data does the Asian have access to as well, who can use, what agents, and all? So kind of really kind of gets into that like AI Gateway kind of Realm as well of like, well, kind of like fall back to different models, kind of control costs, one or not. So, I think we're thinking about all that stuff right now on our side, what we do, have an AI Gateway kind of
 Managed component. I think Corey we've now is a controversial to say that. We are kind of exploring open source.
 Right. I would go beyond that and say, you know, the mlflow has and for you but like kind of to like we are, I think the plan is to invest substantially more into kind of an open source. Governance kind of Gateway solution here, right? 100% absolutely. And so going a level deeper on that access. I think Danny. There's did a fantastic job of discussing the sort of aspects of model access to data and then developer access to the data that an agent produces. So going back to that that model access to data problem.
 We are invested pretty heavily in providing cost controls for model access as part of Open Source mlflow also providing data access controls. So I have a bunch of tools. I have like this awesome Tool, Catalog with mCP, I'm gonna hook it up to my agents. How do I make sure that, you know, the right tools are exposed to the right agents for different use cases and that, you know, they you don't have an agent accessing data that it shouldn't be allowed to, for example, about it, completely different customer like these knowledge bases are are pretty sensitive and then so walking down tool access is really important but then, as I mentioned, just cost controls, I have two teams building. Agents one is building a low QPS chatbot. The other one is processing billions of documents every day they require different budgets and blowing the budget at a one that project and sort of exceeding costs can mean that the other developers don't get to do anything. And so like, folks definitely need what we
 Call this AI Gateway that allows organizations to say, all right, for your use case, here's your budget. Here are the models you have access to here, the tools that you have access to, if you need something else, you know, come and talk to us, talk to your admins and we'll grant that access and a structured manner. Otherwise it's, you know, API Keys flying around access to data on regulated. It becomes chaos and there's like real risks there from a model access to data and costs governance perspective and I think we could probably talk about that all day and as Danny mentioned in terms of developer access to agent data, that second consideration, there's some big challenges around that as well. Pii ensuring that only privileged eyes can look at certain slices of those traces. That's all super important and that's where databricks Unity catalog has been the crafted and tuned for years to.
 Provides governed, access to model data, agent data, bi data. All of basically, that this is no difference than any arbitrary set of sensitive data. You're ingesting into our platform, it's just that it's coming out of an agent. You need to govern and manage it and share it, just as securely. And so that's where, like, we've been building databricks on all flow on top of unity catalog. In this amazing secure Enterprise grade and governed data Lake on databricks.
 Yeah. Um yeah. I have a separate video where I also think a lot about Pi detection and and how it really, so you need to catalog. And I think one thing is
 A similar to what we're seeing here which is the easiest thing that people will start with is they will just redact all pi. And so like nobody has access to this or totally lock it down, but rarely is that the actual goal? So you catalog there's been a lot of Investments on. Like, how do you do you know a back attribute based access control? So that and what people are trying to do now is like, okay, well if there's a scalable way to manage governance, then I would actually open a data as much as possible to people who should have access to it. But in absence of that technology then they'll say let me just fully lock it down. Nobody can see this information and that's kind of like now you're losing a lot of business value that you that you could be kind of getting access to, right? I think that's all going to be playing out within the Asian space. Too, where if you want to really debug, why is an Asian going wrong or you want to do like Downstream like bi inside out of this stuff, sometimes you you are gonna need to capture some sort of like pee from information. Just not everybody has that. And so at least were we've seen some early kind of success.
 Here is having all this traces, get ingested directly into adults table. And then now you have access to all the governance controls, you have with the catalog and you can go back to kind of
 Of having an open access and sharing and doing all your data streams, right? Whereas, a lot of, I think, the other competitors, or vendor Solutions out there, their default reaction is like, let's give you a tool to just redact all API and have a never landed to begin with, to me. That's, that's a great first step, but it means that you're, you're locking away a lot of that, like super valuable data that you could be really analyzing in and parsing. Well, I've talked to people who can't use memory tools because the memory tools just by default, you know, the open source ones that you get
 Off the shelf now, expose all of this conversation that's happening with their users and and there's pi.
 All over, it's just like littered throughout that even if you extract away.
 Sensitive data, that's obviously sensitive. Like a social security number. For example, inside of the Enterprise, if you're asking a chatbot to help you craft an email to fire someone that's all. So sensitive. And like, that's much harder to
 Get.
 When you're thinking of it, like in the traditional sense of pii, you know, it's not like oh that's not my credit card number, that's not my social security number, but that is very sensitive. So you're talking about Dimitri, you're talking about, you know, the the well Daniel was talking about the Pai that can be redacted and there comes up quite a bit in Meetup talks about how we actually going to redact the traces but images, I think you bring up a point where important is I'm crafting an email which is going to fire someone or I'm crafting email, that's going to promote someone that's considered kind of, you know, private information. How do you actually read out that, you know, what's the context? So the context is important. How do I actually take a piece of trace or a piece of output that has come up and then evaluate and categorize that essensity announcements do right? That's again and evaluation criteria. Well, I was gonna say and actually the process of redacting Pi is actually a hard process. So, I think a lot of people will start thinking, like, let me just write something like random redirects.
 For people who are a little bit more sophisticated, they might use like Microsoft Presidio. But I think everybody I've talked to including ourselves who've tried it is like he just gets so many false positives. You, you miss everything. So, you know what, what we end up having to build for our own API detection? Offering is, we had to build an agency essentially, like a whole, like, kind of complex, like sequence of LMS, and hero 671, not to kind of actually figure out, you know, with relatively low cost, what is pi, what is not. And we've seen that is much much better than my kind of the office off the shelf Solutions out there.
 Here. And what makes this a lot easier is actually having access to the rod data if I can treat this thing as a data frame or as a table and use all the Brilliance, you know, SQL and Python and ecosystem tools for scrubbing that data analyzing and tweaking and feeding only subsets that I know are safe into a memory database. So that when Demetrius is tool reads from, and I know that it's reading information that, you know, is, is safe and secure, that's really important. Unfortunately, a lot of solutions out there don't actually give you access the raw data. Like, typically, these traces get dumped into some shared like big multi-tenant, like postgres database and if you ask for like hey how can I get like a table or a data frame or the raw records that they can do some sort of more complex analysis or find pi? And redact it. The answer is good luck it's you can't actually get that data back and so that's where again.
 This sort of databricks unity catalog, Delta table approach that stores, all of your data as a beautiful, tabular formats. And data frames makes this kind of analysis. A lot easier you can plug in any tool from the ecosystem. You can use sequel. You can express any sort of transformation or Pai reduction test that you'd like and so that's really powerful as well.
 Now I'm excited for kind of what future stuff we start building was like base as well because like bases kind of tailored to this kind of use case that Demetrius you're talking about. Right? You want like a database that it has fast access retrieval and we're not but, as well, governed? Right? And that is, I think kind of the promise of Lake based on database as well.