And you are not just providing a functional to your feature to the business anymore. You also need to help the business to go through this big transformation when we share these tools to our colleagues. They came back to things that we didn't expect. Yeah, well practically the problem didn't start with AI. We had that problem in the past as well so you mentioned that it's hard to measure, it's not
 Hard to measure. The question is, are you measure the right thing?
 We can start out with just the idea of how you are thinking through, enabling your different departments and The Wider company to be more productive with AI.
 having the role has changed quite a lot for me as well, because
 Up until two years ago. Your focus as a CTO is to basically provide the functional to. The company needs comes from product comes from marketing, comes from any department, and also your responsible for the scalable to stability of the platform. You have to make people trust what your building in the last two years, which AI, which church EBT being available to everyone. You also realize that, okay, this this is big
 And you are not just providing a functional to your feature to the business anymore. You also need to help the business to go through this big transformation, about how you're thinking have your process, is how your way of working needs to change. And as, as you are the responsible person for the technology in the company, overall, you have to basically leave your comfort zone.
 And then push the boundaries across all these departments to help them understand what is coming up, how big that will be for those departments for the business for ecosystem and for the Ford worlds?
 And also in the background, build the foundations and the platforms, so people can operate on these things as well. So you have to become a Shepherd. Yeah, exactly, exactly. So, you are not responding to requests anymore, you start to push the boundaries of the business more than you have ever done. So it was a change. So a couple of additions, let's say, because I can see everything that matters has said so far. The few decisions that we made even before charged PT is that all these Technologies large language, models, independent of the fact that they're not perfect, in the end of the fact that this is, you know, it's going to take, he is, before we get something, which is full proof and so on, but there are so new and so impactful that you need to find the way to learn them. And then you can decide to mandate to create education. So on or you can take the opposite direction, which is let's give everybody the tools so they can experiment bottom up.
 And I totally convinced that for every organization that I see, this is one of the best options you can possibly have.
 It's provided within boundaries that got boundaries, being the guardrail, such a way that you do not make catastrophic mistakes and so on, but give the tools that people can decide how to create agents for themselves. How to, in fact, create the things that they need for their work, right? And the reason why we did that is because long ago, gpt3 I think it was really, really early stages when we share this to our colleagues, they came back to things that we didn't expect, use cases that. I can't invent that thing, because I'm not in that line on this. I will never be able to figure that out. So by giving everybody
 And means, literally everybody tools that can experiment. There are two things that we Bank on the first one. They will create something useful for them. So there's going to be a productivity quality of work that increases, which then if you multiply for the organization is going to be
 Material very much material.
 And the second thing is that there is a distinct advantage in having everybody understand where this tools work, where they fail, how I can tweak them to make them work where I need.
 Because then they can apply the same intuition to the applications that they develop for our consumers, right? So these two things tend to become let's say they are the same thing in the end but they're very important. So Collective discovery on one hand but also Collective Awareness on the other end. Actually I think one thing that I was shocked by was when Paul was telling me how you guys encourage most developers to use various types of coding agent tools. Yeah and it's for the sole fact of, hey, if there's something valuable here, you should know about it. And you should choose. We don't want to say, oh, we're gonna mandate, you can only use cursor or we're gonna mandate. You can only use XYZ insert, your favorite coding tool Claude code, but we want you to be able to find the one you like and use it if it gives you that productivity. Yeah, so I'm you should wait on this one as well, but there are advantages in both sides. So the one hand, there is a big advantage of one tool.
 Everybody standardized on that one, you can get better conditions, commercial conditions and also a lot of knowledge can be easily shared, right? So because you just create, let's say a lot of knowledge. However,
 The reality is that these tools are still developing. This moment there is a winner but next year's might be another winner. So there's going to be, let's say, an evolving landscape and you also want to make sure that you continuously test and stretch all the tools out there for various reasons because for many use cases, you'd better use different tools. It's just a reality of that but second because it is evolving. So you don't really bet on anything. But you bet on a way of learning this tools, right? So in that case, having a certain degree or total degree of freedom is fine. The other thing that we are not really, let's say compromising on is to to push everybody to use this tools, right? So this is a deliberate mandate to experiment and get to 100% adoption as fast as possible for the same reason that they discussed before.
 Yeah. So we don't exactly follow the same approach in terms of just let everyone use everything. Are we started with Git Up? Co-pilots? We also launched Amazon Qdoba. Loopers, we are right now. Launching cursor.
 Um, I think the Android engines are using gemiini, and I know there are lots of other tools as well. Available, the reason why we didn't just let all is to also get provides some Focus to the organization. And also I share some of the learnings about how to make the best about a certain tool instead of jumping from two to because there's a trust element on if an engineer will trust the technology and
 Um, for building the trust, I think you need to spend some time like, you need to give the credit to the, to the, to the AI or to whatever it is that well, it will not solve everything at first go. So you need to get better with your prompts. The AI will get better with the models and there are things that it will be great on doing and it will, there will be things, which they're really not good at. So. And if our assumption was that, if we let them just just jump from one tool Turner dish, we may lose that that momentum on investing, right time. And also, we would like to provide some guidance about how to get past about a certain tool, internal training, having the providers coming and telling people how to, how to use those tools and Commercial side. It also helps with the negotiations as well, if you, if you go with a few of them but I fully agree. Like the idea is not not to limit or or make it very strict. So that people feel like okay I mean this company is
 Just not evolving. This company is against these two. So if you give that feeling and if you just push for a certain thing, I think Engineers are very smart to understand that this company is not going to be in a way to this company will disappear. So, but also giving some guidance helped helped, I believe to us. We were quite
 Pushy about the adoption of these tools as well. So in the All Hands as I was very clear that like, you guys have to use this, so there's just no, there's no option because it's not about AI replacing you, but someone using app will replace you. So it's not about you losing a job on. Just did, you are losing your profession if you're not using this these things because these things will not disappear. These things will be part of our work and the adoption has picked up very much like I think we have more than 95% of the engineers using these two with a day for their day-to-day work. Always if we didn't, we six months ago, we made a calculation that we believe 340% of the code generated is AI generated on the production systems and it's very difficult to measure that because you have suggestions, they can copy paste, they can use other tools as well. So you cannot easily 100% accurate with this number.
 But the reality is, we also didn't see a 30% velocity increase in the organization because software development is just not, not just quoting, it has lots of functionalities as well. And all these things are getting better over time, but the trust to learning the way we are operating is changing over time. And I think I mean, we are in a very strange and interesting and exciting time you bring up a really good point around
 30% or 40% of the code is generated by AI. It's hard to track the metrics on that.
 I want to get to that in a second but I also want to raise the other point that you said it, which is
 We didn't see a 30% increase in velocity because that's not the bottleneck. And I have a joke, I'm always saying that the bottleneck usually is the aligning part. It's not the actual, like hands on the keyboard part. The aligning takes much longer. Well, it's getting bitter as well. As you said, there's just so many factors in place on affecting your total outputs capacity of velocity.
 That depends on the coating. That depends on the line. Won't be to your team members with other teams, with product manager, rating for ux, waiting, for QA. So, but I think AI is helping in all areas in parallel. So it will be a mistake to just bet on certain process and just optimize it to a maximum and then ignore that. I think it's all evil at a certain Pace, but all the reality is behind the scenes. There are some new work that is coming up about being more, AI or AI available as an organization as a as a technology, as well. So, the way we have architected these platforms.
 We're based on the fact that's always software Engineers will scale themselves. The teams will scale themselves. We started with monoliths and microservices and probably right now we will see a different shifts of how software will be architect to so that AI can generate code much much more satisfactory than
 The then the limitations of today's architecture so that means additional work will come to the product and Engineers as well. So you mentioned that it's hard to measure is not
 Hard to measure it is. So there are many tools to measure productivity of golden, right? So then and there are companies that have tools for for that job. So you can do that. The question is, are you measured the right thing? So the fact that you have the same more PR's, let's say, or things like this, is that that's a good measurement of productivity. So that remains the thing, is that in any case, coding is just one of the works dreams of a company. So, if you look at the company, this engineering that does many things including so there's a group of people that are contributors to code contributors to the thing. But then,
 This type of Automotive automation. But in any case, agentic used to improve your work and be applied everywhere across the organization.
 Then interesting, part of coding, but that's similar to what you see in legal profession. What you see in customer support. You see in some ways also in marketing is that software development is a well understood discipline.
 So there are schools, there are training, they are coding tools, of course, you know, the best practices. And so people more or less know how to do it. And then others can judge if they're doing well or not and so on. So there are parts of the organization where these kind of a genetic augmentation follows a fairly well-defined path just because the discipline is, well, defined
 And then you have a long tail and extremely long tail of how to call it is sort of the way to the way I look at Dennis tiny total, addressable markets, why clothing? It is a large total, addressable Market is within an organization, you have thousands of tiny. Total addressable markets will Market might be one or 10 people, right? Almost an individual basis.
 And they can use these tools to make their work. More impactful can be faster, can be better, can be whatever metrics you have. Now that's another very to me, is a very interesting segment of a genetic automation, which usually is not captured by let's say one or two companies, but it is created by every single worker bottom up.
 so that's one of the reasons why we have been working so much and pushing so hard in getting
 in the hands of everybody in the group tools, such a way that people can create their own agents because there might be three people in the marketing department. The struggle with that problem. They repeat the same thing over and over and over. Now, they can actually do it much faster, so we can do something else.
 How do I get that problem if I'm not there, right? So there's no way that they can invent. So it's not like coding, we know what good looks like. But here, I don't know what good looks like they know, right? And they are the only ones that they can put together something that makes their work better.
 So it's a tiny addressable Market but they have it thousand times, right? So if you have a very low barrier to creating this agents, then you can create thousands of automations across the organizations which in total, they're really impactful, right? So that's the long tail of let's say agents in at work and it doesn't make sense for a company to create a product to service the three people in marketing, of course, there's no Market, exactly, there's no market. So you need to have a tool that makes it extremely simple and extremely easy to create this automations for you. Then it makes economic and practical sense, right? And that's the reason why I was not done and until recently it's probably good segue to go in to
 Your bold vision for 30,000 agents in production. What is it next year, March March March. Yes, about you are right now. We're 18, 18. Okay. Yeah, that's how we're getting there much faster than that. So, yeah, we're going to go there was a graph that graph is exponential. So it's let's go back and and think about why is that a good idea even if it looks silly? I think it's a very good idea because it has to do with change management. It has to do much less with the agents but that's the much more ability to everybody of removing the barrier that they believe for agents. So back in July, when we have seen the agents created with token, the platform that we use for that.
 Sort of plateau in spite of the fact that had all the functionalities They connect to mcps. They could connect with, let's say, dozens of internal systems on the people, were not adopting more than the technical people, right? So only technical people were sort of this and then we had to figure out the reason. The reason was extremely simple is because everybody else thought that this is a tool for engineers, Engineers can create agents. So we have to wait for them. So it's going to come in and help and so on and so on. So we had to remove that barrier and how do you remove it by sitting with the marketing people by sitting with HR people and creating this agents in 30 minutes if 15 minutes not in a week, right? And once you go through the process, then you see that you say, wow, is that that's it, is that only takes to create an automation makes a difference. Yeah, that's it takes, right? It is at this point in time, is simple. So that what made the difference in the started going up?
 A few other things happen, when you're at that point.
 But if you have created an automation that helps you you're going to tell somebody else. Hey I've done this one and it's going to help why don't you do it too, right? And so on and so on and so on. So that mechanism goes into let's say a percolation of knowledge that enables everybody to create the ages. What? We don't know how useful and how good this agents are we are going to focus on that afterwards. First, we make sure that everybody feels that he's that within very specific boundaries, right? So you can do what you want. You have very strong, security provisions, within those security provisions, everybody is encouraged to experiment. As much as they can in such a way, they can become the owners of these tools. So this is not an engineering tools a tool for everybody. Once you get there, you start getting into.
 What are the good agents? How can make them better, right? How can you change the processes then?
 That generated this inefficient in the first place. So, how can you change the organization?
 It's very hard to get there if you don't have everybody on board, right? So 30,000 agents, in fact means it's a change management process, it's a cultural process. It's a, it's a, it's an organization thing. It's nothing to do with the technology itself. I actually really, really like that approach and I can I can just add to what you said, although we just became part of process. So token, it's not available to our organization yet but we will help the 30,000 for sure. So you make it before March, hopefully help but get over that. I like the fact that there are two things, one is you put a bolt
 Idea out there which no one can ignore and that creates a question mark in people. Like what is that? Why total 2000 what is talk on? Like should I be worried about that? Should I get a training? So you created demand, you created an expectation, you can create a question mark in people's. If you don't go bald, if you don't make a challenge, it's very easy for people to just move on, ignore and focus on their own work. And the second one. I also agree, like I have a daughter, eight years old. She's she's learning to read and write.
 And my kpi is how many pages she reads a day. I actually don't care if she's reading the stories that I want her to read or not. But at this point, I want her to read. I want her to understand. I want her to get better more comfortable with what reading is and then when she gets more comfortable, then we will discuss about which stories which values should get about it. But right now similar to probably what era was doing is
 you need to create the first level of understanding about what is going on and everyone needs to jump into that boats by forcing themselves and the value. How you are going to get the maximum value out of that. Is the next stage. So I think that's that's the right intentionally imagination. Anyway. Yeah. Absolutely. And then as I said before, it will always go back to some let's say of the tests that we did in back in 21 and it was really really early on with with llms and and people were really coming back, right? They didn't they haven't seen this tools before. So the stitched together. Very simple user interface on top of GPT three. Yes I think something like that. And we gave it to our colleagues and do something because it looks interesting. We never been able to do good stuff with with NLP at this level. So, this is really bad at this moment that time tools really bad, right? But we're really promising too and they were improving really fast. So if that trajectory holds,
 Then you are going to have in one or two years something which is really surprising and we never had before but how do you use it? Do I know. So we had the ideas of course. We, you know, we repeated all the NLP use cases that we had before with with llms, but then we gave it to about 100 colleges at that time. Sort of so really, really mentary judge BT where you could test and so on and people were coming back with things like they created exams for Schools, they're completely personalized. They put let's say a small CV of let's say students in the prompts doing experiments like that and so on. And how did you think about this concept? It's actually great idea. It's a great idea. Did I have that? I know and so many other things. So that what convinced us that because of the novelty of these tools because we never had tools similar like this one, you have to get
 You have to experiment bottom up, if you want to find out, let's say what you can possibly do with this thing. So, all the errors, you can find this mechanism of let's say, Collective experimentation is one of the greatest sources of threats potential errors on. That's how we learn where we should not go because then you don't have one team figuring out. In fact, he got 30,000, people doing red, teaming all the time.
 It is also very useful. Well yeah, because you're scratching your own itch. At the end of the day, I have whether it's professional or personal. I have this thing that I want to do. Let's try and see if I can build an agent to do that.
 Absolutely and then and then you can see that somebody else has done an agent or an education just that exam. But you can do the same thing for customers support you connect the dots, right? And that dot connection is what you need because we just don't have an intuition yet about let's say the full potential of this tool. So how do you get to explore the boundaries just stretching them as far as you can? Yeah, it feels like there is a lot of the change management piece here and being able to encourage the employees to spend time with it.
 As an employee.
 I would probably think.
 I'm so busy with other stuff.
 To put the time in even though I know that later on maybe it's gonna make my life easier. But to front load that like pain is kind of a hard ass you know that they like the mattress but I think this is a completely wrong way. Yeah. All right. Well I'm glad that you because I think these tools are going to be used only if they are immediately helpful to you.
 There is no way that you can push these tools down the throat of people, by the way, this is smart colleagues. They know how to work. And really, they've been optimizing their work. Dealing with, let's say, inefficiency all the time, it has to be beneficial to you, right? And you have to be the one that recognizes that if that's not the case, you should not do it in the first place, right? So there's no sense to do something doesn't help right. Now, what we can do is that we need to make sure that the barrier to get to that point is as low as possible.
 That means the tools work they give access to all the knowledge base that you have, prevent you to make big mistakes. So you got to say boundary conditions provide examples, right? That of what others are doing. So the the barriers should be very, very low. But once the barrier low, the only thing that makes sense is that if you do you create something that helps you, if you don't have, you should not do it right? So yes, it's fine to test here and there, but that's why we're not even thinking about allocating special time for this should not be necessary, it should be immediately useful, right? So otherwise he actually create friction and then you have to deal with friction then you have all the other discussions. So we don't I don't I don't think it's a good idea.
 He also need to recognize them the excellent work that some people do.
 so,
 One of the instruments and there are many is gamification. So we have this program which we call process as talent, I think, I think, I think that's the name. But anyway, it's it's designed around Shark Tank. It's a competition. It's starts now across the entire group, everybody creates agents, right? And there's going to be, let's see, a committee that selects agents every month.
 So the best ones are going to be published across the group, they're going to be, let's say, featured, everybody knows about that. And then there is this, let's say Cristiano towards smart. There's a big event as a big selection of the winning agent and the winner will spend the week in the Silicon Valley and many other things, right? So there is a reason to do that because it makes a difference for you, but it also something that, you know it is recognized by others. So, you need a lot of these things, but the idea of separate say, making this a sort of separate program of innovation, I think that's not a good.
 Yeah, I like that. And the real key there is
 You are now.
 Creating a very high bar for the product itself for the agent Builder, because it has to be instantly useful but this will be immediately useful. And we also accept and we're really, I think it's a very good idea. Is that let's say certain portion of this agents are just experimentation
 I'll do it because I really want to stretch the tool and find out what it does. What it does, right? It's not the waste of time, you know? You just call it learning and I think it is learning and learning is not a waste of time. Yeah of course and you might win a trip to Silicon Valley and then you might even get a trip to feel that's so cool. The maybe it's a good moment to shift a little bit towards the idea of governance and how you're thinking about 30,000 agents. You kind of mapped out you have boundary conditions but then there's probably a lot of other pieces that you're thinking about when it comes to the governance of how these agents work and what
 You're thinking through as your looking at like okay we're introducing AI into the company. What does that mean?
 On a governance level.
 Assessing topic going in general. But 28, I guess gonna go going and work when people don't feel that, but it is somehow protecting them be without provide putting so much pain or hassle to the people. So,
 Always, it depends on on the area for software Engineers. The governance is, if it's generating code of reviewing code, you need to have the technology or the tool sets that is basically, governing, governing what your agent is doing. So, there should be other agents or people governing what is going on so that it will allow. So fare Engineers to be more flexible, more confident about what they're doing, so you need to build those things. So you cannot just take governments as a Google form that I want is to submit something before doing it, or a dpu needs to improve the use cases. So, that is bureaucracy. That is also needed. Don't get me wrong for cases, that is needed as well, but the idea is how you are going to be able to governance. So people don't feel that. There's a governance happening or it will be quite practical for people to understand that. Okay, they are asking these questions or they have these boundaries or limitations to protect me or the business from from these cases. So in
 In the early days of AI hype.
 We had almost no governance in terms of tool sets selection because it's just so many tools available for marketing for sales and other stuff.
 And we have, I mean, it didn't took more than six months, to realize that everyone is buying tools without talking to each other. And even they're buying the same tool from different departments, with different contracts as well with different pricing.
 But as you said it is, it is fine because you also don't want to slow people from from adopting or trying these things. There's definitely a cost of it and usually the technology department pays the cost in terms of making sure these things are secure or have a proper procurement as well but you just want them to try and after six six months almost 8 months we started to put more government. A lot of how you can procure, an AI tool, how you can launch it, how you can make it access to these things. But you also try to be practical in this once. You don't want people to feel like, okay, you're killing invasion of the business. You are just slowing me down. You're just making it harder for me to just because you don't have the budget. So we don't we don't want to do that but we started with almost no governance. We are right now in the middle. I don't know. If we will go more on to the more strict governance over time. I think our approach should be more at automating these governing tools so that you will feel less about
 One is governing or something is going. So we build autonomy within a framework kind of solution for for the business and again it depends on the area software engineering is different. Procurement is different launching. A sales AI agent is completely different or a customer service to use cases, completely different. So as long as you are practical lighting, you can find a solution
 And so if I can add a few things because you mentioned there's something I think is very important is it's really hard to get this tools to work, 100%, right? So you have to accept today, make mistakes that they're saying they're residual hallucination still there and sometimes they, you know they're they're just not the typical tool which is deterministic if I have let's say I follow a certain sequence I'm going to get always the same out so this tools are not like that but that's that's and nature of that. So you need to accept a little bit that even for experimentation. It can go somewhat wrong, right? It's just the nature of that. So the question is, what is it the things that you accepted? What are the things that you think they're not acceptable?
 Well, you have many levers to decide that. So the first one is, let's say, whatever authentication authorization these tools. Have you should never be more than what people have. It can be less, but actually even less than you. But anyway, no more. So that's one thing. The second thing that at this point in time, I think we are sort of uncomfortable, letting agents do their work completing autonomy. So there's always a level of supervision and that's supervision is at the end of the process or it can be weaved in into the process itself.
 For instance, we know that prompt injection in many agents, it's still an issue. So you want to make sure that if there is a sense that perhaps another model, so this is potential risk. You have a pop-up, you have something that the human has to say
 Do you really want to visit this site or better? Do you really want me to go to that side? Write or execute this for execute? So you need to introduce a lot of this checks in between.
 And, which makes let's say the process potential is lower. But in any case you enable more use cases in that, right? So a lot of these things are part of the way, you design things, the other, the other observation is.
 How do you in figure this out which which are these measures, right by enabling testing because people come up with additional ideas or issues and threats and therefore you react to that. So to the extent to which you try to limit the worst case, right? So you really say, what is the worst case? What's the acceptable worst case? Then you have the Baseline on top of that, you can develop, but I think part of the experimentation and part of the education to people using this tool is that you have to accept it 20.
 So there is they will at this point in time, you know, it's experimenting, then not work, they're not going to work to the level you want. You need to do some work, it's until you get them to do exactly what they want, right? But experimentation has to be based on this acceptance. Yes, it's going to be some iteration before I get into what I need. Yeah, and you said something around the
 Sprawl of tools and just being able to grab whatever you needed. And then you you kind of had
 To tighten up the policies, a little bit more on that. It reminded me of a piece that my friend told me on how
 His he was in a data governance role. And his job was to go around and check all of the different tools that were being used that were with AI or whether it was like Claude code. Or it was a marketing tool that sent data somewhere that was being processed by Ai. And he thought like, yeah, all right, cool, we're not that big of a company, there's probably 10 or 12, maybe and after he went around to every team, it was like, oh, there's 90. And that's not even all of them that I know about because I'm sure there's people that are grabbing some data and then throwing it into their personal Gemini, account or whatever. And so in that regard, I often wonder like
 Because it is.
 such a tool that we want to be able to like
 enable folks with
 But at the same time, you can open yourself up to all these different vectors that you weren't thinking about before.
 How do you think through that in a way? That is like, okay. Now we are paying for 92 tools that we didn't realize we were paying for and these tools we may or may not be using them to
 Send data off like through another service, right? So in a way I'm thinking about the
 product management tool that records, your calls, and then sends all of that, all your calls to Chachi BT, to get it analyzed.
 You're paying for the product management tool, but then you have to know in that governance position that oh all of that data is going to chat GPT. Yeah, well practically the problem didn't start with AI. We had that problem in the past as well. Still for almost since technology started, every Department, can buy a tool for themselves and they do and then they do yeah. Like two years ago when the Enterprise RT team, showed me the number of licensees. We have from different tools paid by the credit cards of the Departments. It's mind blowing and they had no AI in any labeling or domain names. So this problem all the way always existed. So what we build as a technology department is one how we are going to basically secure ourselves because the court, the concern is that you understand people want to make their organization more productive and
 All the problem as soon as possible. That's, that's the entrepreneurial mindset. So, that's that's sort of the fine but we also have a responsibility to protect the business and the customers and the partners as well. So,
 For that. You need to have the policies that are practical so people can follow and at least even they buy something they can just tell that. Okay, I'm buying this one. Hey would you like to integrate to our okta system or our security system? Is it behind that scale or or something else? And you need to educate people about okay. We know you are building a shadow it from time to time but at least let us help you so that we won't have a bigger problem as a business. So that's that's that's that's we have been trying to do. And I think we are more successful because the technology also helps about understanding which tools are being used, the finance reports, tell us what is going on the network logs as it. Look at this. So much traffic going in this in this direction, we try not to prevent it from from the beginning but we are trying to educate them to to help them help us protect them and the business as much as possible because I think one, one concern I usually have about the technology
 The department or me or my people, is that we sometimes overestimate our capabilities in terms of, oh, I can solve their problem. I just need five more Engineers so I can also solve marketing problem, but that's that's not the best. Use of your five Engineers for that problem. There's already Solutions out there, let them just just Outsource this problem to someone as we have a Core Business problem food, delivery grocery delivery. We need to use that five Engineers to solve our problem. If it's 100K, that we will spend, let's spend it as a business. So that's why we need, we need to, I think, empower the people to basically operate in a in a given.
 Framework and help us help them about how we will go on or procure or maintain this things. And I will be honest, I think we didn't nail it.
 We still have Painful areas as well, but in order to 100% nail it, then I think you need to have a very strict governance which will eventually come with the cost of innovation in the business. So you need to take certain risks based on based on your business model based on based on what you have and educate people to help you. That's that's how we try to solve it.
 I totally agree and the and so the education comes in many different forms and so on.
 but one way, you already said that right as these tools
 They make new mistakes, but they can also make majority of the mistakes we really made before, they just better at them. The sense that you have more ways to do that. Yeah, more ways to make mistakes. And so, so you need to make sure that people are aware of that, but you can also act at various levels for instance. You can act, let's say at the point in which you say, this type of data and it's a very specifically lists of data types.
 Should not be used with these tools who's stop.
 That's it, so that's you. Cannot go there.
 Then you can also introduce a certain number of friction in various places. For instance, if you have an agent that sends CMA is not allowed to send bulky male only one of the time, right? And you are not allowed to Loop it so that you reduce the chance that this is going to start spamming the word. So therefore variety of measures that, you know, can be put in place.
 And if you put them all in place, you eliminate all errors, then you have don't really have much use of these tools, right? But then you start peeling them away to see what's acceptable. It's not acceptable. You get to the point in which you, you find, let's say some sort of balance between the need of innovation. And also the need to protect the organization. Yeah, that's for sure. The case what is it's also extremely important is that a lot of the things we learn in the past about all these are still valid but these tools are also new.
 So, you find out a lot of the vulnerabilities if you want by doing right, therefore, if you have tools that enable you to test, without making big mistakes, then you find the things that can go wrong. And then you serve that and then on and on and on and on and on and on and on, it's a process of learning.
 The fastest you learn the fastest you can use this tools at scale for important things which is very beneficial. I want to end with one question that wasn't on the docket but as we're talking, it's like it's
 Very important. And I've heard folks in the community talk about it, I've heard folks in different communities. Talk about
 How they are thinking through provisioning resources and provision mainly financed towards AI because it is such a like we saw the graph yesterday. It was exponential. Did you know it was gonna go exponential, probably not. Were you thinking maybe it could? Yeah but how are you allocating budget towards gpus or openai bills or all of the different things that come with the AI? If you are going to see the explosion that you're expecting,
 So that's a super interesting, an important Point too, because we see two phenomena, the one hand that say, the let's go to Unique ghost of intelligent goes down very quick, so for doing the same thing, a year ago in the South and off and off and off, right? The token, the cost per million token is also going down. So there is a trending, our favor, an old discuss, we will be, let's say decrease in overtime. At the same time, we see another phenomenon. Is that everything that we do requires more tokens, it cost more. So this two things are won against the other.
 Old together. It goes towards being more expensive, right? So the additional use of tokens is overcompensates the reduction of costs.
 at that point, you have many things that you can do
 One thing is optimization, many use cases develop so far are super. Let's say inefficient the prompts are very large. The contests are very let's say worthy and all this kind of thing. So you need to go back and redesign how things are done because otherwise, you know, it costs and so on many tricks and many risks to reduce the number of tokens at the output for many use cases. So people start getting the same pain. A lot of attention to optimization of this, you start getting to the point in which open source models are competitive to with, let's say the bigger commercial models not for us. Yeah, so not for everything, but for many practical use cases. Then at that point, you're just better off hosting the whole infrastructure yourself, right? Which is another way of covering it. But only in all it is, it was not a problem last year because volumes were small. And then let's
 Say use cases were not complex. It is initial now
 That is exactly what I was thinking. Because especially as you're scaling, much more and you're expecting. That usage is not going to slow down anytime soon. No. Like that scale is only going to increase. You're going from 18 to 30. I imagine the next goal and that's just until March by the end of next year, it's 100,000 much more complex. And then let's say the other thing which is obvious now is that the complexity of use cases comes with a complete increasing costs right in the past independent of the the complexity of the question or the task, the usage was the same. Now there's a correlation between complexity and users which also good from the business point of view because it's all the bigger problems. So it makes sense, but still, you need to to, to start paying a lot of a tension because it becomes a significant operating cost if you let it go.
 Yeah, I mean I agree I think.
 Deep inside. You believe that?
 The return of investment of this is always posted somehow like maybe not in the beginning because the learning curve will wrong. Wrong agents are being built or tools are but
 Over time with people learning with the technology is getting better. That option, getting higher, you know, that returning of Investments is going to be higher for the business and every business wants to grow more.
 and I think we all believe that AI is going to be
 the game changer of how fast the business can grow or how fast the business can fail.
 So do you really want to try to optimize your costs on these ones? Or make sure that this company will get two times bigger in five years time or will shrink half in five years time. That's so that's, that's the that's the concern. Obviously, you will optimize those things over time but I think truly, we all believe that return of investment will always be higher for the business. If you will, make people use these Technologies, much better than they have been using a year ago or two years ago. So so you need to have conversations with your CFO or other leaders to to manage that.
 But at the end, like if people are using it, they are finding value out of it. No one is just
 Inherited trying to waste money of the company. Yeah, they get more out of these things and then the company spends obviously, probably some of them don't have the most optimized prompt or the right model, but I think these are very small unit cost at the end that is not the biggest function of me. My concern is how we are going to grow the company to three times. In fact, this time and how AI is going to enable that
 Something funny happened to me, six months ago, I asked deep research for a report on different GPU providers and it was absolutely shit. I couldn't figure out what each nio cloud value prop was so that set me off on my latest side quest of creating a practitioner's guide to choosing gpus. I'm happy to announce this guide, is now ready to see the light of day and you can download it for free right now by clicking the link in the show notes, we've already got some community members feedback of what they wish. They would have known before signing a gigantic contract and I would love to hear from you if this provides any value or you have some things that the rest of the community should be thinking about when they're on the market for gpus. Go ahead. Download that resource right now. Completely free.
 It's in the link in the show notes.