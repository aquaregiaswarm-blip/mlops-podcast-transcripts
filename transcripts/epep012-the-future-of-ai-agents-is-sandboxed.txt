I don't know if this is the right term for it. Like the GitHub ease of like workflows is going to be important. It could be, it could be that that someone
 Figures out a very generic and extensible way to have like a staging environment that like outputs work, the people approve, or don't, it's maybe more open-ended but I think that's that's going to be ultimately another important piece of this puzzle.
 Sandboxes are very hot, these days. They are the topic du jour and I would love to talk with you since you have been in the weeds of sandboxes.
 For so long just break down. What exactly do we mean, when we hear that term being thrown out in the wild,
 Yeah, so I I think it can be in a few different things but in in the context of like, agentic execution, typically what this means is is you're going to give an agent its own sandbox environment where it can do what it pleases and you don't have any kind of concerns of security, kind of vulnerabilities, right? You know, it gets to operate and it's on isolated environment, you know, kind of a key to. This is all so setting that up so that the agent has any tools or any context that it needs to do its job. So I think, typically, you know, when people talk about sandboxes in the engine to contacts, but they're really talking about is giving the agent effectively, its own computer.
 you know, I think we've seen that agents tend to perform better the more tools, you give them, giving them their own computer, I mean that's effectively like the most powerful tool there is really
 And are you giving them beefy compute as they give them? And how do you look at the different?
 Environments that you set up for them, is it something that you want to make sure that they have tools to go and search the web, or is it something a little bit more constrained?
 Yeah, so on the kind of beefiness of the environment, you know, you can create these sandboxes via API on our platform on run Loop AI. You can specify the size you want. You can specify how much memory and how much compute and how much disk you want to be available, you know, depending on your workload. It might be that you want, lots and lots of these things running in parallel and relatively lightweight boxes, or if you have heavier weight tools. Like if you're messing with FFM PG or I don't know sometimes like npm and PM can be pretty beefy too. So you might, you might need more compute. So it's really kind of Dealers Choice there. So we have like, default sandbox images that we launched that have like, you know, common tools, you know, like they have Python and like nodes and stuff like that pre-installed. But most folks end up using one of our products called blueprints which lets the
 Users specify the precise context that they want in the container image. So that basically a blueprint is basically a Docker file plus, plus why is it more than a Docker file? So there are certain things that are hard to express and a Docker file. Like Docker in Docker is kind of awkward to express from one Docker file which we support. So, the blueprint, lets our users say, hey, here's the laundry list of stuff that I want in that container when it launches, as part of the sandbox environment. Let's see, interesting things that we include in the blueprint context, we have an object object API where you can upload arbitrary objects. You can say, hey I want that on that container image. We have an agent API where you can specify agents that you want to include in an image. We also have a notion of code mounts where you can say, have like to include this code repository in this image. So, these are
 All kind of part of the toolkit for configuring, your sandbox. The blueprint is a way to kind of statically ahead of time. Build the container image with all that in there, you can also do it when you're experimenting or getting set up, you can do a dynamically when you launch the debt box. So you can dynamically put all that stuff onto your sandbox environment at launch time. So I think developers will typically start launching Dev boxes and dynamically adding the stuff they need. And then when they're like, okay I got all the right recipe together. Let me put that into a blueprint. Now it's already and it's just it launches that way. It's just faster for watching like that.
 So many questions for you, man. Like first off, you're not
 Specifically.
 Creating Tools in these different environments, are you?
 It's really up to our users to to build their agents as they see fit and choose how to compose them with with tools and skills and things like that. And that's how the repo comes in to play. It's like here, throw this repo in there, then the Aging can understand what's going on and it can
 Grab the needed, mCP servers, and look in the different directories or whatnot.
 Ah you you certainly could do things that way. So typically the the code Mount is really quite literal, it just means, tell me what GitHub repositories? You want in this image, and I'll put them there and it might be a code repository that you intend to work on, like, hey, I'm gonna write some code in here or it might be a code repository that you're using as your suggesting as a source for like tooling and whatnot. This also kind of speaks to the age and API. Like these are very powerful and composable, kind of apis. So there's many ways to to skin any particular cats so to speak. So the the agent API, similarly lets you specify an agent, you'd like to put on the debt box. You can specify an agent that you want to be made available via mpm, or you can also point to a GitHub repository. Or you can actually upload an object, that is a tard up version of your agent. So you have
 Many different ways to kind of compose these things and ultimately, it kind of becomes dealer choice on how you use these apis.
 Do you find that folks will run?
 Various agents inside of one environment.
 Yeah, often sometimes people will run one agent, they'll have a workflow like some folks, use things like temporal outside of the Dead box and they will launch the dev box with whatever Target context. They're trying to work on and you might use something like temporal or any sort of other workflow thing, this may be part of your product layer and then you'll reach in to the the dev box environment. You'll invoke, you know, maybe a first agent when that completes, you may be move on to another stage of your product workflow and invoke a second agent.
 You know, and it's possible that you could snapshot or suspend and resume. So our Dev boxes have useful, orchestration Primitives. So snapshot is hey, we're going to take a point in time. Copy of this. You can Fork it or you can just save it for later. Suspend resume is a feature that uses snapshot. It's like a nice product feature. On top of it, uses snapshots to suspend a dead box, and then resume it, people who have like human in the loop kind of workflows often use this. Just because if, the, if you're waiting for a person for a while, maybe you should shut down the Box, on the person comes back and makes some decision, you might want to resume it. So it's again, these are kind of composable Primitives and it really comes down to what the application that ultimately are user is trying to build.
 and I think there's,
 something I'm still trying to wrap my head around which is
 Why?
 Have a Sandbox for the agent.
 versus just like having that agent, be some API that you can ping
 Yeah. So
 so there's, there's a number of kind of
 Questions underneath that one question, but um, I I think so kind of in a way you're asking, hey, I just hope host an agent execution on a normal server.
 And I think that that there are certainly agent patterns where you can do that, but I think the second you want the agent to have access to, its underlying compute. Like you wants shell access back access. You want it to be able to author code and execute code for security reasons. You really don't want that running right next to your server.
 You know, if you had an llm, if you have a server running and it's hosting some number of these things. First of all, they have very unpredictable load and usage depending on because they're effectively gonna execute tools and do what llms are suggesting they do, right? So they have really unpredictable usage patterns and if you're going to give them back or shell access or access to to dangerous tools if that's running next to your computer, I mean, there's nothing to stop your agent from killing the server. It's running on or RM, Dash RF in the root file system where your server is running and that will be problematic. So, you know, that's, that's really where the kind of the sandbox term. Really originates is your putting it in a sandbox where it's isolated and and it's kind of blast radius of things that that it can impact is low. I think, typically speaking like, you know, I mean, the LMS in the agents are usually not doing silly things, or usually trying to
 Make like forward progress but they can make mistakes or they can also just do things like that are very resource expensive, right? They're just chew up all the cores and memory on the system. So for this reason, like, you know, unless you have an agent with like a very specific and narrow use pattern, if you're looking to give it access to things, like Bosch tools compilers, the ability to execute arbitrary code. If you're looking to do those types of things with your agent, you probably want to in its own sandbox environment. And that's kind of like the
 For security and isolation rationale for doing it. I think kind of the flip side of that coin is just opening up the world of capability to your agent, right? You know, as I said earlier, I mean effectively like a computer is, is like the most valuable and Powerful tool out there, right? Why not give that to your agent? What that lets the agent do. If you give it its on sandboxes, you can really open up. It's it's toolkit. You can let it do a lot of things that would otherwise be a little nerve-racking to have running on your server, right. So you can say hey if you like clone repos download stuff from the internet parse files. Hey you have full access to the file system, right yourself to do list and go work your way through the to-do list. You can write sample code or intermediate state to the local file system. So you, you know, you really do end up by giving your agent its own computer, giving it the, a lot of power and a lot of the kind of
 Open-ended capability.
 So many great things there. And yes, thank you for clarifying that because
 it was like, I had the
 inkling of yeah sandboxes. Of course, there's limiting the blast radius, you don't want to doing stuff in the same place that you have, maybe the production environment, because it can go AWOL or just. Yeah. I think they're causing trouble the answers. Really. There's like the pro in the con and they're actually both opposite sides of the same coin.
 The the con is is you're like oh my God, I don't want my stuff getting routed or destroyed or like maybe ages gonna do something bad. But the pro is like
 Hey like this agent's cool and smart, give it the best tools and yeah, we should. We should give it access. It's it ends up with the same answer, so like, I don't know, pick pick your rationale but I think you you kind of end up with the same result either way. So if I understand this correctly, then you're saying
 for an agent that
 maybe is able to
 look into databases. Are you copying a whole databases over into these environments so that the agent can play and do it at once? But all so if you have Johnny drop tables agent, it's not going to destroy your last whatever year of data. Yeah, you certainly could, I mean, there are a few different ways you could, you could do that.
 So kind of earlier, I mentioned the object API to you on the object's API just it I mean it more or less is like a product layer in front of something like S3, right? So you certainly could take a copy of some set of database tables, put it into the object store and build it into a blueprint or make it available when you launch the debt box and then your agent has its own private copy.
 kind of the other way you would do something like that is you would maybe use an mCP server that had scoped credentials that said hey I have like renal he access
 To to an external database. So that would kind of be the other way to do that sort of thing.
 I want to just,
 Touch on one other piece that you mentioned and it's worth highlighting which is the agents can be resource intensive. And so, when you have a Sandbox, you can kind of control that and make sure that if they do go a little bit wild with the resources. It's only up to the amount that you specify beforehand. Yep. Yeah. It's there in their own isolated sandbox. The most they can use as the full size of the sandbox and
 Um, we actually in our product layer, we'll start to alert you we put into the log traces. When you go over, I think I don't know if its 80 or 90% of memory or CPU is but we actually drop an alert. So we're like
 Hey.
 Getting hot in here. Maybe I need a bigger sandbox, or maybe you need to tone down the stuff you're trying to do one or the other.
 And is this because this is really.
 Geared for production. How do you see it? Scaling horizontally, when you want to start doing that? Is it just replicating the same sandboxes or is it getting a bigger sandbox?
 Uh, yeah, typically people just stand up a large number of sandboxes, like some of our customers will stand up like 6,000 at once. Wow, nice. Yeah, these are sometimes, these are people who are trying to drive like rft or like jobs, like that, and they'll just stand up a whole pile of these things at once. So, kind of the point is, there's kind of an interesting observation here. This is, I think another one of the reasons, you know, if you step back a little and you look at the success of things like Claude codecs. And now actually Lang chain with DB agents. They, you know, increasingly have this metaphor of a really direct computer use, right? Like when you run Club from your terminal on your laptop, it's using your laptop as if it was its own. Right. Same thing with codex. So you're starting to see this this pattern become popularized of like you know agents directly using compute and you know I think there's
 Another advantage to that we mentioned a minute ago. Maybe using something like mCP to give, you know, an agent access to non-local resources, right? And I think that, if you look at the concept of bundling and agent with its code sandbox, you have this nice Atomic unit of like okay. Here's your existing tools. Here's the amount of compute and RAM and stuff. You're allowed, you know, hey, you're in this box and I know you can use all of it, but this is like a nice Atomic unit, and you can actually then assign identity to that to that specific dead box, right? And that identity can also be part of how authentication to mCP. Tools is handed out.
 So I think there's there's just a huge number of very practical reasons to think about deploying agents and this pattern.
 In the identity would be whatever. Johnny dropped tables, agent.
 Yeah, Johnny drop tables agent is the here's the agent ID, it's running on this sandbox ID. I provisioned this sandbox with this specific MCPE access credential which is read only database. So Johnny dropped table Johny table doesn't go too crazy and drop my production database. He has been known to do that. He has been known to do that.
 He can't give him that access man, that's the problem. Wow, yeah I mean that the the database nerds would say you can also you can also create virtual databases that are clones of your production database to solve this. So there are many layers of the stack at which people will solve all these problems you know and actually all the way back to your database question earlier. Like hey can you provision a database onto the dead box? I had suggested people using objects for that. We have a bunch of customers that use Docker files for this. You know, as I mentioned earlier, we support Docker and Docker people will take postgres jam it in a Docker file was some amount of storage or some subset of their production database, and use Docker and Docker for that. So, as it's in computer science, there's always, there's always many particular, there's always many ways to solve a problem. It's just, what is the most expedient, like, judicious way for you to do it now or with what you're currently trying to do.
 Well, it's interesting. Before we hit record you mentioned that agents work a lot more like humans than traditional software.
 Yeah, I think that's I think this is all like a thing that's makes the whole
 Give an agent, a computer Insight even more valuable, right? Like I mean, think about like you've I'm sure you've worked on like a rest server or coded, you know, maybe a fast API or a next server, right? Think about normal servers, right? Like you have schema inputs, right? Like, typically raster maybe grpc with Proto schemas, right? Like you have some amount of like hey, work comes in this door, right? But it's, it's strongly typed, right? And that Maps, somewhere deterministically to code right and that code executes. Sure. Some requests are more expensive than others, but by and large, there is a determined deterministic set of paths through your system, right? And ultimately state gets stored probably in a database. Maybe something goes out the other door to a message or some other API, but you effectively have this deterministic mapping of like requests and inputs.
 Storage, maybe outbound requests, right?
 Now, think about an llm, right? It's natural language is coming in one side, it has some tools, it can invoke but you have no idea if it's going to you know it's an inherently probabilistic probabilistic process and you know, if you even look at like you know clog code or or what a lot of these you know agent Frameworks do now, they start doing things with the local file system. They're like ah yeah it's a big task. I'm gonna write myself a plan and store it on the file system just like you know a human being at their laptop you know it work might have a white like a white piece of paper or a pad next to them be like, oh yeah, things I have to do, like check that off. So I like dump, you know, let my teammates sound, right? So you start to see like agents kind of, you know, not to answer for modifies them but they, they have like very different working patterns. If I'm sure you've seen this using agency where you'll ask it to do something and it may be isn't 100% sure about an API
 or a library and it'll be like, hey, I'm gonna go over to the internet and download this this API or download, you know, some some readme documentation
 Like that's how I work like, you know. Okay, hey, I want to go Cody against this thing. I'm pretty sure I know, I need to use this but like,
 Amigo. Look at the documentation. So, you know, I think kind of the way that agents use compute is a lot different than a traditional server or even a Lambda function which is just kind of like a point in time, one requests slice of a server, right? And we we really do believe that what we're working on here is kind of a new compute pattern.
 And, you know, like, you might you might pause and be like, what do you mean? It's a new computer pattern. It's just like, you're just using a computer but like,
 The think about this, right? Like you're you're effectively saying I have one piece of software that's going to in an open-ended fashion, use its own computer and and if you think about the last time we saw like maybe a new emergent type of compute pattern, I would argue it's kind of maybe the Advent of spark and like databricks write.
 Like where they're like, yeah, spark is kind of like a bunch of servers that like work gets farmed out to, but it has this workflow and a dag. And like, so I kind of think that this might be that different in that kind of unique that this is just kind of a new compute pattern and probably two or three years from now. It like, probably the language around all the stuff we're doing, and other people are doing. We'll get standardized. And you'll be like, oh yeah, there's these dedicated platforms for running agents and like, yeah, of course, it should be that way. Yeah, this is, this is the artwork, we're trying to walk along that to kind of establish this as being, you know, just that
 I could not help but think about Neo in The Matrix when you said, oh yeah I need to know it's like I know Kung Fu. We're now there I downloaded what I needed and now I know Kung Fu maybe that's that's skills, right? That is exactly the conference skill.
 the other thing that I wanted to hit on was,
 when you have these environments happening and you have these agents,
 Kind of getting the Run of it.
 Are you observing everything that they're doing to then be able to go back and eval?
 What?
 They chose and how they chose it. Yeah, yeah. So that's a really important question. So that actually touches on another an entire, another part of our kind of products we that we haven't talked about yet. So let me give you the straightforward answer and then I'll give you the less straightforward answer. So the straightforward answer is, yeah, we have like good build-in observability and debug ability as part of our platform, so you can see an audit everything that happened and that took place for the agent and we'll collect trajectories as well. If you can figure out a platform that way. So that is a great way. If you to be like, hey, like what actually happened here? Let me know about it. But as you kind of talked about evals, I think that's another very important thing. So we have a product we call benchmarks and what benchmarks. Effectively, let you let you do is, is take Dev boxes and unknown state where you can drop your agent on to try to solve a problem and you
 You can tell us, you can give us scoring functions to evaluate how well your agent did. And we think this is pretty important, right? Like, if you've gone through a bunch of effort, to build an agent to accomplish some domain specific task, you want to constantly measure your agent's ability to perform that task, right? And and given that this is a stochastic system. It's, it's not, it's not like a unit test or an IT case, right? It's not gonna be 100%. And so what the Benchmark product lets you do is create a suite of these tests, run your agent against these tests, observe the results and say like, okay great. Am I getting better or worse or let's say hey like I guess Claude 45, Opus just dropped.
 Let's say you're like you know, I'm gonna spend the money. I'm gonna use the more expensive model. Well, how much better?
 Does your agent perform with it. Well, so important just go right into the exact same bounce more flip. The Molly you're using run through the Benchmark in measure the difference.
 and,
 I think you you had called these evaluations. I think it's worth pointing out some rash.
 Yeah, there's a little bit of a rationale so why we call these benchmarks?
 Like a lot of evals are a little more tightly scoped to, like measuring the quality of independent llm requests and responses. Well, we're really trying to do is longitudinally measure the end-to-end ability of the agent, to accomplish its task, maybe across hundreds of llm calls, hundreds of tool calls and oftentimes to evaluate the performance of the the agent.
 You need to understand the context that was working in. So let's take a code base, for example, right? Let's say that you have a coating agent that
 I don't know is updating dependencies or something like that, right?
 You might start off with a repository, in a known State, you know, you might invoke your agent and then you're scoring function. Would look at, you know, your dependency file after it, made the changes and say, okay, like are all the dependencies of a higher than this this number or not, like it should have updated all these things like all score that but you could also make sure that the repository builds, right? Like it didn't hallucinate some version that didn't exist. So this is an example of where like take that relatively mundane or simply example. I just gave you.
 Probably the agent's going to talk to the LM. If you dozen times and make at least four or five, maybe more tool calls, and then for you to evaluate the correctness of this results, you need to look at the file system, okay? Like what is the state of my package Json? Or my requirements lock, or whatever UV lock file, right? And like, yeah, by the way, like, does this build? Because you could have made up some new version that is like higher than the old version, but doesn't exist, right? So again, like to get back to the benchmarks, the point is, is that, you know, for people out there, practitioners out there, you're gonna build an agent, you want it to accomplish some specific goal, you need to consistently measure that you're getting better or at least, not losing ground and we think benchmarks to the right way to do that.
 in your initial question, you hinted at
 A notion of can we take a runtime environment and maybe turn it into a benchmark? We're not there yet. I think that's ultimately. The coolest thing would be, you know, for for someone to have their production traffic running against a bunch of a bunch of dead boxes and for them to flag to us like
 Hey, like that Benchmark, you know?
 I'm sorry that debt box. Dbx 12345, whatever.
 Ah, that didn't end so well, can you like capture all the logs and the initial State and turn that into a benchmark? So I can like I can go work against that again and again until I improve that particular pattern.
 Yeah.
 Those fail cases.
 are almost like,
 The there is valuable as gold, right? Because if if you have an agent that is working well against your benchmarks.
 And then you start to see oh it's got a little bit of a blind spot there. That's very valuable but what you already have said up, sounds like it is infinitely valuable just to understand.
 The basics of is this shippable.
 Is this shippable, does it do what? I think it's supposed to do most of the time.
 Does it you know with some with some percentage points of like just chaos? Does it does it do what I want to do most of the time you know and and you can get into
 you know, some of our customers do pretty cool things where
 They'll you ask questions about multiple agents before?
 They'll have an agent run and then they'll have an agent that runs after it that evaluates, if it did a good enough job and then kicks by the previous agent, right? Kind of the elements of Judge type thing. So, you'll have these
 agent that did work unit, a agent, the judge work Unit, A and then pass you on to Agent B or kicks back to agent a
 Um so you there there you know, once you have these Frameworks in place there are a number of cool method allergies you can start to implement to try to like increase your accuracy right? And make sure you you don't produce low-quality results.
 Well, I really like the idea of looking at it as a holistic.
 System as opposed to just did this one llm call. That's one of
 Potentially hundreds actually work. And if you think about things like code, right, like, the, the yellow on might be telling you to pass one file like, okay? Like,
 It's very hard to judge. Like okay that one file patches immaterial in absence of all the other patches you just did right like that piece. Or that single album called is is not particularly useful, right? Like without
 The tool chain the rest of the code base and all the other patches. So I think ultimately it's just it's just measuring things at a different scale, right? Like what is what is the longitudinal result of like my my end-to-end execution as opposed to like finer grain.
 Do you remember that Meme back in the day? That was the little girl looking at the camera in the burning house is behind her and it said, you know, worked on my machine. It's an Ops problem now. Yeah. Yeah. It just reminds me a little bit of of that. How your
 Basically helping so that that is not happy. Yeah. That mean is not a reality. Yeah. Yeah. And that is that is one of the all-time greats.
 You mentioned that you have folks that are using 6000 sandboxes when they're scaling out.
 Are all of these agents. Highly isolated is there any type of multi-tenancy that you can have? Is it like you want some kind of a shared State? Sometimes, how do you see the different customers working through those questions?
 Yeah, that's a good question. And actually an area of maybe some active development for us right now. So our sandbox supports the notion of opening tunnels. So when you first launched a Sandbox by default, there's no network access to it, but you can say, hey, I'd like to create a tunnel. I'd like to be able to Route network access to this and and some people use this to implement like a product ux, right? Like they might have a front-end server running pick place right for sale Railway or whatever and then they launched an agent and then they want to communicate with the agent, they might open a tunnel. We support websockets, you know, HTTP, whatever.
 But what you can do, if you want the agents to talk to each other, as you can open the tunnels and they can talk to each other over the tunnels, one area that we're starting to look at. Now, the the folks that Lang chain and the Deep age of team has been encouraging us to look into. This is a notion of what it looks like for these agents potentially have a shared file system, so if each sandbox is totally isolated, what does it look like if you mounted some sort of shared file system? So they could communicate through the file system and that would be this potentially interesting.
 It's not, we have not.
 This is not something that is like eminent from a product perspective, but it's probably they're probably a couple things that we're in a prototype sooner or not a notion of volume mounts. Like okay, how do I have a notion of a part of the file system that I can save, you know, because when the dev box is gone, it's gone. But how do I have like an output directory that gets saved some place? Where I can, I can retrieve the agents, you know, intermediate work or its notes or whatever later. And then hey well I mean once you have like some sort of file system abstraction that you're saving outside of the Dead box. What if it was read write shareable. So these are these are things. We're toying with we need to make sure we have the right use cases and the right technology before we actually turn it to products though.
 and so right now has the philosophy, been very much along the lines of cattle, not pets, type ideas, catalog pets, meaning like the
 I don't know if you remember that they can.
 I heard it from devops folks where they would say you want to treat your kubernetes class? Yeah cattle where you're okay killing them at any point in time. Yeah for them.
 and as a vegetarian I'm just gonna say I do not like this but
 it was what I heard and lightning strike ahead. Yeah, I got you. I got you. I got you. Yeah, yeah, I think I think it's also just it's
 I think a there's that like which is just like, I think, effectively what you're talking about is like recoverability, like is something got halfway done and I don't know, like, you know, we run on top of AWS and other Cloud providers, they're good. But they're not perfect machines. Do go away, right? So that would be like a recoverability argument. Like, like how do I like recover? If an angel went away, I think maybe the more interesting thing though is just like what happens, like what does that allow? Interesting? Like inter age of coordination opportunities.
 You know, you could imagine, you could have a pipeline where you had agent a on one dead box and Agent B, and then agency all on separate debt boxes and, you know, you start off a pipeline by asking agent a day to do something and it right something to a file system, then you tell agent or to read that and continue working on it. So you know you might you know and so on it could pass the agency. So you might end up just having different like orchestration patterns that you make available too.
 Ah, but this is, this is a little forward looking, you know, I'll be sure. I'll be sure to give you.
 Give you an email update. Yeah, an update when we when we start to formalize some of these things.
 well, it also feels very much like the idea of
 giving the agents memory in a way and how you can let these ideas, whatever it is that they're working on or whatever they've learned. How can that persist throughout?
 After the sandbox isn't around anymore. Yeah, yeah. I think precisely and what, if the, you know, can you let agents self author or skills or tools from themselves and save those? Um, I think certainly for the next one. Yeah, yeah. I think that's where this starts to become like, quite an interesting
 Well, there is another thing that I wanted to get into around like Frameworks versus harness. Yeah, I thought you had some great points on that. Can you break down like, how you look at these two things? I think, you know, kind of the observation I think it's, um, Harrison Chase from Lang chain, is has been kind of pushing this like distinction of framework versus harness and I actually think it's pretty important. One, I agree with them on this one, and I think that I were about to see, a lot of people take advantage of these harnesses, so to speak. They just make it much easier for them to take, like a really capable agent, like, Clarke code or a deep agent and then build on top of it with their own kind of customizations. I think the harness approach just it's more batteries included, it just makes it easier to accomplish a goal, you know, still power users will build. I'm sure from the ground up there on stuff, you know, like startups that are exclusively.
 Tackling one specific thing. But for people out there who are looking to build like simple workflows, or just automate simple things, I think this will be very powerful
 and I'm not sure if I fully understood what harness is.
 Exactly.
 When you're using something like the Claude agent SDK or Lane chain deep agent, you're kind of assuming that what is already. There is already good at managing contacts doing things like planning and doing things like tool calls and maybe you provided additional tools but you're maybe not really writing the code for to implement tool calling or you're not really writing the code to manage contacts. Like, I'm sure if you use cloud good, you see where it compacts, it's conversations, stuff like that, right? Like doing that really well is hard, I'm sure, you know, startups out there that are purely age. Antics startups are going to have their own stuff, top to bottom because there are Specialists one. It is they do the people out there looking to build an agent that takes two PDFs and turns them into one database entry.
 They might be able to build on top of this and be like great. I have this PDF reader skill. I have this like here's my schema export to database skill and like I'll just give you these inputs, you know how to call tools and skills. You know, if they're using Claude agent SDK. I'll just build on top of you and you're a very capable kind of harness for managing contacts, invoking, tools, making a plan and sticking to it and updating the plan. And then I think it's really, I think the distinction is more about how many batteries are included and how much is the thing just do for you out of the box without you needing to do that much?
 I hadn't even thought about how
 with Lane Graf.
 It was very
 that determinists you're always playing with a determinism versus non-determinism model or way that you want to build your agent and
 with Lane graph, you could
 Get very detailed on. I want you to do this and then I want you to do that. And then if you need to, you have a loop and you, you keep doing that. And then once you get this information, you go and do that, right?
 uh, but
 Harness.
 Throws all of that out the window if I'm understanding it correctly and it's saying it comes with the ability to know what it needs to do next good enough.
 Yeah, I think if you look at how like like the the cloud agent ick, implements skills, you can kind of you can do it as natural language in a skills. MD file, you can like kind of specify like do it, then do be then do see and then the harness is good enough at ingesting that and doing it. I think for deep agents, let me I believe it's called the feature.
 So they're no notion of a skill I think is called the feature but it's a similar concept. So you you're in a state of you coding that you're saying here or kind of workflow Primitives or like sequences of steps. I want you to take I'm going to put them in feature files or skill files and then you're dependent on the harness to be like adapted it following those. You know again first I think for easier he is cases. You're taking advantage of the fact that the that these agent harnesses are just getting smarter and smarter and hopefully you have to do a little less to get an equivalent result. I think that's, that's kind of maybe the better crystallization of what it was. I was trying to say
 Here I think directionally. This is going to keep happening to
 Like I think it easier. These underlying cores are just going to get more and more powerful to the point where, you know, the same with codecs as well. You know, not to leave them out of the conversation but, you know, they they have the advantage of huge use. They can tune models specifically for their own agents. You know, they can invent their own kind of, protocols and conventions for things, like skills, and mCP. And I think maybe at least for people doing relatively simple agents, I think it's going to be much easier to just use these harnesses.
 Those make a lot more sense because
 I remember seeing a lot of,
 Friends graphs in in Lane graph that had a lot of different nodes and a lot of different steps and there was a lot of logic behind you do this and then you do that and then if if you have this or if you have that like think about it or here's some it was almost like a glorified workflow and I do know that the workflows in a way are still quite valuable and I was just talking to a lot of folks when I was in Amsterdam last week about how they'll bundle up.
 A workflow if they see that these three tools are generally called together, then they'll just bundle that up into a workflow. You know, because it's not like we want to have any
 Question about if you're calling this tool and then this tool you don't call that tool because it's just like no these are the three tools. They're always called together and we're trying to abstract up one level on the tool calling and
 It feels like.
 this idea of harness is very much that like when we had sit on here from
 Claude he mentioned,
 One of the biggest things that folks were building agents these days are grappling with is how much harness is too much harness.
 I think that's one of the things that clogged really excels out as well, too is to use. Let's a little bit of a tangent. But yeah. It's very, it's very judicious about one and how he uses tools speaking of the tools.
 And the Agents being able to execute tools going back to that conversation with Sid. He mentioned that a lot of their
 in big improvements and performance gains that they saw with, Claude code was when they were
 deleting stuff.
 From what Claude code had to do. And so instead of saying like,
 It means to use this Bosch tool.
 It would just say, like, here's how Bosch works. Yeah, you go and figure out and create your own bash command. Yeah, and things like that.
 which ties into the environment and it also ties in to another thing that I've heard is very difficult with agents, which is
 The.
 Verification or the making sure that if you wrote something as you mentioned earlier, does it actually does it build?
 Yeah, can you compile it and will it run afterwards? Or is this just some random thing that looks better?
 But doesn't work.
 Yeah.
 Yeah, I think the, the Bosch point you made is kind of goes back to the give give agents their own computers Point. Bosch, like, if I don't know, if you've ever had a friend, who was like, one of one of my co-workers is just like unreal with Bosch, he'll chain like 30 commands together. I mean, I'm, I'm a capable bash user, but like, he's next level,
 But, you know, something like that, you can do almost anything. There's like, I mean, we're talking about 50 years of like, Unix tooling that has gone into this, kind of overlooked program, which is Bosch between like awk and said, and rip crap and get crap and like JQ and like pipes like
 You can do almost anything. So, to some extent this kind of goes back to the like,
 Give an agent a computer. Like, well, you were just kind of observing was like, Hey like you know, how Bosh works. You know, the common tools and Bash Lake
 Use them. Um, yeah, I don't know how often you use any of these kind of CLI tools for coding, but you'll often see like
 if you ever like move files or do stuff like that, you'll see, you know, in a larger could base, you'll see that they're like, oh my God, there's too many references to the file. You just moved from me to hand at it. I'm gonna write my own bash script. Yeah and they'll do is they'll do like, get grab pipe, XR pipes, sad to like replace Imports or like,
 it, you know, and you're like
 Kudos like no one had to write a tool for that. Like you you're you were able to use like a composable sensible tool. That is your own computer and in a way that is like empowering to to you as an agent.
 So I think that is like, that is pretty cool argument for for less is more. And having like, you know, more powerful basic tooling components being a real lift.
 What else is on your mind these days?
 When?
 Enterprises really gonna get going a little bit more with agents. I think it's quite interesting.
 I think that everyone has used by hand, you know, some form of coding agent or, you know, if you're curious, you've authored an agent seen how powerful they are. But I think it'd be very interesting to see when Enterprise kind of starts to catch up and you start to see more like actual agents live in production. For for Enterprise's I think that's going to be an important part of just the broader industry life cycle, right? Like you know Enterprise adoption I think is important is there
 are there some things that you feel like are
 Big blockers to that right now.
 Yeah I mean yeah so okay good leading question. Um so well I'll finish my other two Curiosities which I think are related to what you just asked. I think I'll also be curious to see the future of MCPE and of things like skills.
 I think these are all important components. I would say to your question, like, what are the blockers? I would say until recently, with some of these harnesses, it was, it was, it was hard to build agents, it wasn't impossible. But like, I do think that like, the batteries included aspects of these kind of harnesses, like, Cloud agent, SDK, and Lang deep age and codex. CK are gonna kind of democratize it, or make it easier for people to build agents. So successful. So I think that was one piece. I think people are still in their head deciding on it. But I think, you know, having a very simple isolated sandbox on time environment like what we're building. I think that's an important piece of the puzzle like companies like mine. There are other people trying to do this stuff or they're a lot of cool companies innovating in the space but kind of the existence of a very easy deployment fabric that secure for deploying and observing and auditing agents I think is important.
 I think another thing that like mCP, and to a lesser extent skills, kind of addresses, how do you actually get context into those agents? I think there are a lot of people working on making the auth story around mCP, a little more graceful, I think that's going to be an important piece of the puzzle. You know. Basically giving short-lived read-only credentials, access to these agents to operate in an Enterprise environment is going to be important and well, I have one more that I'll add. I think one of the reasons that
 Agents have been so successful. There are many reasons, but why they've been so successful in coding so far
 Has been the pre-existence of a workflow that lends itself to agents being involved. And what I really mean is things like
 Get and pull requests.
 You know, you and I are Engineers. Let's say we were working on something together and you're like, hey John can you add something?
 Built in to get is this effectively like, you know, immutable audit log of every change, that's taken place in the repository. And not only does it have this audit log, right? It has this wonderful feature of just a convention by one, I want to make a change. I'm going to send you a review and you're going to look at it and say, that's good or bad or John, that's John changed this and then once you've approved it you're not both of our names are on it and it gets out at it.
 And there aren't that many other systems with those proper properties, right? Like like lawyers, I guess you have red lines and like Microsoft Word or something like that but you don't have like a proposed review.
 Like Journal approach, right? Like for for most other.
 Most other that I'm aware of professional workflows. And I think that that's actually like
 That'll be like, let's just say MCPE and all the stuff is magically solved and you know, companies like mine and others, like it trying to build this sort of fabric or perfect and deployed everywhere. And these harnesses are so great. You can just drop a bunch of skills and a few mCP Tools in and everything just works. I think that still of the piece, I just described to you is, is going to be an important workflow component that I think is still kind of missing, like, what is the poll request of like an insurance claim order?
 Yeah, or marketing? Yeah. Or any, any other industry really, right? Like and I think that it's really a notion of like staging, reviewing accepting working committing it. And having the ability, maybe it's unclean or harder in those other kind of spaces to revert, right? Like you do a get reverse like pretty straightforward, right? Maybe it's a little harder to reverse a table update in workday or Salesforce, or something like that. Yeah. But I think that that like I think that's like there's just something I'm noodling on. Its not, I don't really shoot his run Loops problem, but I view it as like a broader industry problem of like
 What is the GitHub of like all these other systems that agency need to interface with? Because the, you know, the GitHub thing makes it quite Nashville for you to have like a, you know, an agency. You're like I go like do a pull request to update these dependencies or something like that and then you review it and you're like, yeah, it passed CI.
 It passed the eye. It looks to me.
 I approve done. And if something's wrong, we can refer
 Put also it.
 Has that ability of does it run? Or does it not? Yeah, I step is important too. Yeah, and like
 kind of, it's kind of a funny thing where you're just sitting in this like,
 Rather highly evolved tooling ecosystem. You kind of take for granted
 and you just don't, you know, it's not top of mind that like other Industries. Don't necessarily have this, or maybe, I'm wrong. Maybe some of these other Industries, do have these things and I don't know, but I think this will ubiquitous. Yeah, I think Leonard is common. It's yeah, I don't think there's a GitHub for lawyers.
 Know.
 and I wonder too if that
 is one of the reasons that the training,
 Has been so valuable on all of this data. Like I I not sure the ins and outs on how they trained the models specifically like these, you know, the cloud model, but did they trained it on?
 The actual issues in vrs as much as they trained it on the code.
 Yeah, yeah. Well you you now have your your
 Your surfacing an important observation, which is that, like your pull request. Log includes the motivation. The rationale is, well as what them like, presumably ultimately a human did to solve that problem. I think that's very powerful. I think there are other aspects of code that just landed to it. Being one of the first things for AI to be great at, you know, it's a, it's a more concise. It's not an open-ended language, like human language, right? Like you, there's only, you know, there's strict syntax, there's a strict set of keywords but maybe, you know, as importantly, you have like, deterministic verification functions that you don't have for.
 Natural language, right? Like I, I can say, you know, clot or debate or whatever codex. Write me hello world and it can deterministically verify that it correctly, did that job? It can compile, that's one deterministic tool. It's able to run, you know, it can also run things like linters and formatters, but then all you can just run the code and be like, did it say hello world? You know, like think now about a lawyer like,
 A lawyer proposing, some sort of legal. Doc, has no tool that says like, yo, compile that like, did it say, John's deserves to do whatever? I'm writing the note. Like, there's, there's no deterministic verification functions and what those deterministic verification functions. Let AI do is is self-play. They can go try things and they can discover things that don't work and then they can ultimately discover things that do work. This can go back into their pre-train they learn. It's almost like kind of like the alpha go thing, right? Like the alpha go thing way, the OG AI moment from Google where they just basically let the AI self-play against itself. And it just had to observe the rules of alphago and there was a deterministic scoring function of who won.
 And they just let it run and run and run and run and it learned novel things.
 I guess there are many aspects but I think that the deterministic scoring function is a big one, the strict syntax the available feedback. Yeah. You you understand like when you're talking about this lawyer use case you probably don't get as fast of a feedback loop, especially if, you know, have to let it go through Court, that's gonna take months or well here and think about it, too, right? You make a program, like, some trivial programming error, like a liner or a bill, or is going to tell you what line it was on. Yeah. Like there's like, there's no like make for like
 Law. And even if there was it was correct in California. It'd be wrong in some other state. Yeah. How would that even look how true? Yeah and that's not just for law but as you were mentioning before Insurance, any of these areas that we play in, that isn't so clear like does this compile or not?
 So I think that the next area is where he'll start to see, like wrapping growth will be things where you have some sort of a verification function. It might not be as clean and as perfect and complete. It's like, the tooling we have and in software engineering
 And I do think that I don't know if this is the right term for like the GitHub application of like workflows is going to be important. It could be, it could be that that someone
 Figures out a very generic and extensible way to have like a staging environment that like outputs work that people approve, or don't, it's maybe more open-ended but I think that's that's going to be ultimately another important piece of this puzzle.
 That is.
 So cool to think about the GitHub application of other areas of work and how valuable that will be for.
 The future agents of the world. I think, I think it's important. I think it's missing.
 It's like agent, have these inputs produce that output human being acts reviewed it, it was pushed into Salesforce or work day or whatever system.
 Or rejected and told ya go back to the drawing board and fix training and then turned into a benchmark.
 See, you get it actually, yeah.