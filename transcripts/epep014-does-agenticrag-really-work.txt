Sole of an agent lies in how dynamically generates it prompt, which is passed to the large language model. For instance, if I can just say, hey these this is my table. Schema generator SQL query for this, it'll give you a SQL query, but the problem would be, it's going to be highly hallucinated.
 It's been an interesting space like so far. We know we started with, you know, a very basic. Let's say if somebody asks me, like like what ml is for instance, you know, like what is this thing. So, back in, if you remember in high school, we would have, you know, this simple equations like we're given two data points, X1 y1 X2 Y2. And we were asked that, you know, okay, what would be the value of y 38 x 3? And that's extra population. And essentially that's what entire MLS. It's just an instant of flying now. We have so complex, you know, data points. So complex, essentially curves and so complex counter maps. Like, it's multi-dimensional, it's not even three-dimensional anymore. We just can't like, you know, visualize it in a human way. But there are so many dimensions to so much of data that we have.
 And essentially, it's all about, you know, like being able to predict based on what we know kind of, you know, finding out the models which are mathematical functions that could simulate what we have going on in the real world. So essentially what we are coming down to is creating more and more of those complex things be like, you know, image processing, for example, in our live video, feed for our cars, in the way more. That we see out there, they're able to predict you know right now of vintage top based on you know like the distance calculations and that's all like functions and how we got here towards this Gene. I, which is a great word right now, a huge Buzz around it.
 Interestingly, we used to have. We started with neural networks. We had like CNN's convolutional neural networks. We had RNs recruit neural networks RNN was RNs. Were the ones which were used extensively for NLP. The natural language processing. Aim was it, how we are talking right now?
 we should be able to talk to machines too and we can understand like you know, like what
 context of the any let's say website is or any what somebody's talking about but the problem there was

A, he did not have much attention mechanism. The attention span was so low that after like a few words, we just did not have the attention window. That could go back and relate to what was being said. It's like a goldfish. Yeah, essentially.
 And that did improve when lstms came into picture, which was long, short term memory, neural Nets, and then grues came into existence.
 But still the attention span was so low and that was primarily because of the architecture that we were following. What we were trying to do is we were trying to induce gates in this neural networks which could assert him. How much of the context for input that is coming through. We want to retain and how much we want to forget. But those control Gates, they were not efficient enough. They were, there is no way, we could have achieved what we have until. And unless the 2017 Revolution happened, and there was the paper, you would have heard of attention is all you need. Yeah.
 The moment, it came through the Transformer, act architecture, when it was introduced, it just changed the entire game. And here we are now.
 One thing that I'm thinking about, with this model Evolution that you just broke down is what the next architecture is going to be. Because there's things that happen with Transformers that are
 Not.
 Things we want like hallucinations, right? And so sometimes some people will argue. Well, that's like a feature, not a bug and others will say, well, you know, like we really want it to be reliable, and if you're gonna have hallucinations, then it's not going to be reliable. But at the same time, it's AI, its machine learning, it is.
 Probabilistic, I don't know if hello stations are the feature but you're just truly said it's all probability models, right? And it essentially depends on like how even the Transformer architecture you want? So from lstms like as we were just you know touching base on that. We essentially found out that okay. What if instead of you know using these gates to control the attention span? What if we were to actually have some self-attention mechanisms from where these architectures started evolving, we had in coder and decoder. Then we had encoder only models and decoder only models and it started from 2017 and eventually, it was, it just grew so much. Oh, so for instance, in coder only models were aimed at just understanding the context of what this particular text is talking about, and we had these query key and value victories. We're using soft matte functions for assigning probabilities to this forward. And then, in the decoder,
 He wanted to make sure that when it's able to predict we have some masking so that it's not able to induce a dialect from already pre learned you know, words so that for example when me and you we are talking before I registered the input, I should not have any biased induced in me and that was the aim. But the problem is that that bias is somehow induced because humans are also using these systems where these hallucinations actually come from even more and since it's a probabilistic model, as such the softness functions, they produce probability is right now.
 But moving ahead in the architectures, which we are using in production right now. Read was a big change. Yeah, that was the one. Yes. So, it wasn't on the model level, it was more on the system level, and that way. And so it went from like, all right, cool, we've got this really important model but now how do we architect the system around it and so last year rag was all the rage. I think probably the last two years rag was very important and that was like the next step in the evolution. We can say we had the Chachi team moment and then we started playing with it. We started using tools and chaining together prompts and then rag became very popular and
 Then you moved on to agent rag or agentic rag, right? But let's talk about Rag and what you were doing there and why it wasn't enough.
 Yeah. Oh
 Very interesting domain specifically because when the hell you see Nation started to come in. Now we have something that understands you know, the context of language. Now we have a model that wants to talk but it just doesn't know what to talk about. You know, essentially and rag is essentially I always envisioned it in a way that it's like a kid, you know, you're watching him give an exam or her given exam.
 And it's an open textbook exam.
 So that kid is referring to the books and getting you what you're asking it.
 But at the same time, we need to check for two things, if it's referring to the right books. Yeah. When the questions are being asked as well as like, when it's answering how much of the context that it is giving makes actually sins. Yeah, so rag essentially came through that picture. Here we would ask the GPD, a random question and it would start giving us irrelevant result, think semantically, correct syntactically correct? But can actually not so yeah, yeah. And I think I remember rag became very popular, just because of the fact that people wanted up to date information,
 And so then it was like all right, well, we're just gonna throw all the most recent information into a vector store and then we'll use that. And anytime there's something that comes up, we can search the vector store and get that information, right? Yeah. And yeah, and, and that was really one of the I would say Genesis off, rag in that sense and moving on to now, like, where we are with the agentic, AI art that we say Iraq agents. It's actually being used Way Cross different context, for which even it was thought of in the beginning. So as we over discussing, then drag started. The aim was to actually ground the llms, you know, let them make much relevant decisions based on the Victor stores that we have.
 And for example now the ones that we are implementing and it could be a very generic case study. Like if I want to talk to my databases let's say right and I need to generate a SQL query llm can generate a sequel query but would it be relevant at all?
 No.
 Again Halley's nation's comment, it doesn't even have a lot of context and more than that. How can we even make sure because now if I'm talking to a production or staging data it's risky because what if some user would come in and just add a drop table statement right there, my job gets more complex. Yeah.
 But and so why did rag fall over? Like where was why did you switch to a Gente Rag and what are the differences between the two?
 And analogous comparison. So when the rest architecture came into existence, we started developing these risk Services. I remember, we started with soap Services, then restful, apis for the standards, then we delved into microservices. Basically contextualizing each service very specific to the use case, so that it's easier to scale. It's much more contextually relevant and as well as it gives us a much more relevant result, with respect to the architecture and reusability of these and here in a gently. I, similarly, when we are, we are grounding the rags for like, very generic use, cases were great, great getting great results. But now in the multi-agent systems, we are creating those identity tracks for a very specific use case. And now these agents are talking to each other rather than having a one whole some agent for. Okay I see. So the idea is trying to like break it down into microservices.
 yes, and say,
 You're in an agent that has access to this Vector database and another agent can almost like use you as a tool. Yeah. And so the tool is
 Search and retrieval type tool but on our data in some place. Yeah, yeah. And that also helps us in the terms of specializing the context of a specific agent. It's a separation of concern and we can have as many layers for security or for enhancing or enriching the data in between and it becomes individual ports who are just taking care of these things and are you
 Making each agent like, only give only giving access to one database. So it's like, that is the marketing database and you can call that agent and it can retrieve everything. And then,
 Enriched or summarize, the answer and then give it back to the main agent.
 Oh yeah, in somewhat in those terms specifically so aim here generally is that we create agents in a way which are like very scalable at, the same time. Making sure we have data governance in place because let's say also we have different dialects of data across different systems. Yeah. And one of the ways that we can Implement is using a SQL clot, which is a security layer, which can transform, but at the same time, sometimes we, that's not desirable because of different systems in place, separation of concern being there. So what we do is essentially design very specific parts for a very specific use cases and it also helps in the cost optimization. What if this was not to be used in production, but it was to be used for when I say production, I mean for the outer world but more like you know, for internal efficiency of the
 workforce, let's say or onboarding. So, do we really need those kind of resources to put in into those agents versus the ones, which are going to be consumer facing? Yeah. So these help in making those kind of decisions. Yeah, exactly. There's a lot of different trade-offs that you can be okay with I imagine if it's just internally facing. Yeah. And on,
 so many different vectors probably quality, and
 On speed on or maybe you're like, no, we have to get it really fast because it's but reliability. I would imagine is you just have
 less high of a bar. Yeah. If its internal because the internal user is going to be much more forgiving than the external user. Oh 100%. Yeah. And hopefully. Yeah, exactly. And so then. All right, so I'm kind of understanding it. I think the thing that I ask myself, if I'm understanding this correctly, you have a agents that are able to query databases why. Not just make an mCP server for the database.
 We possibly could that could be a way to go.
 but,
 the main context comes through, is like what use case Harvey like serving essentially, if it's let's say
 We have to, like talk.
 Like business talk to a database and they want certain reports and just certain ways, you know. And we want to expose that would be one go through the route of mCP server and create hole in another layer to it. It's
 At all like optimizing how much resource and information we should put into for achieving a specific use case? Yeah, at the end of the day for consumer-facing probability, that might make sense for me, what it sounds like is, you have different use cases and they're very verticalized. So maybe there's a team or there's a suite of folks that need information and you create an AI product that can do. That thing really well.
 So, create dashboards from the sales data or the financial data, or whatever it may be.
 and then you have another product and it's
 in a way separated. And so you have like separation of concerns which is really good. But at the same time, you have to create a whole new product around it is that it? I imagine some of the pieces are going to be reusable and you can say, all right, well, this is similar. We just need to change this and tweak some prompts as if it was that easy. But and give it access to this database and that database. But is, is that how it is. It's like each individual product and then you have to upkeep the products for the internal teams. Oh yeah. It's almost in that direction specifically because
 So definitely we can't like discuss details into the internal architecture but in a very wholesome level that essentially what it boils down to. If let's say if I'm building a bot for like one of the teams in slack which aims at onboarding for instance, and similarly and there is a bot which works towards, you know, working with different databases. So let's say, in sales and these two are gonna have some interchangeable components, but their Victory DBS are gonna be different for instance, because let's say, if it were just creating SQL queries, we don't need something like vertex am matching engine or milva Stevie for that matter. We can use something very lightweight like fires which is Facebook, still running hard. Yeah, created all those years ago and it's just amazing. Yeah. Yeah. But I so I understand that it's
 You choose what you need to use all. So depending on the use case because the use case almost dictates what kind of
 Necessities. You're going to have? Yes. Yeah. So some of it can be oh well, you're we're gonna need access to the same databases because there's some overlap
 But I imagine you're not using the same agents for those. You're creating new agents that because then there could be some like context mixing and that could be bad. Oh yeah. Yeah, that's so true. So the soul of an agent lies in how dynamically generates it prompt, which is passed to the large language model. For instance, if I can just say, hey these this is my table. Schema generator SQL query for this, it'll give you a SQL query, but the problem would be, it's going to be highly hallucinated.
 It would not know where to join. It would not know how to or which particular columns to join on, you know, and Amy is the two it essentially eradicate that middle layer where we have to constantly described these things. Yeah, right. So,
 then it boils down to, okay, how do we create that? How do you make sure that our system or our agent creates, this Dynamic prompt, which is passed to the llm, because llm is gonna do its hallucination on its own site, for sure. We can't stop that. So, that's like playing telephone. Yeah. But how are you making the sequel queries that if you're not letting the LM generated?
 So the prompts are actually dynamically generated, SQL queries are definitely generated by the llms. Okay? So the prompt, which is passed to the large language model, those basically are generated based on the documents which are retrieved by the Retriever and those documents are retrieved in this high-dimensional space through semantic similarity, excuse me, searching
 and,
 That's where the vector DB's role come into so much of picture. Like, what kind of vector TVs do you want to use? Yeah. Do we like really want to go with something? Let's see if I had if I want to search across all the like four 40 million products of Walmart and I want to like find out the products for, let's say all the users. It's humongous data. Yeah.
 Even creating embeddings for that. My gosh, it just boils on the system so expensive and just I can't even fathom how much data that would be. Yeah. And so then you have to decide
 What subset of the data you want and then throw it into a vector DB and you're spinning up. New Vector DBS for all these different use cases.
 Oh yeah. So we make a choice on, like, based on what industry standard is being used. Why, how much the cost if do we have an open source solution for it and sometimes open source Solutions are available like milvus TV, Great Vector DB, you know, but the problem is it can have a little bit of high latency even at the same indexes, like, IVF p8, or IVF PQ for vertex AI.
 But the problem there is,
 If I were to generate, let's say recommendations.
 For my customer base.
 and if I were to generate them once a day,
 I can probably use, you know, an open source. Yeah. Why would I want to spend something on?
 Something, you know, which is really costly for me.
 And it's again, very use case, you know specific. But if it for an online serving model, which is, oh, I'm generating them every half an hour. The pipeline is constantly running or if every 15 minutes in that case. Yeah, I would have to, you know, shout out that cost. And then, that would make sense that to use very low latent. And very highly complex indexable vertex, sorry, Victor, DPS. Yeah, we can actually use. If you got the chance to just start from scratch and build something. How would you go about it?
 Okay, let's take an example. Give you an example. What would we need to feel? What would we need to build? Ah, is there something?
 Is there something that you feel is is uniquely valuable in the e-commerce space?
 There are a lot of things. Um, I would say, the two most prominent examples that come to me is
 so sometimes when we launch a new product, for instance,
 Or a new customer experience. We generally go about doing a multim Bandits or EBT testing. Yeah. But sometimes we already have so much of good control experience and that we don't want to, you know, sway away from
 because it can cost us potential users. So and if we try out even in my damn Bandit pay, like we used homes and sampling to say one or two persons of the users RV. Really willing to take that risk in that case because it's like you have so much you don't want to lose what you have. You're playing your defense in a way. Yeah. Yeah. It's Explorer exploitation trade off. Yeah, you know okay yeah that's fascinating to think about that. It's not like
 Yeah, you can't be willy-nilly because you're already so optimized. Yeah. Yeah.
 exploit trade-off that we generally go around here, we think in a way that, okay, I'm gonna go ahead and use my control group, which is how it is right now, but at the same time and I'm gonna exploit it but I might like explore like one person
 It's definitely a cost to the business.
 But it can heal a lot more but it's very dynamic in map, you know, like we'd go about and this is called Thompson, sampling basically, right. So we just sway the users like that, but another way that we can possibly try is, let's say we launch something, some recommendation model and we want to try a direct users to it, but we want to do that. Like, after they have done experiencing the current product, you know, after they paid money after they
 Up so can be something that is experimental. Yeah. So they're like we once I remember used something a kind of a bot you know and that was the first experience of you know us. Trying a rag agents, essentially that specific use case. For instance, we were actually going through a lot of products. A lot of, you know, embeddings from the user data.
 And we could not have gone with something like you know lightweight. So at that time we did explore very specific Vector DPS like you know, word xai or, you know, milva. Stevie yeah, if
 A use case is like that then. Yes, absolutely, yeah, I would go with those ones, but if it's a use case, something like, you know, I want to generate
 See reports for the business. I don't have much data of the schema, you know, like it's barely in kilobytes, you know. Yeah. And in that case, I would use something very lightweight open source which is out there. All I need to make sure is in that lightweight Victory. DB, it's able to pick up at the right time the right context by doing, you know, the rights in Liberty matching, we get cosine, similarity, Pearson correlation, like, however, it finds Pearson correlation being one of the really interesting things, you know, when we think about user reviews and all, but now,
 That's so that's on the vector TV side. What about just architecting it. Architecting the whole system you had a blank slate and you come into a startup and it's like okay sweet. We want to build this product.
 How do you go about that?
 Very much, of course, reliant on the kind of problem that we are solving. But let's assume if you're solving a problem, which comes through the rack agents or agent AI. So, first of all, definitely it would be what kind of data we have.
 Is it a textual data? Is it an image data? Is it like what kind of problem you're trying to solve?
 And let's say if it's a textual data, for instance, do we have like, different context of the data, like, do we have different data governance?
 Rules in place. Like do we have different geographical locations in place? Like Europe has like, you know, huge data privacy requirements compared to, you know. Yeah.
 Here. This is a startup. We don't have shit. There's the I imagine that is a beautiful thing if you have the data governance rules and you know everything on
 Also just where the data goes and how you can do anything with the data, unless you follow the processes.
 sounds like an amazing place to be in, but I imagine that you get there or you
 need.
 A team of people to be focusing on that. That's not something that just magically happens. Right? Yeah, that's true. That's true. But essentially, when I'm trying to build up a prototype, I would want it to be scalable to a point where whenever because those things are gonna come through down the road and I would want to be like, prepared with Market Picture. Yeah. Between clothes that. Oh, interesting. Yeah. So, in that case, I would go about, you know, building a small agents for a very specific use cases.
 And then rerouting the incoming queries. Based upon what that specific context is
 so that
 The agents like which are catering to a very specific data sets this. Its scalability in maintained at the same time, making sure that the reading that is happening. It produces very optimal that Dynamic prompts which can be then used to query, any large language model unit or find unit two way that we want at any temperature that we wanted to and that routing.
 happens with an llm or that's just a router that's like a
 It's part of the rag agent itself. So there's an agent without basically, the last part, which is the generator. So, it's in the augmentation part where essentially, we have query coming in and then we are routing based on the what kind of context we want to route that query specifically in and then specifically quoting those specific agents in a sense to generate for their Dynamic, llm prompts that we would use are there
 Use cases that you.
 particularly like and have seen like a lot of
 usefulness with
 yes.
 Absolutely. So of course data analytics is one part. You know what used to happen before was like business would get back to ingenious and they would be like if you want these kind of reports. Of course they would have their dashboards and like power BS would be there, right? But
 Now, they can just straight away talk to databases essentially. And that's a huge win for any for the data team that doesn't have to service requests anymore. That's for sure. Yeah, I remember talking to donate about this cauchy build a
 Data analyst agent. And one thing that she said was the hardest part in building out the agent so that it gave correct answers. And it understood the context was they had to build out a whole glossary of terms.
 since a lot of what you say, when you are speaking to another person, you're using this like lingo, and even if it isn't
 Marketing lingo, it is still.
 Fuzzy in the way that.
 Our company or our team describes that. So an mql in marketing terms is like a marketing qualified lead and at this company we describe it, someone who has, you know, downloaded the e-book but in another company, it's not until you download the ebook and you come to a live event or you reach out to sales because you went to a webinar. And so there's even with the same term, the same word, it's very loaded, and that happens across the board. And so, like, how do you do? Did you do like a glossary thing like that? Yeah, I and that is a very interesting problem to be honest, you know, oh, even like within the company you don't have a different teams, have different Lingus, things, sometimes, but yeah, and that's one of the things how I like came over with that was essentially defining this key mods, which were being parsed to, let's say the
 raggy that we have been building so you can Define the relationships there or the mapping essentially which is the glossary, you know, because as such the large language model doesn't understand anything. It's just an understanding the language, right? We are telling it like, okay, what this is, how do we need to do stuff and it's just creating a very semantically. Correct, syntactically. Correct answers for us?
 um, but yeah, so a mapping layer is for very specific lingoes which can be mapped to a very
 thick can context specific terms that becomes absolutely necessary for that and actually
 The other piece I think that could get tricky and I would love to hear how you deal with it. Is that
 You get.
 natural language questions like
 How did we do this quarter?
 Which is like, what do you mean by that? Right? And so maybe the the agent can come back and say like are you talking about revenue and it's like yeah are you talking about revenue generated in the whole company on just your team on? There's so many variables that when you talk, it's
 Not clear. True. And so, how are you dealing with that? The agent isn't just
 giving you stuff because the hardest thing in the world is getting the agent to say, I don't understand, right? Like it'll just come back with like, oh, here's how we did this quarter and you're kind of scratching your head. Like I don't know if that's actually what I was looking for.
 So, true and thanks Mutual for that because that really invoking in thoughts, in me. Like I would certainly one is, I don't know if you've heard of Dr. D Chi, he actually has just recently published a book. It's called raising AI. Very interesting. It's about how, you know, we need to work along with the and he's a professor at Stanford here really interesting, but we'll definitely get into it. But from the technical side of it from abstraction to coming to a very specific use case, that has been the biggest challenge for us at the end of the day. And the reason why a large language model or an agent would never say that. I don't know because it works on our confirmation bias. At the end of the day, it just wants to answer, know what?
 Oh, so for specifically for like one of the ways that I was able to achieve it was contextualizing the prompts that we are developing dynamically. Not the ones, you know like which we write statically but the ones which are being developed dynamically in this agents. If and that's why that becomes the soul of the agent. Because if it's able to provide from that abstraction, okay? How did we do this quarter to? Okay, what do we need, like Revenue. We need sales. We need a product sold and all this information can be put out, so it will give us very context, specific results. And how do we do that? Depends on couple of things. One is, how is our data schema and structure? Yeah, and when we say this quarter, it's just going to pick up that date range. How did we do?
 We can always map these queries to revenue products sold. Let's say, sales employees performance, it could be anything but and that's where a very context-specific Agents come into picture. If I am, let's say business and I'm just focused on the sales of it versus. If I am an HR team I'm working on employees performance. I wouldn't want to have two different answers to that and hence even in the reusability of those agents we would have to like configure them or tweak them in a way. Also we can deploy the same ones and they can constantly ask and they'll get the result. Yeah. But we would want them to be very specific and precise to what they are. So this is like a little bit of a personalized agent. Yes. So it knows that I'm on the HR team and when I ask you how we did this quarter, it's like Employee Engagement. Yeah.
 Essentially. Yeah, if I'm on the sales team, it's like how much did we sell? But maybe if I'm on if I'm in the c-suite, I'm looking more at like holistic view of how all the numbers are. Yeah and
 We are agents, right?
 What contacts do I have?
 I'm just gonna go and look up. Okay, what documents or what index I have in my DB, this is the query that came in. There is an embedding of this query. Like let's say, point one, two point three, four, I'm gonna go and see the vector space. Where are like, what are the closest points to this?
 Epic that up.
 I bring it back.
 Now, those specific points in this semantic search. They could mean very different things for different agents.
 But those points which are suspended in there, that depends on, of course, the embedding models that we are using. And because we cannot perform, you know, semantics search when coming from different embedding spaces.
 specifically, because what happens is, it's a way to represent our
 Textual pictorial or any nonlinear data in a numerical form essentially. Yeah, and when we are picking it up from the schema docs, let's say, or from any documents that we are feeding into the rag agent.
 Those schema talks Define, essentially what context or which let's say tables to pick that up. From if I'm picking up from, let's say a sales table, it's going to give me more context around sales. If I'm picking up from let's say employees performance table.
 It's gonna give me more about that, so inherently.
 It as an agent. Wait, sorry. I missed. I missed that. It was from Vector space that it gives you that information. Yes, because those vectors are essentially mapping into numerical terms of what these. Yeah. I see documents or schema so you're enriching the schema with the vector with basically Vector space. Yeah yeah. Okay. So it's like going into this, you know, Multiverse and this finding out. Okay, I'm just need to pick something, but I don't know what it's going to map to because the mapping has been done basically based on what it was fed in before and that what was fed in is very much dependent upon what kind of relationships. We have defined in before. Yeah, how the index has been created.
 and the index creation, essentially happens as a first phase of
 These agentic. Okay, that's super cool. I haven't heard the adding a little bit of extra contacts so that it understands and it's almost like you're saying grounding the information again. It's going back to like rag was all about grounding the models and now we're grounding the agents with little bit of extra space. Some antique yes vectors and all that stuff. Tell me more about
 These dynamically created prompts.
 How does that work?
 Okay, let's take an example and we'll go back to our example. How did we do this here? Yeah. Or in this quarter right?
 and let's say, if I have three tables, let's say we have like
 Yeah, three tables. We have a sales table. We have orders table.
 We have, let's say products table.
 Now, in the schema docs, which are passed to the rag agent from where the index is being created.
 How we Define that, okay? These are the sales we tied to the products and as a key and there's a key for sales products, and third one. Let's say customers for instance, right? So aim is like, how did when we say, like, how did we do this year? It would be based upon we can tell like what customers bought. Like let's say this quarter or what products were sold and and how much revenue was generated. Yeah. Right. So that part, where we are defining this essential, schema of the tables in the docs and the relationships inside the dogs. When the embeddings are being generated, those embeddings are also in capsule, those semantics of the relationships between these three different tables. And when I asked this and if I, this is the only information I have, I have not told to do anything else. When I asked like this quarter, it picks up the time range finds out in that time range about like what all data
 It can fetch from these tables and then we find tune, like how the generation of these Dynamic prompts are gonna be in retriever methods and during our index, service well. And then from there, we can actually be very specific about like what we want in the reports outside. Interesting. So,
 you're getting it, I'm trying to think about like the
 Step by step nature of this. You're getting the query. How do we do this quarter? There is a model that
 receives that
 It also will go and search Vector space.
 And then it outputs a prompt to go for another agent to go and use.
 Yes.
 And in the output, it's where you're very specific because you have the information of all right? This is
 Whatever. This is the relationship between these three tables. Here's what I want you to look for specific agent. Here's what you need to focus on and then go and find that and
 Come back and and then after it finds that it comes back and it tells that Master agent, here's what I got. Yeah. Yeah. And then the master agent will have all of this and it's
 Input. And then,
 To summarize it and then output something. Yeah. And it could even use that to let's say, Retreat more do more. So like we have this output now and we are of course going to have these evals in between, right? Okay. Now let's say we want to relate it to some other set of databases. And then the other agent picks it up and goes and creates a dynamic prompt based on that. So it's multi-agent system, just communicating with each other. At the same time, making sure, they're very contextually relevant to their own specific set of questions. But it's all happening because we are able to create very relative dense embeddings in these Vector spaces and let's say a user. Query comes in that is all. So actually put in the vector space or projected in the vector space that being the right word. And then from there, the picking are of like what is the relevant context that can match this query?
 So we don't even have to manually Define a lot of things for rack agents until unless absolutely necessary that it's not picking up which is a very I would say.
 To win frow process sometimes and that's why prompts are not very successful in getting large language models grounded. There is rag agent. The introduces additional step of suspending in this generated index are relevant information.
 Query comes in its projected. Now, we find the match we come back, we generate the that Dynamic prompt now with that context and that gives us the exact results that we want. Yeah, because you a much more rich.
 Field to play from it's so much more enriched with all of that information as opposed to just my simple words of like, how did we do this quarter? Yeah, yeah, it's essentially.
 Yeah. Yeah, I guess I'm lost on.
 How?
 Putting the query into Vector space, right. And then seeing what it is semantically similar to
 How does it already have all of this stuff that it's semantically similar to?
 Make sense. So all right. Yeah, I was hoping it did but I didn't know because I was confused myself for a second there to me on this. No, no, that's very valid actually point because what happens is. So before even we started asking our agent any questions what we did was we built an index
 Those index was built on some documents.
 And documents is a very abstract term for any set of data that we would use.
 Now imagine a three-dimensional space where we just have like X1 X y1 or Z1 and we have essentially put just the data of our two tables, or even for that matter. This conversation, we let's say, we want to go back and review this conversation. Somebody wants to ask questions about this. So we have created a kind of, you know, analog for this. Yeah. And what we have done is we have actually created embeddings from this from different. Let's each question and each answer represents one document.
 Creating embeddings for these different documents. We are suspending these in the vector space.
 Now, let's say if somebody comes up and asks, okay,
 when Dimitri asked this, what was the answer or did these two questions make sense? This query is essentially gonna go in in the same Vector space and embeddings are going to be generated using the same model, using, which the index was created from our conversation before. Then it's gonna go ahead and perform that semantic similarity source and it can you can tweak it to you, use any similarities, it could be euclidean cosine Pearson.
 Any similarity.
 And then it finds the closest matching similarities and we can also Define how many neighbors we want to for it to find the similarity to and that's like a hyperparameter that we can tune. And let's say if we include too much of a context that it can actually waiver off and too little of a context, it can also wear off. Yeah, but generally, for example, for me, when I did in the rage against five was a five, nearest neighbors also really good approximately your number so much. It was a small use case. Yeah. Essentially, and yeah. So in that case, when the new user query comes in, it's suspended in the same space and then it performs at semantic similarities and gives us back the dynamic prompt which is part of the large language in. You're always updating it with the new queries, you're always adding the new queries to the vector space. Yeah, so that's so what happens let's say if I did not get the results that I wanted. Yeah.
 I'm gonna go back to my docs, I'm gonna check. Like, did I Define the schema correctly? What went wrong? Why was I able to like, you know, find a very relevant. Why I wasn't able to find a relevant context? And I'm definitely going to update those talks, I'm going to Define another talk, which defines this scheme or relationships between these quotients.
 So I'm essentially defining these things prior to make sure the that are large. Language model, whatever answer it gives it's grounded. It comes from what exactly we wanted to do.
 How are you dealing with the?
 Problem of just having.
 Too much data and messy data. Fortunately, the use cases that we have worked with so far.
 The data was wasn't too messy, but let's say we encounter
 So for example, we were taking talking about the one of our conversations. If it's a, let's say a rag agent, which is talking to databases. There is going to be very straight up. It's gonna be scheme off the database. This can be like type of the columns that we have and what relationships between those tables are there and very straight up. No nothing. You know which can can be in for essentially but in a normal context of things we will definitely have to make sure that we have proper data. Preprocessing in place where we're cleaning up the data, we are removing any unnecessary tags which because we wouldn't want our LNM to focus on very unnecessary information like any, yeah, random, you know, overweight like one word and you're like, why why did you care about that word? It's not that big deal. Yeah, and this essential comes down to the our previous initial discussion about how Transformer architecture essentially works. It's so essentially a signing these probabilities to all
 These Sports and doing the MLM, which is masked language modeling, predicting the next word. And if you would want certain words to be updated more, you would have to make sure that you know, for a very specific use case the eliminate the parts which are not at all like desired or required. Yeah. Sorry. Well let's say the some data becomes stale because for some reason or another
 You don't.
 Have the same policy anymore or you don't have this. You realize that oh this data actually was incorrect and so we need to change it out.
 How are you going about swapping things? Because I've heard that is a
 real pain in the butt when it comes to keeping your vector database up to date. Yeah.
 That is actually.
 A big challenge for sure.
 One of the ways would be to and it's definitely going to be a costly process to re-index our database. Oh interesting. Read the index again.
 And as the data increase is indexing. It becomes harder. Yeah, and harder.
 Yeah, if you only want to change one. Yeah it's like we got a reindexed. What?
 Yeah.
 I wonder though like, oh,
 In case, how would we essentially go about it?
 One of the ways could be inducing a negative example. For something that we don't want to include in Iraq, agents, for example. Let's say if certain part of the data became stereo, we can always include a relationship in the documents which is like if asked for this specific data,
 Don't think this is a stale data. We would not want it to go there. So even though in this semantic search, it might be picked up the good part would be when it goes to the large language model. It is going to reject it immediately because in our prompt we are specifying not to actually answer in that but that's more of a sanity check or smoke test, you know. And it's also okay to do if there's one or two pieces of data but if its 20 no no you can't you can't do that. So easy. Yeah it would not be a scalable but that's a that's a really good problem. I'm just thinking because I heard the example of
 You have your HR handbook.
 And certain policies, get updated.
 And then what do you do with when someone's going and asking about their HR questions?
 It's getting information from which handbook which policies are getting referenced here. So this was back in the rag days and I just remember people talking about how hard it was to keep their Vector databases.
 Tighty. Yeah, and up to date.
 and so,
 I find it interesting for you. It's it's almost like
 Maybe you're not having to replace as much data. So then you don't have that problem, you're only creating new data. And then you can just use the time or the date created filter type thing. Yeah, yeah. For most of our use cases that we have like, worked with
 the don't have like a lot of data which needs to be either removed or replaced, but if it were the case, I would believe that if we indexing would definitely solve the problem, but with the scale of the data,
 I'm sure like we can always re-index like certain parts of the victory. DPS will ya by for example, just using like one document which is the part we need to like, you know, update. So those query would be again suspended or projected in that Victory space. But with different values essentially,
 And any negative examples could help us to not entertain our previous policies.
 Um, but that is definitely your interesting problem, and I think would have to look more with detail to that. You don't do anything with LMS and recommenders. Do you? Are you recommender systems? All very traditional still know. We do use really? Yeah. Yeah. How are you using LM for recommenders? So for instance, let's say if you're we have some recipe recommendations and I want to like you know generate a recommendations like based on that page. So and I would want to like extract out, you know, like let's say, cooking tools relating to certain recipes, Give an example and that way we do.
 Takes utilize and using a large language model.
 but,
 You're just contextualizing it and then you're going to the traditional recommender systems style or it's a hybrid. Yeah. Hybrid basic, I've heard about this a few times from people. It's like you slap a llm on top of a recommender system just to make that part so much easier because you don't have to train this model on the whole cooking utensils and everything. Yeah, I mean, fortunately for us, you know, our users don't scroll Walmart, you know, page as often as Instagram. Yeah, they're definitely helps us to have a little bit of latency or like, you know, specifically for the products that I have worked with, you know. Yeah. Oh, have not as much of an online model, but in that case. Yeah, the glittery is like that most important right now.
 But Nick, for the use cases that I have worked with and not to like delve into too much of details for the company's confidentiality. But yeah. So we have like you like large language models in essence, trying out like, you know, like what features can we extract out from the current recommendations so that we can use those features to recommend next set of things.
 Immediately and at the same time, combining the user feedback to make sure how they are essential liking and what else they will. Like, in that sense. So because when we are essentially taking a natural text and trying to get the feedback out of an item or essentially a description of an item which user is following set of instructions. It there are so many products that could be relevant and if one product has captured any users attention, right there. We have user engagement. The top of the funnel is already established. Now, use we have user awareness and we can essentially use the top of the funnel again, like a little bit of marketing term that we can essentially use that to Leverage. What else can we bring into awareness?
 As the user of awareness, increase is for the products, which are related.
 Essentially engagement increases engagement increase leads to potential sales. Yeah, and they definitely come back. And, for example, we were working with how are YouTube Banner Impressions, which have led to the sales of items and there is no way for us to relate. Yeah, and yeah, sorry no, no, okay. So so really your
 Generating the feature or your extracting, the features with the llm. Yeah, yeah.
 And the features, but the features are already in some kind of feature store.
 And this is very dynamic because what we are focusing on is what usually looking at. Yeah. And then we find out let's say, for example, there's a set of recipe that users essentially looking at
 What?
 Products can be mapped to this recipe, that could be gotten from the description of the recipe.
 And that is very user specific. Now, we know for a fact it's gonna be a little bit slow for an online survey. I mean, the latency is going to be high.
 But we do have that uses data. We do know like what we can essentially.
 Further diversify their recommendations when the user logs in next time or even a Time.
 I see not matter.
 Oh, so you're using the LM to enrich the recommendations?
 I understand. So the llm took me a while, but I got here.
 it's
 a random experiment somehow you know when it turned out to be like pretty fast and okay, well like this
 Worked, you know? Yeah. Yeah.
 I really like that. So all of the
 Pages, that I'm looking at on Walmart.
 You can extract especially if it's any type of a blog type of post like a recipe.
 You can extract.
 Everything from that page using an llm and then enrich my customer profile with all that data. Yes for certain products. Yeah. And then later you do the traditional recommender system but it's but you know, like, oh yeah, he did this. He like, you know, he liked this and we have some strong signals that he might need some Chopsticks because he was looking at a sushi Rush, sushi recipe. Yeah. And you know, the interesting part is
 And anymore. If
 You feel right now and you will see you will have. You know the let's say cooking tools that you need to use right there. Yeah. It's it's because it's a very lightweight model and we cache here a lot of things which helps us to immediately, you know, get back and bring it into production in, you know.
 so it's it's definitely, you know, fascinating and
 To our advantage, recommendation systems, they work on confirmation of personal bias, you know, it's gonna tell you more things that you want to see at the end of the day. And that's where, you know, like sometimes I feel whenever you're even scrolling Instagram, you know? Like I like like once kind of page and it just pushes me in that direction like oh my gosh. Okay wow, dude. I know. Yeah, or I have the hardest time because on my iPad, I had I can't log into
 YouTube on my other profile that I normally watch you do on. I have it on my random like profile and the recommendations I get are not that good at all. It's from like three or four years ago when I used to watch things and that I that I liked and so I
 A little bit of a tangent, but it is fascinating to think about that. Okay, so I like it, man. I really like it. I'm just wondering what else is there that might be worth it on?
 Evals. Yeah, how do we evaluate? I like that. All right, yeah. Let's let's talk about that. All right. If you know, very Niche, space off that I would, yeah, we still figuring out one of the most intriguing parts of this space is that
 There are so many components to a rag agent. Where did we, where do we even start evaluating?
 I mean I'm getting an answer, but there are so many steps behind that answer, you know?
 The retrieval systems. There's indexing system.
 where do I, you know, start evaluating like, you know,
 What part is going right and what part is going wrong? Yeah. How do you debug? Yeah, and this you know, boils back to the initial example that we probably yeah, we touch base upon was. It's like, you know, a kid who's having an exam and open book exam and now what we have given a kid, the question that okay, this is the question it goes back to its books finds out the most relevant context, starts giving us the results or answers now,
 One place to look for is is that kid or is a rag agent, referring back to the right set of documents or not.
 That's where the first evaluation starts. If it's not, we need to like, check into our indexing. We need to make sure like are we like, defining the documents correctly or not? Are we passing the right contacts?
 If that's done, that's step one. The next thing would be
 Once the context is retrieved, which is let's say, right? So we can use recall in that, for example, you know, okay, take sensitivity, which is a very
 I would say cliche, you know, evaluation strategies in machine learning. Yeah, and then the next would be okay, once we have that are the prompts the dynamic prompts being generated, are they somehow relevant or not? And the third part would be. Once the prompts are being passed to the large language model, which everyone we are using
 Are they actually generating the right set of output or not?
 So these three steps essentially we use for evaluations in Rag. And what are you doing? Just printing with how and having people label this? How do you go about this? And I imagine it's all offline, it's not in. Yeah, in the moment. Okay. Yeah, yeah it's it's all offline. Yeah. And it's not like of course not. I mean this prototype's here is we started to look into each and everything like debugging, like manually. Yeah. But as I was like, mention like recall because the one parameter that we use at the time of, you know, document
 A picking for Iraq and good. Recall means like, okay, we are actually picking up the contextual relevant documents over there.
 And then secondly, for example like how the large language model is performing that is essentially evaluation done based upon the hyper parameters of that large language model, how much of a temperature we are like setting what is the value of K that we are using? Is it producing contextually relevant results, or not? And then we also have llm feedback so these results are fed into a large language model to get back, if they are relevant or not. So it's a feedback module where the content generated by one rag, agent is being fed into another large language model, which gives us the evaluation of that.
 It will work in progress right now.
 Again, it's we don't have like a state of the art methodologies. We still like getting there,
 But these three phases are ones like.