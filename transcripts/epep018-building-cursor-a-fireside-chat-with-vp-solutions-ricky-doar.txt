How's it going? I'm good, man. How you doing?
 I'm doing well, I'm doing well, I'm excited to be here. First of all, thanks for coming on here. Thanks for doing this. I'm super excited to chat with you about cursor. It's pretty much every single developer that I know is either a die-hard user of cursor or they are playing around with it and getting a ton of value from it. You've been leading the whole Solutions team you talk with forward deployed Engineers every day that's like literally your job and so I want to go over all of the cool stuff that you've been seeing out there in the wild.
 Yeah, I would love that. That's awesome. It's been a, it's been a wild year. Definitely cursor has been have quite the year, quite the kind of year of growth and we've been talking to 100 if not thousands of Enterprise companies in this point, that's huge man. And congrats on the huge announcement last week. I think everybody saw the number in the race and had to do a double take. So that is very cool. Very exciting. I know you've done a bunch of stuff you were at versaille. You were doing. I mean, you've been
 Doing this kind of thing for ages. I know you're excited about what's going on in cursor and maybe we can just go into this real fast with, like, what patterns are you seeing the most successful folks doing when you're forward deployed Engineers, who are on the front lines are coming back and saying like dude you'll never believe how these guys are using cursor.
 Yeah, yeah. I mean it's definitely you know,
 Engineers everywhere are kind of learning how to build with AI. They have been for the past few years but it's really kind of accelerating this past year. I think, first and foremost it is good to recognize that working with AI is is a new skill. It is still, that is adjacent to engineering. It's not exactly engineering and just just like, you know, great Engineers, learn the right ways to think first and that's kind of more important than, you know, memorizing syntax. It's very, very similar in the AI space is, there's kind of a new way to kind of think alongside these LMS, whether you're kind of working with AI in the foreground and kind of working as a pair programmer, with AI or spinning up these agents in the background and kind of spinning up these tasks kind of the best people that we work with on a day-to-day basis, are the ones that are really, really good at breaking down, large problems into bite-sized tasks that the AI is far more, likely to be able to
 To accomplish rather than, you know, either going way too small or way too large and kind of running into these areas where AI hallucinates. So I think that's probably the first and foremost skill, we see that separates the, you know, power users from the people that, you know, in the early days, struggle is, can you understand the right size of problem, to hand off to Ai and it's really all about, how do you manage context and how do you build your prompts? Yeah, the context is so huge. I actually if you stick around you'll see I made a video about context while I am here in Amsterdam enjoying myself. But is there like a rubric that you have looked through on how to separate, what is too big and what is too small?
 Or is it just like trial and error throwing things at the wall?
 It's a little of both, right, you know, as these models get smarter, you know, this has been obviously again a huge year for AI. We've got seen tons of model releases throughout the year. You know, if we take like anthropic as an example you know Claude 35 turned into 3/7 turned into four turn into four or five. There's also sonnet there's Opus as these models get better and they get smarter, it kind of changes what they're able to accomplish. And you know, there's there's also no substitute for just kind of getting to know these models. These models have different levels of intelligence. They also tend to have, I mean, you talk to engineering that work with these models, on a day-to-day basis, they have personality and what they're at and what they're bad at. I wish there was some easy answer of just a framework of how big a problem should be so that the AI can accomplish it but a lot of it is getting to know how good the AI is at the given task that you're working on and really kind of learning what size tasks can, you know, a model like GP t51 codecs that just released recently solve that
 maybe gpt-4 or even gp5 couldn't solve, you know, two months ago. So, a lot of this is just getting familiarity with the tools you're building with. And then you kind of build this internal barometer of, you know, what can this model accomplish and what can't did, where does it often go wrong or or hallucinate?
 It. All right, Randy, I gotta take off my glasses because I need to see you for this one. I need you to see me for this one. What are some anti-patterns? What are some ways that we shouldn't be using cursor?
 Yeah, it's it's a really great question. I mean, I think one of the like things that we've seen that people struggle with is either, they get extremely reliant on the AI. And they forget that a huge part of building AI is not replacing your brain to strategize, and to understand your code base. And to, you know, build the right, make the right architectural decisions. As you're building out with AI, it's certainly extremely powerful. It's really helpful. You can ask a bunch of questions of your code base, you can use it to ramp up on code bases, very quickly, but I think a lot of there's potentially can skip that. That first stage of, do you know, your code base? Extremely well, right? There's multiple ways. You can use cursor, one of them is just ask mode. It means it's not going to write any code. It's just gonna answer questions about how your code base works. And, you know, one of the downsides of AI is it can make you lazy. And as you get a little bit lazier
 You start giving vager and vager prompts you start, you know, pushing off a lot of the Strategic decisions to AI as well. You know, there's a world where we might get to a place where models are good enough, where that's okay. But for now, it's it actually, just lulls you into a false sense of security. You have to stay in the driver's seat. You have to make the right decisions, architecturally, you have to drive the agent in the right direction. I think that's the biggest anti pattern. We see, you know, across the board. That's so true. You get into this false sense of comfort and you keep off-loading offloading offloading. And then it'll, it'll bite you right when you're not expecting it.
 Exactly. Well since you said, how are we doing? Like you gave us a that anti pattern and you mentioned searching or getting up to date on a code base. Really quickly, I'm wondering about Greenfield versus Brownfield and basically what you're seeing when you're going from 0 to 1 versus what you're seeing when you're dealing with a code base, that's 15 years old, maybe you're still, you got some stuff that's deprecated in that code base to the outside world. But internally, it's not like, how do you deal with cursor in those instances?
 Yeah, I mean you know obviously AI is super exciting in the Greenfield space, it's fun to make a prompt and just see, you know, something it created before your eyes. But I think this is also kind of one of the biggest advancements over the last year with cursor, with other tools. But cursor, is phenomenally good at this. Is how do you take the context of your code base and use that to your advantage? This is also, you know, there's there's discussions debates in the AI space of what is the best way to use a code base correctly, some tools, they only grip through the through the the code base. As you're writing prompts cursor, has found phenomenal success indexing. Your entire code base before you start asking any questions. So it indexes your code base, it creates essentially a semantic search. So if I say like, hey, I want to add authentication to this page, it can actually look throughout, you know, a massive code base, 500,000 files, you know, millions and millions of lines and determine what area of the code base are most
 Into that prompt and pull that in. So one, the tools have got a lot better at Brownfield because they can pull a lot of the surrounding context and actually have patterns to match off of rather than just purely building whatever they decide to build. And I think this is an area where both the engineer has to be familiar with the space that they're building in but also the tools can pull context from different, you know, codes different files throughout the code base to have patterns to build on top of. So I think it's actually a little bit of a of a myth right now, that AI is far better at Greenfield than it is at Brownfield. That was true, maybe last year when there wasn't a good ways to semantically search to quickly grip through your code base. But now with cursor, we have semantic search. We also have lightning fast parallel grip. As you ask these questions, the most important part is, if you can give AI examples to follow, right? You know, I have
 You know, I'm trying to build out a new API or new infrastructure or a new, you know, whatever it is. I try to fix this bug or refactor this area map, it to other areas that, you know, are built well. And do you do that? Just like kind of give pointers, maybe check this file, or I've seen this done well, with this API or I want to create an API like this.
 Yeah, definitely. So I mean specifically in cursory you can act different files and say like, hey, you know, I really love how this API framework is set up. I want to, you know, add a new new endpoint to this API, have it mapped the, the architectural choices that have been made before, right? You can add a lot of these different things. This is also, you know, very important when you're building front end kind of like, you know, changing front end pieces is, you can tell the AI. Hey, we have a very specific design system. Please make sure you're using the components, the colors. The design framework that we've used to build the rest of this application. A lot of building correctly with AI is making sure that you're minimizing the amount of arbitrary decisions that the llm can make it, you know, if the arbitrary decision is, Right 80% of the time, but it has to make 100 arbitrary decisions. You're going to build something that is not quite correct if you can.
 Minimize those arbitrary decisions. And with Brownfield that actually is easier to minimize those arbitrary decisions because you have a code base to map onto. Yeah. Of course, you can say, oh yeah, like this is an obvious example. And I like how you said that with the examples,
 How have you seen the best folks? Maintain their context windows?
 Yeah, so I think there's there's a lot of, I mean, there's a lot of best practices. There's also a lot of like false myths about context, I think, you know, as we go throughout this year, people are learning more and more, but I think there's a regionally this myth that like the more context, you give it the better. It's gonna be and it's just add as much context as you possibly can. You're gonna get a better and better answer. There's a lot of research and a lot of studies that show that even these you know these models that have a million you know token context Windows as they get to about 50% full and they get into 60 70. 80% full you're sending you know 800,000 tokens to let's say gemini or sonic 4/5. There's actually a steep drop off in how how you know how often these models hallucinate. So one is when you're scoping these tasks, make sure the only thing that is going into that context window is relevant to the task, one of the most common piece of advice, we give with cursor.
 Is when you start a new task, start a new chat window. There are a lot of Engineers that might think. Hey, I worked on something adjacent to this problem for a while. In a chat window and a chat window is essentially just a boundary for context. I want to start a new task and maybe this context is going to be helpful. And you you end up staying in the same chat. You have this bloated context window and it's really just a recipe for the AI to again get confused, make an arbitrary decision based off of something you were doing before there's no longer relevant. So the more pristine you can keep your contacts actually the less the better it is as much context as needed without any Superfluous context involved and that that is also a skill, you have to learn but you know just throwing the kitchen sink at a problem. Gets you worse and worse results.
 Just enough context as needed. I like that. The other thing that I like about what you're saying is
 Let's not fool ourselves into thinking because there was a similar problem that we had, we can go down that path and continue with that context window, or that chat, I wonder all. So have you thought about or have you seen folks that will try to turn off pieces of the context when needed? Or is it just like that is too much time?
 Just start a new one.
 so I think starting, I mean, there's also ways you can summarize context, there's also ways that you can, you know, create you,
 Kind of Infuse context into a new chat window by making markdown files. What one really common way? Which actually is now built into the flow of cursor is called plan mode. You're kind of building out a plan for maybe a bigger project or a bigger task and it will essentially build a markdown file of. This is all the things that we need to do and then you can use that artifact in a new chat as the starting point of this. These are all the tasks. All the decisions we made plan mode, will also ask you the engineer. Hey, do you want to do X or Y? Do you want? You know, it'll send you these multiple choice questions of different decision trees that you could go down. Yeah. And then you can use that as you're starting point. I think one really common like easy to conceptualize way that context can cannot be your friend if it's also very common. Let's say, you build out, you know, either you're refactoring, something, or building new feature, whatever it is, and you run into a book and AI debugging is one of the most frustrating.
 In parts of learning how to build with AI because it can just Rabbit Hole down the same problem. Yeah, I think that we see is if you try to solve the bug in the same context window that you created the feature,
 It has all this context built up, that it was that, whatever it did was the right thing to do.
 So it can actually make it harder for it, to figure out what the bug is because it is way down by all these other decisions, that created the bug in the first place,
 Dude like your preaching to the choir right there. I feel like I have had that everybody in here who has played around and Vibe coded for a minute, has felt that. Now, there is something it's a little bit.
 More of a nuanced piece. But I was just talking to a friend and they were like, yeah, last time I was using cursor, there was a moment where I wanted to do something and I knew how I would implement it and I have my experience, but
 What cursor suggested was different. And so I'm at this Crossroads, do I go towards what cursor suggested or do I go my way of doing it and why and what is going to be better? How
 Would you recommend that we solve that kind of thing? Because it's like I know that my way might work but I also know that I don't know things like the no unknowns and then the unknown unknowns, right? And so now I'm looking at what cursive suggesting and I'm thinking is that going to cause problems in the future? Because I totally don't understand what is happening here or is this the better way of doing it?
 Yeah. And I think this is kind of the biggest difference between, like, Vibe coating on the weekend and building something fun and then building sustainable software in the Enterprise, I would definitely say it's not really an either or but I would not build anything for an Enterprise solution that has to go out to maybe millions of billions of users that I don't understand. Right. A huge part of this has to be like, I need to understand the solution that I'm putting out and even if the AI wrote it, like the engineers responsible for the pr that they ship to production, you know, we also see it, a lot of a lot of companies, you know, a bug will surface in production and someone's like, oh, what happened here? And you know, it's a very obvious feeling but I don't know the AI wrote it and Cloud today.
 I mean yeah, we have the half the internet down right now. I swear man, these AI just did that but sorry I didn't mean to cut you off, you know. Don't worry. You know, the kind of best culture. I've seen built around AI is that you can't blame the AI. If a bug comes out, you have to be responsible, understanding what the code was and what was written. But that doesn't mean that you always have to choose your solution. It just means if there's a solution path that looks promising, but you don't fully understand why it chose that. I mean, one of the beauties of AI is you can say, hey, I would have done it this way. What are the pros and cons of my way versus your way, right? You use AI to make yourself more informed about the choices that are happening, you know, it is really enticing to, you know, get into that flow of moving so fast and just building constantly and shipping code and riding lines of code, but you kind of you have to especially in the Enterprise world.
 slow yourself down at Key decision points, inform yourself, learn dive in, ask questions, understand and then make the best decision path forward because it's if you Branch off into completely different areas,
 If you've chose the one that you understand less, it's just gonna make the next 20 steps harder. Totally. That's so true. So maybe you have a special way of dealing with this next problem that I've encountered where I have my rules.
 But some rules, you know, it's like playing the guitar and you have your scales, but scales are like a recommendation sometimes, or scales can really make the sound. Not sound good. So I have like, hard rules that. I want the agent to follow, and I have like recommendations. And how do I distinguish between those and make it so that this rule, like, if for example, it's don't output a function longer than 21 lines of code and that's like a recommendation if it really needs to be, it can be 23, but like keep it at 21 ish.
 I can't say that in the rules like, 21 ish.
 Uh yeah, it's a great question. I mean, I think this is one of the probably biggest learning curves of working with AI is just fundamentally, you're working with a non-deterministic system. And even if you write a rule, I mean, this is, you know, a huge frustration for a lot of Engineers working across any AI tool. You could write a rule that is like, this is the most important rule never, you know, start a new server. A new Dev server, right? I think that's the most common. When I see for front end developers is like never mpm, run down because I already have the host running and I know where this do it, you know, one out of 10 times. I mean, this is actually a thing that a lot of AI researchers are currently working on is like, how do you build a model that is thoughtful and creative but also closely follows instructions and is easily steerable and gullible. The answer is it's not a solved problem. Yet, there's no way to know deterministically. A lot of tools cursor has built this. There's this idea of
 Books in cursor where you can actually put deterministic hooks into the agent flow. So, that at given moments either right before your prompt sends, right, after your prompt gets back before the codes. Generated after the codes generated, you can actually do deterministic things. This is not as creative as AI, but if you have to, you know, you know let's say always check after code is written for certain things. You know, some some companies they'll check for copy left data in the code that is outputted. That is something you want to do in a terministic hook rather than a rule. But if you have just a strong rule, versus a soft rule,
 Really the best way to do it today is you it's like you're dealing with a like a junior engineer, right? You can say, in plain English like this is a rule to always follow like most highest priority and it will follow it generally more than say like, hey I want it to stay generally under 21 lines of code. If it goes over, it's not a big deal. But like please, whenever you can try to prioritize under 21 lines of code output, it that actually will work it will it will like Loosely follow one and try to strongly follow the other but again, models have different personalities. Some are hard to steer, some are easier to steer your mileage may vary.
 So I often think about it as like, wow, wouldn't it be nice if we just had, you know, a volume knob that we could crank up or crank down? Because exactly what you're saying, is so fuzzy in the language piece, where it's like, yeah, this one hard rule, this never break, this Rule, and then on the other side, as kind of a rule, and you have to put that in your language and it can be interpreted in different ways every time, like, every single turn, right? So 100. It's like, wow, sometimes I
 Ponder in the shower couldn't we have just some knobs that will give me more steer ability or more control in that regard. But also I understand that that kind of like defeats the purpose of. This is so cool. We get to just talk to it and then it does it. And if you now are breaking out knobs for this, where does it end? So, yeah, I don't, you know, this is like the original idea of like temperature. Yeah. When you're, you're like talking to an API is like you can set that really high temperature and it's going to come up with novel and crazy ideas and it's gonna be like this really creative partner to work with or you can set a really, really low temperature and it's like fairly deterministic to the point answers. But there is that Goldilocks zone of like what what's the point of working with an AI? If it's like, very, it's only gonna tell me one solution every single time even if it's a bad solution versus try creative, try to think of new things. Yeah. So I think this might be the last question for you and it's one that I've been grappling with for a while, how much harness is
 Too much harness. And how do you think about that when you're letting the agents? Do what they want?
 Yeah, when you say harness like when they harness at cursor we talk about our agent harness and like what? We do that you don't have to do in your prompt. That's how we harness a cursor but I just want to make sure we're on the same page. We can go down that road. I was also thinking about as I'm building my agent, how do I make sure that I am
 Giving it enough freedom on that temperature. Gauge put also not saying this is where you go, this is how you do it and the opposite side of that, making sure that if something is very well known, it does it every time.
 Yeah, no, it's a great question. So I mean I think we have a lot of experience building our own harness which can be very helpful for customers building their own agents you know in the Enterprise space there is you know, kind of as you're working it's always been true. That like data is very important you should measure the outcome of these things. As in production, you know what what leads to better outcomes versus worse. It's almost more true in the AI space. Working with these non-deterministic llms is when we build our own harness, right? Like, you know, you hear these like bench scores with, with LMS of, how good are they at solving certain problems? Bench scores. Inherently are imperfect because you can game bench scores, you can, you know, build things specifically for those things. But one thing that we noticed is like, you know depending on what that agent is supposed to do is it just supposed to Output you know? Is it a chatbot is just supposed to like talk with you in which case you can be a little bit loose with the harness you can like give it
 Some some guidelines obviously you know most of the LMS you're going to use under the hood. Have a lot of the safety guidelines already built in. So you don't have to give it too much Direction in the harness but like for something like cursor, where it can do almost anything within a code base, we have to build a very extensive harness of like, how does it work with tools. You know, there's an edit tool, there's a Search tool, there's a web tool, there's a like planning tool, and there you need like a very prescriptive harness of how do you work with these things? Like if your agent is interacting with a lot of mcp's, write those become mCP tools that you have to work with and in that case you do really want to kind of fiddle with that harness and test and see what gives you the best outcomes.
 And you know, like for us, our GPT 5 harness, like two months ago we actually figured out, we had a bug in it. It wasn't actually the best possible harness for GT5. Oh, and when we the bench score went from like 45% to 65%, so it can make a huge difference to kind of optimize this harness. Its special in this world where AI can output, you know,
 It can hallucinate and it'll give you great. Yeah, AB test that harness. I like it.
 Dude, Randy, thank you so much for coming on here. I know that this was something that you probably did, not realize you were getting yourself into when you signed up for it, you've been really a great Sport and I want to just like thank you from the bottom of my heart for jumping on here and sharing this with everybody on the live stream.
 Yeah, this was great. I I've been on a lot of Fireside Chats. I've never seen someone dressed this well for a fireside chat. This was awesome. Thank you for having me next time. Feel free to come and you're best Halloween costume.
 I I didn't realize I should have dressed up. Yeah you look great man.