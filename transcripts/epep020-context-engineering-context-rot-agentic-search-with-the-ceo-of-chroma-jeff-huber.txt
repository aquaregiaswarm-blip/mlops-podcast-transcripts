And most people like don't want to believe that intelligence AI intelligence. Artificial intelligence is Spike the most Insidious distracting. Information is the information that looks relevant, but is not for some subtle reason. This is why some ages, I think are like a powerful idea is like, literally, it's because context rocked, I think implies the existence of like context window encapsulation. And that is, by the way, maximizing recall, maximizing Precision, maybe like the largest, you know, install base, in chroma of chroma, in 10 years is going to be, you know, inside of like 5 billion robots.
 What's context wrote?
 Are we talking now? Yeah. Okay. I never gonna be like, hey, everybody like, you know, I don't get that quiet on it. You guys put out that blog and it caught fire. Yeah, yeah, I think we're really passionate about helping developers, build useful stuff with AI and there's a lot of misinformation flying around. I think that AI is really hypey, that's because it has a lot of promise, but also that has a meeting. There's a lot of also, like snake oil, and a dealer like understanding like what's actually working for people and what's not actually working for people at times. There's asymmetry in that information. So we were hearing from a lot of developers that like, you know, these sort of million plus token context Windows while you know, kind of advertised as being perfect. Because look, they they're very, they're very good and they don't Haystack Benchmark. They're clearly very good. It's like most developers actually, they're intuition was like, yeah, I don't trust, Claude pass 40,000 tokens,
 Or opening I or gemini or any of these models, right? And he doesn't you don't need to pick on anybody here and
 Like that's interesting that seems really important we should help Builders understand that. And so we sort of launched, this technical report, the multi-month investigation we tested. I think 17 plus models across a suite of a suite of benchmarks and tasks and tests to see like when do models stop working the way to you expect and you know, Model Behavior is in there is not invariant to the length. The congressman when you use more context, both its ability to pay attention goes down and its ability to reason it goes down. And you know, obviously needle on Haystack is like the easiest task, you have to pay attention to a needle and you have to, if you ever look at like the needle and Haystack benchmarks, like it's all lexical matching. There's 18 words in common but between the search query and the needle. And so it's like a very easy task 0 reason, power required, but of course most like real world stuff like oh you have to like connect these 18 things together and then you need to like all so reason about all that and so that's what we wanted to figure.
 Out. And so, yeah, I think that report I think it's like now, 140 of views on YouTube or something like that and you know, you got any event and people have heard about it. And I think it was popular because it's just sort of like, it ratified things that people were already feeling. That's it was like we all knew that was happening but we didn't have any data to point to it. Yeah. And so then when you came out with that it's like yeah.
 Exactly. It's also funny that needle in the haystack is the only thing we had. It feels like we shouldn't only have that and props to Greg for creating that because I love the needle in the haystack. But at the end of the day, there should be 20 of these to really stress, test, the context window and figure out. Yeah. What is going on and how we can take advantage of it to the map? Yes. Unclear to me like, maybe the labs are with Labs. Probably already knew this really had an intuition about it. Do they have their own internal benchmarks? They're training again, so that they know about, you know, I think the reality is that when you're a marketing, something you invariably have to pick the things where you do the best and you kind of ignore the things where you don't do the best. And, of course, there's something that you don't do well on that. Nobody else. Measures themself on either publicly, then you're definitely not going to talk about that, why would? Yeah. And so, like, I think also, just like the how viciously competitive, like, the sort of state-of-the-art large language model world, is today,
 like, that creates a negative incentive to being as
 Transparent as possible about the models, strengths and weaknesses. And the most people like don't want to believe that intelligence AI intelligence. Artificial intelligence is spiky, we want to believe that it's AGI. We want to play the artificial superintelligence is coming that these things should be better than humans in all ways and like while that may happen someday, I'm not sure there's really strong evidence today to prove that that's the case or not the case. Again, it may happen but like today we don't know but I think the best way to think about these models is that they are very spiky and what they're good at and what they're not good at and of course this is like classic computers as well. You know, cost computers are incredibly good at arithmetic, much better than humans that arithmetic and of course you know, way worse out of the things and so that that's me is like the most likely honest way to approach models and their strengths and we're sort of and their capabilities is to be sort of honestly try to pursue, you know, the map of their strengths. I also want to talk about search and just break down search because it has been
 Something that I know you think about a bunch and we can kind of contextualize how we had keyword search or we have, we have all of these but keyword search. Then we go semantic search and then we start doing hybrid search. And then we now are like thinking about search within agents and all of that feels
 like,
 Problems that you need to have a strong grasp on. Yeah. If you're building with AI. Yeah. Maybe like a quick sort of addendum to that like there's this term that's been thrown around now for a few years like vector database. And we've always thought that that's like a dumb term. These are just like VCS trying to like meme a category of enterprise software as they do, right? But like that's actually a the problem to be solved and that's actually also not what developers like really actually like want to buy as a result, right? These are things are very correlated problem to be solved with people want to buy like they want to solve the problem of information retrieval, broadly. And of course dense vectors search happens to be one useful tool in the toolbox will impact in a second. Why? But certainly not a
 Not a Panacea to all of your search problems and we've never cleaned that it is either. You know, the way that I kind of describe what chroma is then is that it's not a vector database. But we're building is modern search infrastructure for AI or maybe modern retrieval infrastructure for AI. And like the modern pieces in contrast like Legacy search systems where chroma scales much better as much much, much, much easier to run, it's much more cost-effective and we really been able to put, you know, the last 10 years of distributed systems research. Both on the theoretical side, on the applied side, like into this architecture, to make it that much better. And then, the four AI part, I think is really interesting to unpack is it means many things like it means, the AI developer. So it's no longer just a search engineer, who is implementing these systems. It's everybody, every single engineer on Earth. And frankly, we're even seeing like semi technical slash non-technical users also starting to like sort of wheat, you know, weighed into these waters for AI. Also means AI search and ai workloads. Ai workloads are different than classic.
 Search workloads, classic search workloads. You have, like, one giant index that everybody's querying and AI workloads. You'll have many indexes every team, every workspace, every user will have one or many search indexes that they want to query over. And so that's like a much different shape workload that kind of the technical level. And then I think the other observation here is that
 In the past, humans were the ones, doing the querying, and humans were the ones, doing the last mile of search, which is digesting the search results, and so much of search for. So long has been about 10 Blue Links because that's all that a human brain really can assimilate and absorbent process at a time. But now, with llms, like they are doing a, the majority of the querying and they're not just doing one query. They could be doing 100 queries. And for every query, they're not just processing 10 Blue Links, they could be processing, like tens or you know, hundreds of thousands of pages of documents. And so like that's a much different thing at the end of the day. Also just to interject about how you have.
 The shape of search so different with these many teams that are doing different queries on different, corpuses of data. And sometimes it overlaps, sometimes it doesn't
 I hadn't thought about that. And then the next thing of like,
 Search now has transformed from Tumblr links where we as a human interact with it in a way that we're looking for. That headline. Is this relevant to me? We click into it. We kind of look and maybe, maybe not know, just try to process as much information as you can and then give it back out and see if we can get that like squeeze the juice out of this great fruit as much as possible. Yeah, exactly. And I think like, the way to reason about this like pipeline, then of like, how do you bring information to the model, or how do you give the model a tool that it can use to go get information? And I think like a two-stage process is a good way to think about it. And the first stage is you want to maximize recall you want to get all possible relevant information because if you miss one piece of relevant information you go get it wrong. Right? And so of course, if you're maximizing recall, you're not going to be maximizing Precision, sort of by definition, right? So firstly, first age is maximized recall. Well, unpack that in a second, and the second stage is maximizing Precision because also contacts wrote, you know, contact us or not.
 Perfect. They're very sensitive to distractors as well. And so you want to like, then basically a massive pool of relevant information, maximizing recall and then call that pool down to only the most relevant bits before you have the model. Do it kind of a final reasoning pass over it. And so this kind of two stage pipeline it is a helpful way to think about the goals. Because do you see still that, as you mentioned it's very prone to distraction very like I've noticed that, sometimes you give it more information and then that just fucks everything up. Yeah. Oh yeah. I mean, we've seen for sure, like, within we've done some experiments and like, kind of agent learning and they're sort of intuitively who makes sense. Like, if you give agents access to Prior similar situations, where it did a thing and it was successful, kind of give it a few examples of that from dynamically if you shot prompting when it comes to a similar situation again. Oh this would be really this would be helpful. I the human would find that helpful probably and what we find the models just tend to sort of like slide straight into that local Minima.
 And they're like, oh my gosh! Thank you. You already gave me the answer. I don't think about this anymore and like, the most Insidious distracting information is the information that looks relevant, but it's not for some subtle reason. And so, yeah, this is definitely a hard problem. Wow. Okay. And that, just a bit of a tangent goes into this whole thing on memory of task, completion, and how you can help the agent complete tasks that it's already done before or that it has kind of done before. And so you're like, hey, you've done this before, remember how you did it. But if it is a slightly different and it looks the same, then you send the agent off and it doesn't do it. And you're like why you were so close? You had everything you needed. Yeah. Yeah. It's definitely not a solved problem. I think that, you know, kind of the the data pipeline of retrieval or a part of the solution of what people want to call memory, which is like kind of Engineering in some sense, right? There's sort of a right path how do you glean insights from prior contexts and
 And how do you store those? And then there's, like a reed path where your path. How do you use those prior insights in the Next Generation by the continual the same trace or for historical traces but it's yeah. So then solve problem, I think the data piece is certainly an important part of it, but not also. Again, they Panacea like they're probably needs to be some like learned representation to really be the level of expressive that we want these things, to be able to be. And I think the goal really is like to be able to communicate Tacs and knowledge to models and models, like the themselves, lean toxicology as they're doing things and kind of like build up a library of skills and I think the library of skills. Again, both exist in documents load into a search index but also probably need to be some way and into like the weights of something. So, all right so I got you off track with precision and recall. Yeah. And you're talking about how basically you want to maximize both of these first. It's recalled and its precision and then yeah so maximizing recall. I think it's worth unpacking that because you asked a question about, you know, dense vectors search text search lexical, search,
 Many terms. That's true. It's actually surged. There's so much misinformation on the internet about this. And you know, I'll make the point, you know, basically unless you have data, it's just you're just an opinion, you know, and there's just so many opinions flying around and there's like it's just so mimetic too, you know? Like, so, and so does this. Therefore I should do that, like, have you even thought for a single moment about either why they do that or they're right? They're skills. Exactly, I don't know if you remember back in the day we used to always say in the community just like
 When people would come in and be like, I'm gonna build this gigantic ml Ops platform. I need these tools and this, because, and then we're gonna throw it all on kubernetes. And it goes to the meme of like I have 12. Concurrent users. I guess I'm ready for kubernetes and then we would be sharing the same article which is you are not Google. It's like that. Classic blog of like, don't trick yourself into thinking that you need something that Google needs because Google scale is a whole different level. They're company, their business is a whole different level and so yeah. Back to this point of like you need to really like kind of be Discerning on what you see and why they're doing it that way just because they do it. Yeah take inspiration but don't think that it's the Bible. Exactly. And so I'm packing this like goal with first stage or first step of maximizing recall like how did how to think about that? Well you should think about it. First of all you know you shouldn't just copy and paste with somebody else's done you should spend the five minutes.
 To think about it. And you know, for some applications, get to be, you know what information, which files are currently open.
 For other applications. It's going to be wish files or make sure recently touched or recently created right reasons. He can be important for other other situations, you know. It could be semantic similarity. It could be text sort of matching lexical matching. I think a good way to reason about kind of the strengths weaknesses are like dense and dense bars and lexical. Search is
 Dense is useful when you don't know the language.
 Of the corpus.
 So Naples, say that the other way full text search is very good if you know the words to search for. And actually in most Enterprise search workloads, you'll see lexical full text searches being, the primarily useful tool because I'm looking for a document that I made in the past. Yeah, I kind of already know. I remember where I can't remember where it is, but I kind of know what I named it. You know. I know what words are in it and like, this is actually not that hard to find document using lexical search. Semantic, search comes really helpful when you need that semantic expansion. You need that term semantic matching. I don't remember, even what the words were, but I need to go and find it around this idea. Exactly. Exactly. And so these tools are again, not I think most applications is not an either, or the complementary. Now might be 90/10 or 1090, depending on the use case, right? Like, which of those are the most helpful, but I think it's like, almost like never in either. Or in the court. This way people call like hybrid Circle, things like a dumb phrase because like, nobody knows what it means, it means 10 different things and so we should just like not call it that
 You know, she's just be, oh yeah, I'm using X Factor search and lexical. Search say that. So, it's a mouthful, though, it is an awful, but at least it's, at least people talking about. Yeah. So I think that going back to like the dense and Sparks, and like, closer all. So there's this recent, I will
 This recent paper from Google on like the limits of single dense embeddings.
 Great mathematically provable. I think every kind of already knew it. But again as this like good use of it put, you know, math around with reality, kind of like new improved. It which is great, you know, dense embeddings have their limit. And of course, the entire internet there's graph that shows like bm25 search as it's like perfect flat line and then like all these dense vectors approaches like failing and like much much lower kind of yeah quality. And again, that's fine. That's not wrong. But if you go and look at you, look at the actual like data for, they use that experiment. And you look at the queries, they are all lexical searches. And so, of course, between 25 is gonna freaking crush it at that because like, 25 is, you know, full text Search terms frequency bass. And so like, I was gonna do it very good at that task. And of course, again, the Nuance is important to helpful, but, like, what the mean that went to the internet was that like, oh my,
 Gosh, all the dense vectors stuff is BS. If you useless you just need 25 and like maybe that's true for your use case but like maybe it's not and again I'm just really now ranting about what people aren't thinking enough. So I'll stop there. Yeah, I want to bring in one other piece, which I think is important with search which got brought up a few weeks ago on the podcast with my buddy. Nishi. And he was saying how
 For him, a lot of his search woes and problems. Come in when someone searches for
 Romantic dinner and it's like, semantic, search. Kind of can do that. The idea is there, but it's not quite there and he was having the hardest time because what was happening is you would come in and be like, all right, well romantic for you means what you need a bit of personalization. So if you're talking to the lemon and you're saying, I want to plan me a romantic dinner with my wife, right? But romantic is for you is one thing for me. It's another thing and and then he was saying you can just really have problems on
 How we have you asked the new type of yeah. Experience when you're trying to bring like old stuff into the new ux that we have where people can ask anything and say anything. Yeah, and I wonder if you've seen that where like, okay, these, these two types of searches are not breaking down completely, but yeah, you don't have them there. And another example that he gave which I'll think about too. Is how
 We're semantic search. Didn't really work? Is if you are a vegetarian and you say, I want vegetarian pizza. It's not normally going to give you cheese pizza as an option because that's not tagged as vegetarian. Mmm, you know? So and even in like, in Vector space, like, vegetarian pizza. Yeah, yeah. Cheese pizza is way over here. Totally, totally, totally, or even more complicated to say, me, a romantic dinner tonight. Yeah. No, like, which is open, which ones are close to you and your budget. Yeah. And your vegetarian, and like, you know, it becomes a pretty complicated query. I think it is a thing that like an LM 10 internally process like fairly well. Like, he doesn't have to be all rigid explicit system, but I think, you know, the rating dinner piece like I think it's possible to get that out of vector space because
 Kind of how I would approach that. Like, let's say, I'm Yelp, right? And I want to support like, romantic dinner. I'm going to take all the reviews and then I'm going to
 Have an L, have an LM generate, maybe like 20 to 30, like tags, like, for every review, and then those 23 tags can become like clusters. So, I understand. Now like what clusters of ideas are like, commonly associated with this establishment and then I can associate, like, either the centroid of this clusters, or maybe all the information, those clusters with that business. And then now, when I start romantic dinner, and as somebody else said, like whatever a synonym to romantic is cozy and intimate evening meal, you know, like actually you might be able to get that from the reviews so I don't know that like that is impossible to get out of like vector space. But I'm see that level of fourth thought that you want to support the kind of a query, and then the level of then work required to kind of make that query possible again. Requires you to think it's not just all for free. So yeah, yeah. You can't just slap cement search on it and be like we're good now. Yeah type whatever you want. I mean you can but like your competitor is doing more.
 Work. And therefore, your competitors product is better. Yeah. And that means they're gonna win. Yeah. Experience is gonna be much better. Exactly, exactly. Yeah. I tend to these days when I look for coffee establishments because I like V60 and I like the, you know, the specialty coffee. Yeah. I don't type in.
 Cafe because that can give me anything from Starbucks to Dunkin' Donuts, you know? Yep. Yeah. What I'll do is I try to search like V60 as a term and you have the cafes that come up and they'll they'll show. But this is like super easy because it's like a review has V60 in it, right? But sometimes on notice that the review doesn't have V60, it's just like, oh, this is a specialty coffee place or this is a coffee roasters. And so there I'm like, hey, this is interesting search issue really that shows how you've adapted to the strengths of weaknesses of the system. You know, I think one of the interesting
 Ux challenges and opportunities now with LMS, because we all became very good at Googling. Yeah, and like, how do I compose this question? I have into a set of terms and minus signs most friction and like we all learned how to search but within the limits of Google's oncology and there was professional search Google Earth. Right. I mean he doesn't know that our way better at people that are way worse PDF, you know, like the whole shebang and like now llms it's just a sort of its beautiful like a magical box and that's a good thing because users don't have to, like, do the mental, you know, gymnastics. How do I change the question that I have into this particular kind of domain syntax Google is going to accept, like that's actually a better user experience, but of course, that means that users are going to ask a lot more questions and a lot wider variants. And, you know, in some ways, I think the cool part about like user research protective is like now.
 Application is do have a larger surface area of like how they can help their users and of course that's a blessing and a curse. Right. The blessing is like you can do a lot more. It's pretty. It's pretty cool. The curse is that like you have to do a lot more and like you know again it's not all for free. Yeah and you're you're now supporting all these different use cases and all these different ways that users are doing it and
 If one of them is kind of a shity experience, you're struggling. Yeah. I as a user, am not going to look at this new AI feature that you created as something that is worth my time, right? And right, actually, it kind of brings me back to context wrote in a way because if now, we're talking to these AI features so much and we're continuously going back and forth with them. You put yourself in the position to have that context wrote, right? But you have to almost be like a janitor of the context and really get good at yeah. Cleaning it. Yeah. And one thing that I saw last night from a guy at the event he was like, I'm trying to create like a jupyter notebook style thing for the context window and I want to be able to like turn off different pieces of my contacts because it's not needed because maybe there's sometimes that I'll just ask something and then it's there in the context and it's always referencing. Yeah. And so now
 It made me think like, oh man, like context.
 Kind of etiquette or not etiquette, but context, like cleanliness is a thing in a way. Yeah, 100%, you both see it in like, multi-turn, kind of user Model, A conversations Jetty, Etc. You also see it inside of like a genetic Loops where like there's LM running in a loop and it's, you know, getting some context generation output and then it needs to generate then it needs to receive that output into itself and then generate, you know, more information and I think your intuition is exactly right. Which is like this eventually breaks down. You know, we've seen stuff like inside of cloud code, there's the slash compact command which does like a summarization and tries to like kind of clean it up. I think I saw something from the cloud code team on Twitter recently, where like, they don't use compact, they actually just they just dump it. They just clearly. Yeah, clear it. Yeah. Yeah. And actually we've done some research that is like shown that sort of verified that result that like actually naive summarization of the context history is
 Like no better than just drinking. Yeah, of course we start from scratch is also sort of cheaper because you're like flushing, all that baggage. So begs the question though like
 How should we think about?
 Prior State being available to sort of the current iteration for Again by their multi-turn user conversation or an agentic Loop. And I think one way to think about this is like this certain compaction content compaction or context when they're distillation or somewhere there and it is not a solved problem, I don't know, maybe a solve the problem, but there's a few interesting ideas, you know, one is a user prompt of some kind to help the model isolate, the relevant actually like relevant High single pieces and then pass those down to the next step. Because a lot of like, crafting agents, you just jumps of laws and these jobs of test results and like it can't pay attention to all that. So like giving a model the ability to like kind of snip out the piece that are actually important for the next turn to the model detailed prompt. I think is a hell, is more helpful than a generic prompt. And then the other thing that'll say, is that, like another interesting approach to this that I've seen some people take is that they give the the next sort of
 turn of the loop, whether it be the next model or the agent, the ability to search
 In the history of that actual individual conversation trace or whatever. And you know, these things are not also mutually exclusive. You can do both obviously but it's not easy. How are you seeing the search done with that in within the context or within the model doing that? Because that does seem like
 The obvious step to take, right? Like, hey, if we just make the search, the big piece here? Yeah, it's kind of like the theme of what we've been talking about this whole time. Like how important search is and how not solved yet. It is are still like there's still a lot to be done in the new paradigm. Yeah. I see the way that most people are doing this right now, the predominant strategy people use you call it search. But it's
 It is kind of search but it's also maybe not search, is they'll just use like a file.
 And the file will have the list of to Do's, right? Like I want to see the teacher, I need these 15 things in this order and then each like checkbox gets encapsulated by one kind of agent locations, which minimizes or helps cut down on like the baggage of all the context from all the difference. So like the sub agent ideas. Exactly. This is why I said ages. I think are like a powerful idea is like, literally, it's because context rocked, I think implies the existence of like contact window and capsulation. That's why he's like a good idea. I mean in any cases. Yeah, of course their challenge was that agents which all of a sudden has to be able to report up to the orchestrator agent the right compaction information as well. It's like I got your next paper, man. Like how many sub-agents until you get sub agent Raw?

It, we've looked at this a little bit. We've done some experiments with a deep research agents which are this shape of thing. A little bit different. You're not a checklist that you're running down but it is. The orchestrator says, okay, these are asked this question. I want to look at these like 85 places, so I'm gonna have like, 85 sub-agents. Go out and do a bunch of research, but I was just study agents. Can't bring back all the research that they find, they have to compact before they boil it up to the orchestrator, right? And
 I think in, I mean, you know, state-of-the-art today, maybe is like the Deep research and implementations inside of Clawd, you know, Etc. And I don't know if you've used those with like topics that you know about. But yes, usually like this. Yeah. You're like um, kind of I guess. But I saw a great tweet that was like, I used to research the other day for something that I know a lot about and it wasn't very good. So I'll just use it for everything else instead.
 which is like,
 It's the point, hence, the nail on the head, right? You know, what I think is also something that never happens is, when you have these sub agents, and they go and do their research and then they come back. Yeah, they're never gonna tell you. I didn't find shit. It wasn't relevant, like, don't use me as a sub-agent. Yeah, that is not one of the out of all the possible outcomes I can guarantee you. That's not one of them, probably speaking. You know the ability for models to know what, they don't know. Still seems like unsolved and of course you can put into the prompt like if you don't know, please reply, I don't know.
 And then that can work and it doesn't make some models, are more cautious than others. Like clawed is like much more cautious about like claiming to know things versus like opening eyes like willing to take more leaps for example and again that's not always it depends on the context whether that's a strength or weakness but you know probably the conservativeness of Claude is a reason that developers tend to really like clawed so because it doesn't blow up your database it doesn't just go delete shit. I mean I I'm sure there's examples of it leading shit. So then I feel like I got you off track again. When we're talking about what we're just talking about with the so you're spawning these sub agents and they're coming back with their information. It's I've heard it put as like in accordion, where you can look at an accordion and it goes out, and it gets all this air and gets all of this stuff and then comes back in and you like condense it down to these like five slits. Yeah. Yeah. And that is by the way, Max doesn't recall maximizer. I like that.
 Same great.
 Visual of. All right, we're just gonna get everything and then we're gonna figure out what's important. Exactly. So, you recently put out Android chroma or what is it exactly released from a swift into beta and then imminently, we'll Antico Madrid. Is it somebody that uses chroma on their phone? Or is it that now you have a chroma instance on the edge? Like, give me the brake device intuition, here is that?
 Intelligence will be everywhere.
 Certainly in the cloud or already is increasingly on devices that you own your laptop, your phone? Yeah. And inside of every device, as we know it in the future maybe we'll have you know
 Devices that were previously inanimate to become animated, you know, talk to the plant behind me or something. I don't know or the cameras. I'm sure there's a world where we don't have to hit auto-focus when we just say all right. Yeah. That would have been great. Yeah.
 And then within that right there are
 trade-offs, there are pros and cons to doing.
 Surgeon retrieval locally on whatever device that is versus the cloud and they don't have to be music exclusive. You can also do both. And actually most use cases that we see people doing stuff on the phone and I've also having sort of a cloud syncing story going on, but of course local its privacy preserving
 So you can give the user those guarantees.
 Local is also going to be if the facto faster because going over the network and then local is also going to be you don't have to worry about like you can be in a place where there's no reception.
 Right. So yeah offline connectivity and then also sorry cheaper as well. If you're doing the compute on the device, you already own, you've already paid the money for that compute and you can just now use that compute as you want and so, you know, those are very good reasons to do things. I think probably the Privacy preserving reason at the biggest, use case that I feel like exists there, it's just like really great to be able to like have users do things on their own device and not have to worry about like the data being aggressed unless they explicitly wanted to be. Um, so yeah. But there's a lot of really exciting stuff there. I mean, like we're a little early still and like the Arc of this, but I think also like the reason that Chrome is written in Rust and like, because Chrome is written in Rust, it can run anywhere. Yeah. Like, we all. So, there's some kind of early early work in like, getting chroma to run on like robots.
 Because like they're all so gonna need their own like level of memory and like yeah. So you know maybe like the largest, you know, install basic in Chrome of chroma in 10 years is going to be you know inside of like 5 billion robots, you could think about that way. So and what was it, what was the engineering feat that had to happen? Did you?
 Make it super lightweight so that people could just grab it and have it on.
 Their device without any worries that this is going to break my phone.
 Yeah, I mean, rust gives you a lot of safety and guarantees which is useful. We
 Have now a single node version of chroma, which you can run as a server.
 We also have a fully distributed version of chroma, which you can run across many nodes and a cloud environment. That's also serverless.
 And then that single node version can also be run as a library embedded into any given context. So today people very frequently use chroma in Python with just like python bindings, all the underlying rust and they'll use that inside of jupyter, notebooks, or inside of Python scripts or demos or all kinds of other. You know, contacts. It's like really useful to not have to like have a server if you're just doing something like your weight, but of course when you're ready to go to a server, the API is the same and you just change how you connect to it. And then when you're ready to scale to like, you know, petabytes of data in the cloud, you get, you just change how you connect to it. And so, like I think that as always been our goal, it's aspirational. It's not easy to do but we wanted to make sure that like wherever you have a search or retrieval workload, like chroma is there to serve you. I like that. And you don't have to like pick and choose. Do I want this or that? The answer is you can have both and it's the same API.
 What made you think this was necessary? Were you like hearing people ask you for it and they were like I'm building this app but I need something super small. I need to have it on device. Yeah. What I mean? Not not everybody I guess but the ideal version of running a company is you do have a vision of what you're building. Well in the Visionary card on me, I'm not going to actually I'm going to walk it back like immediately but hear me out. You have a vision of what you're building is 10 years in the future, but the reality is that in business you get 0 points for being write 10 years from now, that's true. Does not matter at all. Would you have to do is be right? Six months from now and
 Ultimately, you really have to be, this is cliche to be clear, but like you have to be obsessed with like your customers and what they are ready for what they need and want. So there's no reason that we couldn't done chroma on mobile.
 A long time ago, but nobody cared until very recently. It was in the last few months we've started to see more people really want that. But why do you is there apps that are being what apps are being built? It's I mean it's a lot of AI chat stuff. It's a lot of like media you have on your phone that you want to like you know research and retrieve over. You know I can't I can't clean predict. Like why the exact timing of these things happen? You know, similarly we're chatting off camera before we got rolling. Here is like, you know, accuracy. We've always thought is incredibly important. Yeah, you can only manage what you measure like, why would you not want to know how this thing is in the real world? And then nobody has really seen to care that much and Niche of developers and Engineers have always cared obviously, but like the masses haven't? And so we've sort of like, held off like doing much work in that domain and particularly the most of it is like not research. Most of it is like
 Product right? We've kind of like held off for the reason that like not we didn't think it was important but that like the our customer base didn't yet believed it was important. There wasn't a masks from the community like hey we really need to get this accuracy thing solved. I mean some, yeah, exactly. Sometimes people say they want a faster horse so they actually want as a car, you know. But they maybe that's true. But like when they say they want to faster horse, you know, you you sort of can't jump all the way to selling them, a supersonic aircraft, you know. And so like I think just like being very sensitive to the timing is is kind of an underrated underrated skill and I mean you can use the word like taste I think. Yeah, tastes maybe another history. The Lindy version of the worst case is just wisdom you know like and it's hard because like a one-hand again you you have a vision what you want to build and you just stay very true to that vision and believe it incredibly deeply at the same time. You kind of need to be very
 Reactive. But you need to hold that Loosely. Yeah, you're near term plans and it's like you know no no plan has survived the first minute of battle, you know? Like that's like that is absolutely the case.
 yeah, that's such a great point because you get
 All these inputs. I imagine every day of people that are asking for things and ideas that you're seeing out there and then you think, wow, we're going to do it like this and when you bring it to the masses then it's like actually we might need to change it. How it's done like this but it's through those iterations that things happen. This is why I tell myself that, you know, the time that I spend online, you know, is like goodbye time like it's your Twitter time. Taps me into like the public Consciousness, if you will. So this is very Visionary on the basically Chrome and running anywhere at any scale.
 What else are you thinking about? As far as like the vision of how things are going to change? Is it on the Precision side or you can start building now, 4 Precision, because it feels like people are interested in it.
 I think it's very good intuition.
 We don't have kind of the specifics either planned out or kind of register all the thinking about, but I'll share like a high level observation. Which is, the reason that chroma started is, I had worked in applied, machine learning and developer tools for 10 years and
 Building Technology with machine learning, now we call it AI. Thank you, thankfully, we've dropped calling it Jenai, very happy about that. Oh, did I still see some shit? Somebody the other day was like Jen aiops. I'm like oh really? That's that's the term. That's not term. Yeah we're just gonna say it right now. No. Sorry. I digress know. Yeah. Um the goal of chroma
 Going back to that experience. And honestly, the pain that I felt building stuff with
 Applied machine. Learning a lot of his in the computer vision domain is
 This is really powerful. It could change the world but it feels a lot more like Alchemy than it does like engineering. Yeah, I'm sure you've seen the xkcd comic where the person's like, you know, oh this is your machine learning Pipeline and he's the guy standing on a pile of garbage.
 And he's like the follow-up questions, like what if it doesn't work? And the guy in the garbage is like, oh, you just like, you know, you fix it. Yeah. Mix it up. And try it again, you know? And that's still the case, right? That's actually still kind of the case today. It's true and change some proms. We turned the temperature. We try and figure out the recall whatever. Yeah.
 So true. And like, I think, you know, a lot of the labs maybe because they're sort of fundraising requirements have to sort of Market. This idea that like, we're building God, like we are literally building a deus ex machina. We're building a God will emerge from the machine and like it will solve all of our problems. And like we don't have to pull that sort of party line because we don't care, frankly about that. And I think it's wrong. And so our term has always been AI is useful.
 And we have like stickers. I'll give you security AI is useful. And of course, it's only useful. If you give it the right information and the right context and it's only really useful, if it can learn from that information too. And so, that's the stuff that like we are really excited about. And really seeing to be where the people on the bleeding edge of agents are kind of running into the edge. Yeah, current capabilities is we have these large state of the art reasons. They're powerful. Anyways, they have their own weaknesses obviously, but you can kind of work around them.
 We have also, you know, these retrieval systems, search systems, they are very good, what they do as well but it kind of feels like the map's not complete yet. Because sort of Again by definition, we're not getting the capabilities that we want. And again the capability that I want is like, you know, if I pick up a cup one time and I put it back down, like I know how to do it again. I don't have to like, you know, think for 45 seconds for how to do that and that ability to kind of like learn from experience, is something that would be incredibly useful for making useful stuff because the real world is practically complex and the real world has so much so many education, I think people that work machine learning a long time. You know the phrase getting mugged by reality, you know, will resonate quite deeply. And and so we need a systems that can can learn to adjust edge cases. Otherwise, you know, we're just kind of doing what we've done, what we did in computer vision and autonomy in the first era.
 Or we're just like putting on like, 100 Band-Aids on top, and around this thing to try to make it work. So that's why it became so hard to put any ml model into production, because it's, it's exactly that. Like, and there's if you're in the Enterprise, there's just so much alignment that needs to happen. And so many meetings that you need to have cleared because it is
 A lot more risky than software. It's not running the way that you think it's gonna run every single time and so, yeah, you have to be okay with that for your use cases, right? You probably remember this phrase from because maybe 27 teen.
 Means 26, 2018, it was like inside of things, either the title of a Google paper or inside of Google paper that was machine. Learning is the high interest, credit cards, go of technical debt. I still quote that we had these go on here, back in the day and it was like, legendary, it was one of the first people that I was interviewing. I was like, oh, how am I on this call right now? Yeah. And it is that. And you know, what's funny is, there's that new way of looking at it, where you remember the diagram in that? Oh yeah. Where the model was the small piece and then everything around it. Yes. It's the same thing. Yeah, it's just that now the stuff the boxes around it have changed. Yep. But it's that's where the hard part is and it's a little bit easier. The, because now, we can hit an API for the model, you don't have to create the model but still it's the same idea. Yeah, yeah.
 What's old is new again? Yeah. Did you get a professional to design? This will TJ is a professional. So no, I mean like the the office. Oh, the office. Yes, we also had professional do that. Yeah, but not some guy, our team obviously.