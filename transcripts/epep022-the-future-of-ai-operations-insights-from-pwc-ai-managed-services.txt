We have a very special episode. I have the opportunity to hang out here in the PWC offices, in San Francisco for my trip. And I'm here with Ronnie, we're gonna talk all about what she's been up to and how you're looking at.
 Ml Ops AI Ops agents, everything of the sort.
 We should probably start with a little bit of who you are. Sure. Thank you dimitrios and excited to be talking to you. I've watched a number of episodes of your podcasts and then I decided to be in your own will I imagine you were forced, absolutely not decided to torture. You it's 100% off my own freezer but in so, lovely to be here, talking to you. So a little bit about myself, I have about three decades of experience in the Consulting industry and over the last 20 years, I've been with PWC today. I'm a partner with PWC and I lead our data analytics and AI managed Services space. So with pwcs spent the majority of my career in healthcare in Payson providers, and then over the past five years, I decided to make a shift, you know, back into technology. And this was a very it was a very deliberate shift where I
 Wanted to move into the managed Services space and the way PWC approaches managed Services. We want to lead with Automation and AI so that excited me because Management Services or keep the lights on is a space that consumes over a large majority of a cio's time and resources and all of that and having the opportunity to shape that to be more efficient. Excited me for the uninitiated. What are many services managed Services is the space where we take over, support of anything that moves into production. So it's some people call it. Keep the lights on it is. As soon as somebody works on a project its life with consumers, all of the support related to that software asset, whether that is the hardware, the software, The Experience, right? All of that falls into the managed Services space,
 And the area that my team focuses on is data analytics, and Ai. And I think AI Minette services in itself is one of the coolest things that is happening today. Yeah. Have your work cut out for you. Yes, you thinking in production and making sure that they're
 Going off without a hitch. I imagine there's a lot there.
 Definitely definitely. It's an exciting time to be in this space a because I think AI is at area that has been growing exponentially and at least in my lifetime, it is the area that's I've seen grow the fastest and so not only our new technologies and tools being introduced all. So the pace at which innovation is happening which is the applicability of AI. That is also something that I've never before seen, right? The pace at which things are growing. I would say 12 months ago, I was in conversations with clients where they were talking about investing in Ai and experimenting a piloting in AI today. Fast-forward, even in the last six months, I would say I'm seeing clients who have implemented, Ai and are now looking to, I thinking about Roi, how to scale it, how to sustain it, which brings in right the manatee, Services component of scaling,
 Sustaining and so on and so forth. Yeah, it's so funny that you mentioned that because I think we went through the exploration phase know, it is in the face of. Okay? So what can this actually do and how can this how can I prove that it is
 Mmm, better for my company. If I'm using AI, I'm sure. You've thought you've seen the recent paper that came out. That was saying people using AI think that they're doing things faster, but in reality they're doing it like 20% slower. But when they're asked
 Hey, did AI help you? It's like oh yeah it was way better. And
 We all instinctively.
 Are probably a little bit like that where if we're using AI in some way shape or form, we feel like, oh yeah, this is better. But then
 If you were to compare that with us, not using it. I often wonder. And I asked myself sometimes like oh yeah. Is it is it better? Am I just following myself and a friend of mine told me? Yeah, dude look.
 The experience of chatting back and forth and getting being able to say, like okay, now go off and do that.
 And you know, write me that thing that we've been talking about is so much nicer than having to really like squeeze every one of those words out of your mind and all that. But sorry, I digress. I went on a bit of a tangent, it's all good. Yeah, I think that is a the it's probably a two-part conversation, right? There's a personal growth, a personal productivity that which which is what you're talking about and then there is the Enterprise growth, Enterprise productivity right on the other hand too. And you've got to like take both of these together. Yeah.
 And one of the recent studies that I saw said that, you know, if you're not spending say four to eight hours a week or like X percent of your time learning and studying and upskilling then you're not going to be AI enabled. Yeah. Right. And I think that is true for personal growth but also for organizations who are looking to upskill their employees. There is a huge investment in terms of training and, you know, giving people access to the tools and technology so that they know how to be productive right with all of these two. And I think there is a whole change management piece that goes along with AI enablement. Which if you undermine that effort, then it's never going to be successful. It's a cultural thing. It's a cultural thing, 100%. Yeah, so ground Us in the current state of what you've been seeing out there with it operations. How you see mlms? Maybe let me start with what is the current state of
 organizations, right? This tons of data out there, you know, most organizations I know have alerting systems. So there's lots and lots and lots of alerts that if a human being has to sift through all of it, a lot of it is noise and I'll give you maybe a couple of examples
 So, incident detection today is, you know, it is. So if you get a hundred alerts, which one is truly an incident? Which one is not, is something that if you have to go through 100, then you're losing time on it. Whereas if you train a model to look for these, you know this criteria, it's going to pop up. Hey here are the 10% of alerts which we think you need to look at and then triaging becomes a lot more easier. You're Bringing Down the mean time to resolution just by doing that. Another example is, you know, when it comes to infrastructure, right? Like the say, CPU utilization, resource utilization, sometimes tends to be cyclical. And if you are actually monitoring for certain events, you know that the usage is going to spike, six hours from now, or 20 minutes from now. And if you had a system that was looking for these things, then you would know. Hey, I need to add more memory to the server. Otherwise, you know, this is going to crash in like two hours. So this
 is what I would call, you know, proactive monitoring where you're preventing an incident from happening because of how you're effectively using your data. So those are examples of how I think mops and aiops can be combined to create a very compelling case for it operations. Yeah, the
 Predictive monitoring is incredible. I really like that. And then also, alert fatigue is real. It is real. What about agents and where you seeing them? What have you been doing with agents? Because that's like the buzzword of this the last year year and a half. Yeah, I think the, the concept of Asians is very real and we are starting to see I think some organizations adopting agents first and foremost. There's a lot of process standardization work that needs to be done if everyone is following a different process, then it's very hard to put an agent to replace a, or to do a process, right? So this process standardization, the second piece is data quality, right? I do not know of a single organization that says that I have perfect data, the quality is great. So, you know, there's an and so for these two things, I, I do see a lot of data modernization, app modernization type of initiators that organizations are taking on
 And so once you have this Foundation set, then it becomes a lot easier to go and say, well these are my standardized processes and these processes get used 75% of the time. So I would get the most bang for my buck. If I wear to agent defy these processes, right? And in, and then, and I am oversimplifying it as I'm kind of walking through this, right? But the other piece of it is, you know, I often get asked
 Should we start with one agent? One process. And then look at how that goes experiment with it, build the pilot and then scale it, or do we do multiple, right? And so I've done a lot of reading on this and thoughts on this, right? And what we've seen work is I think centralizing the management of Agents is important because you do not want different people just using different technology within the same organization to build their own agents. So there's some kind of centralization that's required that also then advocates for responsible Ai and all those ethical considerations but then it is okay. To start parallel development of Agents, if there are processes that are being used, 75% of the time and four different departments.
 Start writing up like four agents and today with all the ideas and the low code, no code Solutions, it is so easy to spin up an agent. So, I think the most, the majority of the time needs to be spent designing the agent, then a little bit of time, writing the agent and then the rest of the time in evaluating. So the evals and the, you know, and measuring the impact of the Asian looking for hallucination. Looking for biases. That's where then the rest of the time gets spent, right? So this is how like we see a lot of clients, like, embracing Asian, take clothes. And I think this is still pretty nascent, right? There's not a lot of, you know, I think organizations who have agents too many agents and production, right? I think we are still in that like I'm building an agent. I'm evaluating it. Yeah, yeah.
 It reminds me a little bit of the MLB stays early MLM stays when I started the community in 2020 where a lot of folks were trying to get the models into production still. And yeah it's just really hard to make something a that.
 You can clearly attribute Roi to and then.
 Be something that actually like works. That's right. Yeah, and my buddy Zach was just telling me this morning. One thing they see is, if folks can get an agent on these standardized processes into production quickly, then they start to see where the
 Long tail of their data or their knowledge base is missing. And so, because you run into problems constantly, and then you have to
 Amplify or you have to really enrich your data and it's almost like a way for you to recognize where you're faulty. Yeah, I think it is interesting how we build upon these things. And, you know, yes, you get your agent into production, and that's what I think, you know. So that's what, that's what makes Manet services. So exciting for me because these things are numb and production and then we are tuning optimizing training it so managed services, for AI takes on a different meaning than your traditional managed Services. Right? Traditional Manet Services is all about breaks Incident Management. Make sure the systems up and running.
 Here we are not only doing that but we're also making sure that we are delivering on the right outcomes that that it was intended to deliver to begin with. And that means that we are, you know, constantly measuring. And so even the skill set of the people that we have, in AI managed Services is very different from your traditional Management Services. Folks, we are hiring mlops Engineers, we are hiring data scientists. We are hiring, you know, more advanced analytic folks, and who can tune and optimize the models to kind of reduce the bias reduced the hallucinations and so on and so forth and so on that note, right? We also have a framework Called Agent OS. That PWC has developed for our clients, so we can go in it's all based on, you know, python base Frameworks and things like that. So it's very interoperable works with all the hype scalars has apis to connect into any EC
 System. And it is an agent framework. So you can build one you can. It's got an agent orchestration layer inside it and I know there's a lot of them in the market, but when we go to clients, we do go with a framework and you know, I think that's one of the things we tell the clients that you don't have to build from scratch. There's so much that's already out there. You should leverage and then build on top of it. So then the focus becomes the clients specific business outcomes versus trying to figure out the technology that you need to make this happen. Yeah that makes sense because as you know from last night there's a new startup that comes every day. Every minute I think in this space it's it's a hot space and again there's a lot of unsolved problems so that makes it right for the startups to come up and try and find the best way and the most optimal solution. Yeah, and you said something interesting there where it's different from.
 The traditional managed Services because there's not that break fix. And it's it feels like it is very

And like random but not random in a way. I guess, that's not the right word, I'm looking for, I don't know if, if that makes sense like it's, you have to kind of just let it
 Let it do its thing or break, fix is very clear because it's doing this thing and it's not working. Right? There's so many other things that could go wrong with an agent, that it could be working in that it actually executes, but it's not working because there's all this other stuff that it's doing wrong. Yeah. You need a human in the loop on until it is proven that this model can work with a human in the loop and there are probably very few use cases today. Where someone would be comfortable making that you know that just judgment right then and it's interesting I think we've when you talk about humans and agents that are so many this jargons that have come out for lack of a better word like this human in the loop, that's human on the loop and there's human out of the little. Oh yeah.
 It is, you know, how many more ways can we use those words together, right? The human in the loop is where you have to approve a workflow, right, approve, the outputs that the agents are coming up with human on the loop is where you're approving, only the exceptions so you're comfortable. That the regular workflow is something that the agent is able to take care of and outside the loop is outside the loop, right? So, and I find that very interesting because in highly regulated Industries, I think they kind of move or lean towards the human in the loop model where they are approving, everything not just the exceptions and the norms.
 and,
 I find that fascinating the especially because the
 like, going back to the idea of
 we create these systems so that they can hopefully give us
 Incredible lived, right? But at the end of the day, if we're still in the loop or is it? Is it giving you the list? Is it giving you the lift? And that's probably the big question that I imagine. A lot of these folks are grappling with, like, how much better are we, because we're using this, you know, it's there like in and so you can't say, I don't want to do it because, you know, there are companies that are finding success with it. And I draw this parallel back to like the traditional predictive ML and how
 You have gigantic companies that are optimizing their fraud detection by 0.01% and saving millions of dollars. And so other companies see that and they think, well, yeah, we have to do machine learning because look at that, but then there's all these nuances of like, yeah, are you at that scale though? Are you, is that The Right Use case for you and these questions come into play and maybe you can start with something a little bit.
 More simple.
 Just to see in like put your foot in the water. Test it out, see if it works and and actually see start having the conversations of what we need to do to get that into production. Yeah, yeah. 100%. And in you I I also think that most organizations have started in their back office you know playing around with agents and Ai and all of that and that is given them a lot of comfort because it's a more forgiving place to really start, right? And then we are now starting to see them get into their own core products and services and I think that's the next wave. And this is this is going to get very interesting because now you have to deliver on business outcomes and that means there's going to be business, people who are actually going to take advantage of that natural language processing. And I think this is why Vibe coding is like picking up so much, right? And that's now the latest that we hear about every everywhere because they've made it.
 They as in the technology companies have made it so easy for just about anyone to write a piece of code, because you can say it in English and then you can get cold out of it, right? That's, that's really the whole concept behind it and have you seen?
 a good ways or ways that you can get alignment from
 the two sides of the house is like and I know it's normally not so black and white where it's like the tech side and the business side but maybe where folks are like you're saying business folks are coming into this and they're starting to have their say and they also have needs on how it's being built and they yeah they play with chat GT. So they think like, hey, I should do this or should be like that. Yeah I think it's it's a really good question. And I have seen pockets of this, right? So as I think mentioned, my background is in healthcare and I'll take a healthcare a couple of Health Care examples, right? So one is the function, you know, when you go to a doctor you get approved for the insurance and so that is something called a pre-authorization. That's done for services. The pre-op process, right? Is pretty manually. Intensive like it involves calling the insurance company to make sure you're covered and so on and so forth.
 That is a process that I think has been identified or AI enabled and it, you know, and and so that works, but they still need to make sure that it's something gets denied, you know, by AI. They have a human in the loop to make sure that, you know, this denial, it's appropriate and so on and so forth. So that is an area where, you know, this cannot be done by an engineer alone. This has to, you know, you have to bring in people from and I will call this the business right to come in and say, yes, you know, these are the right policies. This is the right reason for denial and so on and so forth. And when you build it,
 Keeping the business persons requirements in mind and in the Forefront I think you get it right? But if an if you try to do it from the back end with an engineer trying to come up with a solution based on because you see as an engineer, you see the data flows, you see there is a call from this system. There's a that to this system. And this comes back with, you know, this sort of an answer. This is the data flow diagram, that's how I think an engineer's mind works. Whereas a business person looks at it saying, well, a person comes in with a fever and you're making you know asking them if you can get surgery for it like you're going to deny and this is exaggerating again this goes back like the late and failures or the silent failures where it's it's failing but in ways that the engineer isn't going to see it. Exactly. So I think that is where, you know, you've got to take the business input to create these agents and that's where it becomes more successful. And so, the, the extension to that is, you know, Epic Systems is one of the biggest like electronic medical records.
 Systems. Right. That's used by a lot of the big Healthcare Providers. They have recently. The past couple of months they've introduced AI capabilities into their software and I think organizations are still adopting in their check. You know, they are still in that experimenting piloting more where they're trying to figure out if this is something that they want to introduce, but an example is drafting males to patients, right? That is a functionality that they have introduced in it and today people do spend a lot of time, drafting those emails. And what the system is doing is giving them a draft and then, yes, I think you have to change most of them for tone, for things like that. But it's a start that, you know, actually saves a lot of time. I I think honestly, that's a really good example of, you know, usage of AI. Yeah. And well it's actually if you're typing
 In. And this goes to a little bit of like a data engineering type of thing because it I imagine the drafting of the email Isn't High. Your appointment is this day, right? It's a lot of random variables that need to be piped in and so it's not as simple as just having a template that's right? Yeah. Yeah. And this contextualization. Yeah right there's tonality there's so many things that have to be looked into. Yeah. Yeah, totally. So when you're working with companies and and you get alignment and then something's actually in production, one thing that is always fun and as a friend of mine would always joke like
 You can never go wrong. If at the end of any talk that you see like at a Meetup. You ask the question and say, yeah, but how does it scale? And so whenever I think about scale I think about that exact phrase and so I'm gonna ask you know like how do you scale? Yeah you know I'm gonna use that for your next Meetup. Yeah, I'm sure the presenters you gonna be happy because she was telling me and like oh there's always one person that asked that how does this scale? And
 technically it's a good question because yeah you want to know but also oh yeah, right. See I think of scaling into different ways. So one is, you know, today's in today's world the way you go through
 Sdlc for lack of a better word, right? It's not waterfall, it's not agile. But the way agents are being developed as you spend some time thinking about what is the use case or what is a process you want to identify and then or you know, you want to AI enabled, right? And then you think about, you spend a lot of time in design. And then once you have the design ready, the coding really takes very little time. It's the least amount of time. And then the evals and the measurement piece, right? That takes like the next big chunk of time, but once you move something into production, right? Scaling is not a problem. If process a is what was automated or, you know, was was changed then at the same process is being used in multiple places. In the same company, that's an easy way to do it. So coding, right? When you think about scaling, the coding piece is not a challenge because of all the tools that is there. But the second piece is how do you scale if you?
 Got one process, right? How do you get the next 10 processes, right? That is what I think takes more time. So this is where it goes back to having a good handle on your on the processes within your company and standardizing a lot of the processes. If the
 Sorry, if the processes are standardized, then I think scaling becomes a lot more easier, right? And going back to like my example of pre-auth, a pre-auth in a
 In the case of a cold or a cold or a fever is probably going to look very different from a pre-auth, from an oncology department, right? So the process is the same, but the business context is really important, so I think those are considerations right to bake in, but I think if you have a good foundation of processes and if you have business, weighing in the coding, is not the challenge, it is the adoption and we went back to culture, right? The change management piece of it, I think that is where the focus needs to be to scale it. And then the third piece I would add is a measurement piece, right? The Evils are so important when it comes to scaling because you don't want to scale something that is giving you different outputs, then what you want. So, as long as you're able to measure and correct and measure and correct, and you have a fairly good, you know, model in place, then it becomes a lot easier to replicate. I look at emails and I had a friend. Say, man,
 We tried a lot of different tools, but we realized that the tooling isn't the bottleneck or the, what the tools do isn't, where the bottleneck is, what the bottleneck is. A lot of this data labeling on. As you were saying earlier, when the business comes and they look at the interactions, right?
 And they say, wait a minute, why are you giving them a hamburger when they asked for pizza? That's not what they're asking for. And so you have those
 Data labeling issues that take a lot of time because if you're labeling it with humans and your labeling it with humans at the company and you have a lot of data that can just be a lot of work and at the end of the day, that's going to give you the best lift. So he told me.
 You know, you can do these evals and you can get this element as a judge, or LM is a jury all this fancy stuff.
 But what we found works, the best is just getting people into an office.
 Having a pizza party and labeling data together.
 Well, that and also having a good Baseline to start with, right? And that's was the standardization piece. I was talking about, right? If you know what this yielded before it was, you know, identified then you know what the increase in productivity, is if it's working. All that good stuff, right? Yes, there is and this is also where, you know, we talked a little bit about measuring the ROI right and to measuring the ROI is not just a straight. Hey I replaced four humans with four agents. So now I'm going to take out the cost of the humans. It's not that right? Because there is a cost to putting these agents in there is a human in the loop who is reviewing it for eval. So whatever, right? And so there's a cost associated with that, there is a cost of retrieval storage, and all of that in the back end, right? Which you're not accounting for which was not there. When it was just a human doing the work, and you've got, like, compute all of this together to get the true Roi.
 Reminds me of the build verse by question, right? Everybody thinks. Yeah, I'm not gonna buy it because it's too expensive and they're like, I'm just gonna go higher for engineers. Yeah, so help us maintain it. Yeah, exactly, I hope those Engineers are placed in San Francisco. Let's talk for a minute about
 Some of the sustainability considerations that you've thought about.
 I would buckets sustainability into three different buckets and I think some of it we already talked about a little bit. The first one is environmental, right? So that is the energy, use the carbon emissions. What are consumption, right? Are you picking the right sites? Where recycled water is being used? You know, that's sort of thing, a hardware materials like eBay, all that stuff, right? So that's kind of the first chunk of environmental. Then there is the economic cost that we talked about a little, right? So there is the cost of building, the Asian cost of the human reviewing, the agent cost of the storage, the retrieval and
 When you don't need a large model, are you running a smaller model, right? Are you making those selections? Very deliberately, right? And that's what I would put into the economic bucket. And then there is a social bucket, right? Which is hugely important. And we talk about responsible AI, quite a bit. We talked about the ethics associated with it, so that's good. But there's also accessibility localization. You know, transparency explainability of the models, right? There's all those kind of things that I would add into the social bucket. So to make something sustainable and I think this is what I was saying at the beginning. I think we are just getting into that phase where we are starting to think about what is sustainability, how do you measure the ROI so far? It's all been about. I'm so excited about Ai and the possibilities of AI. Where do I start now? People have started, they've deployed a few things and it's like now how do I scale? But hey, wait a second. How do I sustain this?
 So I think these are all going to come in, you know, in into the picture a lot more. I think over the next three to six months or I'm saying three to six months, I would have said three to six years. If you talk to me a decade back. Yes.
 Yes, I know who knows? I do like how you are facing this? You're kind of abstracting it away from the
 weeds of the tech and really trying to think a bit more big picture and say, look if we want to
 Sustain. These efforts.
 There's certain things that we're going to need to invest in, right? And none of those things are like this new hot Tech that or framework that just came out. That's not the big picture type of thing, right? We need to look at the big picture here is, how are we going to make sure that we can justify this? Yeah, that's, that's 100% it because we are gonna get there and you know, think about all the ways that we've been through over the past couple of decades, right better. It's the internet where that it is the cloud, right? Moving to the cloud, he'd all came back to like, even you think about the cloud, the cio's came back to wait. I'm gonna spend a lot of money if I just keep storing all my data in the cloud is that a more optimized way of storing the data in the cloud. Right. It kind of and I think that's the next wave with AI as well. Yeah, I've already heard folks talking about how
 They're starting to look at the cost and they're starting to think if we do deploy this to production. As soon as you start thinking Enterprise scale for that feature,
 Now.
 You're hurting every time, the user says please. Yeah, I think efficiency and Perfection has a price and I think everybody or every company has to decide what does that right? Trade off right where you want to spend so much money for just so much perfect and, you know, everything doesn't have to be 100%. I'm sure you're seeing it on the different use cases, where there's more appetite for risk or there's less appetite for risk or there's more appetite to get this into production just because we really need something here. Yeah, but on this side, maybe the process is they're not fully dialed in and so we can take a little bit longer. That's right. Yeah. And it's I think the same call or a similar call that a lot of organizations make even in the Dr. Space. Right. How much do I need? And how much is good enough? Right, you can keep building on that. And those are massive and Investments and I think everywhere. It's
 Like the same principles that you've got to apply. I think the skill.
 Skill set.
 In the skills that you need both to build and maintain, right? From a, from a software engineering lens. I think that's evolved so much right? The it as you're saying, right? The low code, no code IDs,
 It almost is like you don't need a full stack developer as much as you needed them. I'm not saying you don't need them at all but you don't need them as much as you need. Yes, but you need them for other stuff. If you're looking at the whole product, as a circle, you need them for this part of the circle, and then you can try and get the the other stakeholders involved in this part. And right, this was always kind of the case, I don't know. If you remember it with the ml, you needed stakeholders involved but they couldn't really Act.
 They had to be in the meetings and talk about things and say like yeah but we can't do that because of this or that. But yeah you didn't have this idea of well, let's just Empower you with a no-code solution. That's right. Yeah. And that's what I think it's becoming very interesting, right? So the skills that you need on the build side is is less coding and more designing. And once you design the right solution, it's very little time to to code. And then you spend a lot of time in evals and measurement and all that. So prompt engineering, you know, data scientists who can measure compare run models simulations. That comes into the picture and then on the man at Services side it is again data scientists who continue the models, the skill set is completely shifting, right? It's not about coding anymore. You know, when you are deploying agents which is very interesting, right? Because that
 Impacts, I think the skill set of who you hire, you know, as, as you're like, beginner kind of workers to how they grow, what path do they take to grow right within the organization and I think the business knowledge is becoming more and more relevant, you know, now you want, you know, yesterday, I think when I was at your event, I heard about how product owners can start coding because you don't write need to know code. So, you know, the product owners who understand the functionality, who have been designing, the product are probably the luckiest right in in this whole mix because now they don't need to rely on an engineer to come out with a cold. They can actually code, right? And it's oversimplifying it obviously, but I think I do see that shift. Exactly. I have been
 Hearing the term product engineer, more and more. And then I also saw the trend of the CPO CTO, where they're the same person, and so it's like, yeah. This is, It's a product person, but they're Technical. And that's the new thing to do. And what I do think that the
 The hard part? Yeah isn't maybe it's not in the code. The hard part is in the
 making sure that if
 You have an agent, it has the right context and so there's still a lot of data engineering that's going on in the background because the getting that context that's all moving data around and who's really good at that data Engineers. That's right. That's right. Yeah, so it's interesting how it's all shifting and it's also, I think, you know, if you raise it up to the c-suite, right? That is the birth of the chief AI officer, right? We have the CIO, we had the analytics and digital and then now we have ai and I think with AI, right? It's shifting more towards the business, it's more of someone who understands the business, who can then advise the organization on what are the right AI Investments that's going to. Then impact, you know, the companies productivity and efficiencies and all that good stuff. Well, wrap up here. There's a few really cool ideas that I'm taking away from this conversation, one being
 the idea of
 Thinking about your system from this sustainability. First, I always thought about sustainability and the other bucket that you put it in which was like, yeah, the hardware. Yeah. Try and reduce reuse. Recycle sustaining environmental concentrations? Yeah. This is like, we want this AI product to have a long shelf life. So how can we
 Design with that goal in mind, right? And that's probably like the biggest thing that I'm going to be taking away. Its it is very
 Oversimplified? Because? Yeah, of course you would think like that. Why wouldn't you? But I yeah, how much, and it's also easier said than done, right? Right. As anyone knows you're working in on rag today. And then,
 You're working on agents tomorrow and next time it's prompt engineering or its Context Engineering and so yeah.
 yeah, you know if I had to talk about future ready
 It operations, right? It is really
 Look at it with a business mindset, right? There's I I think as we look into the future like making sure your stabilizing, your current state is so important with data modernization, app modernization, process standardization. All those good things, right? They're all I think very important for a large organization to start with. And and I also think that, you know, if
 If a company has never dabbled in AI before start with your back office. Like I said it's more forgiving and and then kind of move slowly into your front office as you build that muscle right within the organization invest in good training and upskilling right for your people sort of thing. Because
 It's not you, you're not going to replace people like overnight, you're gonna upskill them, you're gonna Elevate them, you're gonna still have a human in the loop and educate them so that it doesn't become a culture of fear but it's a culture of learning and then, you know, there's I would say, you know, start pilots and different departments. So everyone starts getting excited but with that kind of centralized governance if you will. Right? So that there's some guardrails around everything and take advantage of all the cool technologies that everyone's putting out there. Like the number of AI startups that out there is just mind-boggling and the names. Right. So interesting. Yeah so fun and you know like like Asian tourists that PWC has a lot of Frameworks out there, take advantage of those Frameworks because a lot of thought has gone into building those Frameworks just to make this journey easy. And then last but not least, I was a measure measurement measure, right, that feedback.
 Loop is just so important.