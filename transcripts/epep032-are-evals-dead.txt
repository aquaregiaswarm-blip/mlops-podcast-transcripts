Why do you even need evils in the first place, right? We started to see where things were failing and started to build evils for that. Specifically, you know, it's easy to say, let's just test the agent and twenty. What does Antoine mean in the times of machine learning, we were training any model and you roll it out, you know? Now it's coming to agents because agents are being production is
 How are you? Evaluating how has that changed over time? We learn
 Lots of the past years at process. We've been building many agents for many applications and for the lesson, we quickly learned that you need to have a way to understand how your agent is doing. Like now it's very easy to wipe code something and get it in production, but that doesn't scale. If you want to release this to millions of users, you will find that many times. The break and
 this is also the reason why people don't trust agents 100%, but we're here to change that the way we approach it is also from kind of scientific approach because I'm a data scientist Myself by background and
 like it's easy to use models with an API, just run a prompt and get a response. But actually, what you write in this prompt can change a lot of the results. And the way I see it is as much more like a data science, kind of thoughts, where when you write a prompt, your kind of training a model because you're determining the outputs of the agent, right?
 So, it's very important to have good evaluation sets.
 And at the beginning, you don't have really production data. So you need to kind of bootstrap this whole thing and it was very important to have a very creative set of examples. For instance this could come from product manager insights discussion in the team or maybe this is a specific use case that you want the agent to work on and and there we build all these examples. So at the beginning to bootstrap this this agent we created a curated set of test cases, we approach it like software engineering project. In this case, we wanted to have some flows that we were sure that we're gonna work those flows you can use in cicd pipelines and everything like more more like you would do for a softer development project. Yeah. And there you you make sure you like the main use cases are covered but you also might want to
 See all the parts that need testing and make sure that nothing breaks along the pipeline. That was the first level that we did. So understand what goes wrong, where fix it like, iterate fast? And then we started to test the system ourselves like on a wider, whether team within the company and getting feedback from users. I was very specific feedback because we had internal users that were helping us with this.
 And with that, we started to see where things were failing and started to build evils for that. Specifically, something we did was also building it was for multi-term conversations. Like, for guardrails, this is all the obvious use case.
 Because when you have single turned conversations, it's very hard to to see if the guardrail can be broken or not. Sometimes you need the user who's very persistent. I'm not talking really bad stuff because that is usually covered if you're using Foundation models.
 But there are things like platforms to Convention that is a bit more sneaky like trying to use their front to get information. That is not supposed to be public, you know. There is a lot of things. Yeah, that we were users trying to get discounts, I imagine. Oh, yeah. You know, once you release it, people are gonna try the worst or the best. So yeah. Wait and can you break down this multi-turn conversation, evals and how you were basically. It was for guardrails but you also want to evaluate that it was in giving discounts when it needed to be or it wasn't telling secret information that it shouldn't be. Did you basically set up another agent to try and read team? It were you specifically trying to read team at how did that work? So we created personas
 There is um yeah we created an agent that was super simple user who had certain intention and we were defining this at the beginning.
 And the agent had to be very persistent to try and break our own agent. So for instance, yeah no matter what the other agent responds, try to get this information. Try to be persuasive and Swan. Nice. And sometimes it was working. And yeah, that's where, you know, you need to, to do something about it. How are you updating that? You realize something went wrong, you realize that the agent gave away information. It didn't need to. What did you then do to you just changed the prompts? Or did you also add specific Like hard-coded rules? Yeah, it depends on the
 Sometimes.
 Sometimes you might have security.
 Surface at the time. And then you need to to do changes in the actual code, sometimes it's just prompting being a bit more aggressive or you cannot reviewer, that's evaluates the output and checks, you know, sort of content moderation step before the response.
 So there are different techniques, you know, always start simple and then build up. If it doesn't work, you did talk to me a bit about the simulations that you were doing because I think that kind of is in line with what you're talking about. Now, you are simulating in different ways like can you go into that? So the simulation that we did were kind of multi-term with a predefined number.
 Of turns. So we didn't drag them for very long but just wanted to see whether the agent could break our agent in like four or five turns of conversation, what will? And we had few phases to Define this. Like first, we came up with examples ourselves because we wanted to. Yeah, just make sure Basics are our covert. Yeah. Then we started to get feedback, and then after this phase, you can move to production data and see like there. Maybe we can move to other parts of evils, which is equally important, which is at a analysis. So there you really want to pinpoint, what's wrong. And, and this, to your Revelation set,
 So for instance, if we see a case where user managed to get some information from the agent, then you added to this set and you make sure you run it every time you're updating the code for biologically, depending on frequently needed. One thing that I'm fascinated by is the evals your kind of doing offline, right? But then you have
 Prompt changes that you roll out and those are almost like online. Do you think about how to update the production system with the new prompts but almost like do it in a champion Challenger Way? Or is it done slowly or do you just change it and see where things break afterwards?
 if you understand,
 It and like is it. It's about basically measuring the impact of the change on the in the Productions. Yeah. Basically and and especially because you change the prompt. How are you rolling it out? Yeah. This is the same as rolling out a new model. Like in the times of machine learning, we were training any model and you roll it out.
 So first of all, you want to see how it compares to the previous. So you have an evil set to run it and you see if there are discrepancies, if it's better. If there are some some tests that now fail and didn't before, like if you introduce new errors this is really important to first of all, on a trusted evil said you run it there and then you can start to roll it out so you can do an AP test. For instance, there are a lot of literature about this.
 Um because yeah, this is what data science Community has been doing a lot before. And, you know, now it's coming to agents because agents are
 you know, you've been production honest.
 So now all these little things are starting to connect. Finally it was I didn't see this as much before but now yeah it's really nice that we can use all this technique is important in the world of Agents. Maybe that's a nice thread to pull on is what
 Are some things that you can take from the traditional predictive machine learning world, like those eval sets that you testing offline and then the slow roll outs are the championship a-b testing. When you do roll out the model. But now we're not rolling out a new model. We're rolling out new prompts
 What are some other things that you're taking or what are some things that are completely different? Imagine you have an input of a black box which is your model your prompt and then you have the output so that part is to be tested. Well so that I approached like data science problem more than traditional machine learning cents.
 Even though it's there is a more there is a bit more explainability there, we can make easier changes in the prompts but still that that part is very analogous. We've seen. The other is the fact that you're integrating this into a big software engineering project that is more complex than just getting an output from a model. So for instance, tools tool calling is really important to get it right? Sometimes tools are whole workflows by themselves, like who can be almost an entire agent or sometimes agents? So
 there is much more complexity in an agent system.
 So many more things that can go wrong. And if you start to, for instance, change API calls, where you have always probability that things, might go wrong that you're gonna get the wrong output. So it's really important to find out what the steps are and test everything.
 It's yeah, you know is it to say let's just test the agent and to end, what does Antoine mean? There are all these components and you really need to understand what goes wrong.
 It's also at the end. Why do you even need evils in the first place, right? You want to see what's wrong? What to improve is the product good enough to be released
 I think for if you're a software engineering team, you want to know what to do next? Yeah, it's the most important you want to iterate fast, get output and release new version quickly.
 So you need it was to understand where things go wrong, what are the errors. Why does certain things happen? Having agents use other agents as tools? Those agents now are they can fail and maybe you don't really recognize where that happens. So debugging becomes very complex. It's the core of evaluating agents because the end users care about the endpoint experience, right? We want to give a good experience to the user. So end-to-end is really important. So there is a level in which you want to test like how things go like how what's the experience for the user itself? Yeah.
 So that's very important, it's not the only part of it. Like it's very important to also dig deep and see for every step. What can go wrong? This is more similar to traditional software engineering where you have unit tests. You know, for functions you want to we want to have granular visibility into what can go wrong. We say we have the end-to-end experience where you can. For instance, say this is what it would you use their types and this is what the agent responds. Like the agent is doing web search. These are the search results things like that or the user is searching for gluten free food. Did the Asian return gluten free food or not? This is this is something important to check, right? Because there are allergies, in case of food. So to give you some ideas, um, but then, yeah, you check the user experience is fine. But then from the developers side, if I want to know what to fix then I need to take all these two calls. See
 What goes wrong and where and I think it's very important to to use the right tools for that tool. So we find many times, this project have complex workflows and traditional platforms for evolves, are sometimes not enough like to visualize data and outputs. We sometimes had to build our own tooling. Oh interesting, tell me more about that.
 For instance, to evaluate conversations in order for people who are non-technical to understand and see if something was good enough or not, we needed a good way to visualize this. And, you know, you have agents in tools, most Frameworks, allow you to visualize these things. But sometimes in the tool, you also have other things going on that are very domain specific. So we had to build ups for for them to to visualize conversations and I think this stuff is sometimes overlooked, but it's very important because you really need domain experts. You need PMS to look at this data because there are guiding the development of the product itself. So they really need to understand what is going on, and it needs to be easy for them. I heard about a friend who actually Willem who came on the podcast, probably three months ago, and he was talking about how they built heat maps for where, the agents were failing. And once they build these heat Maps, they could
 See very clearly what the agent was good at, and what it was not so good at. Yeah, I think this is the, this is the core of evils, like, understanding where things go wrong. We had it at the beginning. We had simple prompts to evaluate the conversations which were like is the agent returning. A good response is the tone upgrades. Like all the things. This is what you love him as a judge. Yeah, yeah, this is for lemons Dodge.
 And it's very easy for the for the judge to think that everything goes well, but you really need to to instruct them to look at the errors. Like, find out what is wrong because they don't have this capabilities but themselves like they're very eager to please you. When you show them data usually.
 Talking about various Foundation models here that you can use to evaluate, but what you want is not dead. So yeah, this is a very cyclical process. You need to keep people in the loop. You need to pull people from Technical and products sides in the loop because you have evaluation sets what you evaluate. But also how what is the quality of the evaluation output?
 Because I think it's the worst to get wrong outputs to not get them at all. Okay? You think everything goes well and then it doesn't. Of course, you don't want that. Yeah, those silent failures. Is something that I want to ask you about to because
 A lot of times maybe everything seems like on the surface it when well. But when you dig in and you look at that user experience you go. Wow this was a horrible user experience. Yeah I think nothing beats using the app and testing it and being your own beta tester. Basically yes we I think dog footing is one of the most important things to do. If you're developing a product, you need to use it. You need other people to use it. You need your grandma to use it, too, it's really
 Think about? Yeah, it's really important to see the flow because often these apps involved by elements. It's not just the agent itself.
 It's it's easy to sit in our corner and just evaluate the agent like she's in the tool. Well, yeah. But what if the user gets? A completed? A wrong experience. Yeah, so then yeah, there is a specialty. The beginning of this project, there is a lot of information sharing and yeah the involving multiple people in this. Well, there is a fascinating thing here, too. When it comes to the
 Evaluations that you were talking about and the visualization.
 That you were mentioning, you want to be able to visualize it in ways. So that non-technical people can also get an idea of what's happening. I just mentioned heat maps with my buddy Williams. Did you have other ways of
 Changing that data to visualize it for the others that you found particularly helpful. Yeah, I think distributions are really important.
 Sometimes you don't know what is going to encounter. So it's so important to keep things freeform, like, have the agents like summarize, what went wrong simply and then aggregate everything. Later is very important because you can think, okay, what are the category of pharaohs that I can get? What is the taxonomy? You can have a tool error? You can have I don't know timeout error. Like there are many you can start writing them all.
 But at the end you have if you gather data then you can just use that right? I mean these things are not mutually exclusive of course yes yeah so that's a
 Interesting way of putting it is.
 Why not? Just let the agent, tell you what went wrong and summarize what we're wrong and then you can have that as another data point. Exactly? Yeah, yeah.
 I imagine there's no shortage of data.
 For this. So,
 In a way you can always get more data, you can always ask the agent for more data and say what went wrong or show. Then you can also look at the traces and logs from different tools, you can look at the eval sets and if it's failing any of those, yeah.
 They're like a 80/20 here that we if you're going into it, you're saying these I'm 100% need to have is data points.
 And they need to be this granular or this sophisticated before, I feel confident and pushing my agent out to production.
 yeah, that's
 some it because it's really hard, especially at the beginning to know how this is gonna. Oh, this is gonna look right. Like,
 If you don't have production data, you're kind of imagining how things are gonna work until you are allowed to user? Yeah you don't you don't have this information even like you can think these are all the use cases. I want to cover, okay, 99% are covered will release or maybe 50% or more comfortable.
 Um but at the end you need really user data. So I would frame it the opposite way. Like I think that the goal is really get feedback as soon as possible and get and get this out. And if you release to some users, for instance, you will start to to have conversation logs and then you can really see where all the failures are.
 So there are some that you might want to accept some that you might not want. In the case of search there is always precision and recall. Problem for instance, in like with information about food, you want to be precise because if someone else certain that her restriction don't want to give them wrong information.
 So it really depends on the project. You really need to sit down and with people from engineering products and find what these things are. Yeah, maybe we haven't talked about it, so yeah. But the finding what metrics are comfortable with is really important at the beginning.
 So okay, to have an idea of what is good enough, right? Will I tolerate 50% of the searches going wrong? Like what is, what is the tolerance level for errors? What do you accept?
 Yeah, again it goes back to this idea of like there are Hardline things that we for sure. Cannot do like, giving free food away or telling a vegan that this food is vegan, but it's not, it's actually meet. Those are things that you're gonna piss, some people off very quickly with
 And so you recognize along these lines. We need very, very low margin of error but over here
 We can be a little bit more.
 Yeah, generous with our margins of error and I mean this is AI we need to embrace it right? It's not
 Like use it also expects the agent to be conversational, so they might get something wrong and the recover and and then give the right result at the end, you know, there can be some hiccups along the way, especially at the beginning.
 But I think with new AI products, people are a bit more comfortable with that by now, of course, then you want to improve it but you cannot do that without enough data. So it's a bit. You need to be comfortable with this uncertainty and leverage it, like use the feedback and build on top.
 Is there anything that you are?
 Particularly surprised about that. You've learned along the Journey of building these agents, putting them in the production that you want to share right now, or that you can share with us that if you were going back and you were building this on day one,
 You would focus a lot more attention on.
 I think in general how complex, this systems can be
 Like how many moving parts that can go wrong? I think.
 At the beginning, it's easy to underestimate like you, you're testing yourself a couple of use cases, it works and then you're good. Not talking about this agent in particular, like in general, my experience building agents,
 But then yeah, you release it to use users and they will find ways to Multiverse you're someone building agent. So your mindset is completely different.
 So for me, the most important learning point was to really trust users and get feedback from them, trust users to break the thing. Yeah, like get get people that you're that, with your opinion, you value and, and asked them to test. Yeah. And how are you reincorporate that back into the product? One of the things you can do is to get a channel for feedback and have people posing feedback and then Gathering all this feedback aggregating it and and defining a roadmap based on that.
 Like, what are the main issues to fix? You know, you can use llms in every step here?
 Yeah, you can have an agent reviewing feedback and prompting you all things to improve when you're doing these different kinds of visualizations and looking at the distribution and all of that fun stuff and you find certain things that are out of whack.
 You're then just reincorporate that into your eval set or I guess. Where is Step One?
 Yeah, it's both. It's like when you find another in an application and you fix it like you find the bug fix it. Usually good practice to add a unit test for that if you can. So I think having a curated set of examples and edge cases also is really valuable here. So when you find something that is a bit off and it's good idea to include it and depending on the severity then you can you can fix it like this is more of a product problem. Then are you comfortable with that? Yes, or no?
 I think that's good. Is there anything else you want to hit on? If you want to work on good projects. Like this, we're hiring at process. So feel free to messages and there's me and yeah, let's keep building. Great stuff. Nice.