People talk a lot about system. Prompt tweaking The Prompt or choosing different model. What's the best model out there? One of our hard-earned lessons is I spend so much time on Context, Engineering with the teams. Let me explain what context means, right?
 So in my role, I build AI agents in these different Industries. And these agents are kind of used by 2 billion customers around the world. Talk to me about
 what agents you've been building. So in general, there are two classes of agents that have been working on one is agents for productivity so internally we build a tool called tokan you know about it. This tool is used by 15,000 users internally at process and these are people from finance and designers product managers ingenious and people use it for different things. So we started building this tool, three years back. The technology has evolved so much initially. We did a lot of things to make the llm work so that, you know, there was an intent detection at the beginning and so on it blows my mind that you know now you don't need like the model itself is so better. That you don't need these things. So sorry. So one one category is a tools for agents for productivity. So tokan second is agents for e-commerce and there. I have experience in building agents for online shopping assistant. So OLX is one of our portfolio companies. So with OLX, we build a shopping assistant, the idea is that it helps understand the user intent. So you can say that you know I want
 The latest headphone or, you know, I I'm going for a hiking trip and I don't know what to buy help me. So it can take very broad requests from the user, understand connect to the OLX catalog.
 So that's an example and currently I'm working on a project with enough food delivery business where
 We are kind of reimagining how people would order food in next.
 Year or two years where, you know, things like not. So usually, if you think about how people order food, right now, they go on the search bar, you enter burger, right? But how about it also, you know, you can go in, you can say that, you know, I want to have a romantic dinner with my wife and then it understands you, it connects to the catalog and so on. Okay, so we've got four
 Themes that we want to touch on? Yeah.
 First one, which is on everyone's mind, is the context and Engineering piece. You've got some takes on that. Yeah, totally. So Andrea recently published a tweet where he said that he likes the word Context Engineering more than prompt engineering. And, you know, I was reading it and in my experience in building these agents, I spent so much time together with the engineering teams working on the context, so I was completely as soon as the moment, he published the post. I think the context in generating,
 Up what got viral. And when I think about Context Engineering, it reminds me of the day with you know, when you know, data engineering data scientist you know I was a Hands-On data scientist at some point in my career and you know, we always said that, you know, data science or AI is the shiny thing. But you know, if your garbage and garbage out, if you don't work on your data Context, Engineering for me is like that. We are doing a podcast. I see people talking on Twitter on different podcasts and people talk a lot about.
 System prompt tweaking, The Prompt or choosing different model. What's the best model out there recently about mCP tools and so on. But for example, you know, if you take about the latest models, if you think about the latest models, for my use case, it's not, it's not really about that model. If I use model A versus model B, and if both of them are state of the art, they already do a good job. What makes a difference between A and B is
 A's with context B is without context. Then a would be much better than me. And let me explain what context means, right? So a lot of times, so imagine a food delivery chatbot
 Aside from people, asking for food, when you put a chatbot in, wild people ask about many different things, right? Imagine, you know, you want to order food their food, that's what I instantly. Exactly, exactly. So people care, different people care about different things. So there are people who care a lot about free food, cheap food, promotions. There are people who care about maybe you have a particular coupon or payment card. So, lot of time people come to a chatbot. They say that. Okay, do you accept this payment card? Show me. Show me sushi on promotions right now or maybe it's lunch time and or let them maybe it's breakfast time and I am asking for pizza and none of the pizza places are open. And you know, you don't know about my use case as a user. Maybe I'm someone I'm an executive assistant. I'm planning already for lunch or maybe I want to have pizza. So in this case, for example, the opening and closing are of a restaurant becomes the context, so if you're llm or if your agent doesn't know about that. Then you know, we have tried
 This before, if you're LL, if your agent is just good at searching for for pizza and it doesn't know about the opening closing, our it doesn't know about the promotion, it doesn't know about the payment information.
 If you put it in real world, it's not useful for people. Because, you know, I care about a lot of these things users care about a lot of these things. So, one of our hard-earned lessons, I spend so much time on Context Engineering with the team. So, you know, I cannot stop talking about it and I don't wanna brush over. There's a few different places where it's a very difficult problem that you were just saying,
 When it comes to ordering the food and the promotions for example, and you were saying that it's not like there's a database that you can query that has all the most up-to-date promotions. Yeah. So the data challenge in real time, knowing what promotions are live from what companies and then feeding that into the context window is where the real challenge is. And so it's almost I look at it a little bit like
 when you're crafting that prompt, you put in variables,
 And getting the proper data into those variables, is the unsexy job of the data engineer. Or it's it's more data engineering like than anything but now I guess we're calling it Context Engineering. I think we all agree that you know, most of the Enterprise data, you know, it might look good from outside, but it's messy. So, unfortunately, you know, if you go in an Enterprise, it's it's not like there's a database where you can say, just give me all everything on promotions. A lot of data is real time, maybe a restaurant, is running a real-time promotion during lunch from 12 to 3 on sushi, right? It's very difficult for a database to have that. So a lot of that information is in real-time databases, it's scattered over different databases. So the unsexy work here is that, you know, of data engineers in the team, engineers in the team who spend a lot of time, kind of connecting all this. So, imagine now user request comes in and they say, show me sushi on promotion,
 you kick in these data pipelines, you bring the right context, you make it a part of the prompt and then the response that you get is what the response of the user wants to hear and there's not a way to do this with search. That is more simple.
 Know. So search is the next topic I want to talk about but search solves a different problem. Search is more about for me searches about when you're when you're searching for dishes or when you're searching for restaurants. That's what that's a burger. Vegetarian burger pizza or McDonald's but when you're searching for does McDonald's, have promotion. Does McDonald's. Accept payment card does McDonald is McDonald's open right now that is context. So that's the difference for me between search and context is made up of four things. In my opinions, I spoke about one of them. There's another one, which I want to cover. So one of them being the dirty data, getting it the right contacts win. Yeah, it's composed of four things, two of them are simple. Everyone talks about it system prompt user message and I think we have heard enough about, you know, you need to have the best system prompt user messages. The dynamic message is the users and so you put that in the prompt as well.
 We spoke already spoke about, you know, bringing the Enterprise context, the dirty data Pipeline and the fourth context which is very important is user history. And this is where, for example, it's connected to memory. So you know, there's been a lot of discussion on long-term memory short-term memory.
 the way I think about is,
 There are so many agents products out there. Imagine, you know, everyone is building a shopping assistant. Everyone is building a food ordering assistant.
 If even I think about me as a user, I don't have a loyalty for a particular product, I'm using charge GPT. Today I'm using another product next day.
 For me, something that creates taking us to a product is if that product knows about me.
 So, if I'm using chat GPT as an example, for last 15 days, and now you give me a new tool, I would use it.
 Unless chat Deputy knows so much about me that. Okay? I like cast this higher. The switching cost is high and that comes from memory and that is also part of context where you put the user history as part of context. Yes, it's funny. You mention that because somebody said when they were joining the community, they were very interested in the problem set of being able to, or that memory and that context from one provider to the next. Yeah, that's amazing.
 If you do that then then you could switch, right? Because if you do that but it's not like you can just hit like, download CSV and upload it to the next thing. The lot of data privacy issue with that because it contains a lot of information about you which you might want to share or not.
 and again, one last point, there is
 So I'm sure you have seen these diagrams that people share about memory, long-term memory short-term memory, you know, episodic memory and so on. So memory can be handled in many different ways. It's almost like you know, when you're having a conversation
 Maybe there's a model, which is running behind the scene. It's noting facts and that's a part of memory. But to be honest, one of the things that we tried which is much simpler than any of this, and it works is
 Let's say you're building a shopping assistant or a food ordering board.
 And you already have a product, right? So there's already OLX or iFood so there's already users who are on OLX, who did not have conversation yet? Who are on a food, you've got that data, you got the data, you already know what food, they ordered you already know what items they browsed you can easily use that as a simple cold start context. And I have seen that create wonders where, you know, when you launch your product, you don't need to be, you know, my advice here would be when you launch your product because there's already so many things to solve and you know, your obsession should be about product Market fit and you know, not having the best technical solution out there.
 You should just use the context that you have from your existing app and put that in the memory put that in, you know, the context or the system prompt and that already does wonder that will give you a runtime of three months. And then you start, you know, when people start having conversation more, they start using your product more than you get some dynamic memory. But I think that's one of the lesson that I've learned where I think the way people think about it, already from day one, it's complicated. There's a simple solution for two, cool stuff. Okay. So that is Context Engineering, yeah, that is context engineer, that's it search search.
 Searches search, you know?
 So, my experience has been building AI agents and e-commerce shopping food.
 and,
 Search is a fundamental tool when it comes to that, it doesn't apply for all the agents out there. Let's say if you're building an agent for suppliers side, you know, for for a car dealer or, you know, for a restaurant, so it's not that important. But when you're building an e-commerce agent searches, the most fundamental thing, it's actually the start of the user Journey if you don't get the search right, let's say if I search for Burger and if I get pizza
 My eye drop at that point you know. It breaks the trust. I will. Maybe you have other tools to manage my cart you to do other things, but I will never go further in the journey. So search is very important and I spend a lot of time with the engineering team on, you know, fixing search. Let me talk about a few things that, you know, we've learned. So most of the Enterprise search is still keyword based and, you know, not to put it down, it's for the right reason because keyword, based works, if I say Burger, there's a tax Noemi defined for burger. And you know, show me Burger, you don't need fancy semantic search to do that. So keyword based works.
 But if you imagine people searching on a search bar versus people talking to an agent, the chatbot you put out there, they kind of conversation and special. When you have voice enabled, the way people Express themselves to an agent to a chatbot is so different. The kind of queries are so different. I would go and I would say that, you know, already give these examples, right? You know, I want to have a romantic dinner with my wife presenter, very broad. Or if I'm building a shopping assistant, we have seen people just go and say that, I'm going for a hiking trip, I am a beginner, I don't know what to buy, help me or help me finish my house. These are the kind of queries that you get and you cannot deliver these queries with keyword search.
 So, we spent a lot of time thinking about first, semantic search and semantic. Search is not new. It has been around. It's a search based on embeddings. So you, you know, you know, for the search Engineers out there. So you, they're already standard way of doing these things semantic. Search is hard, it's still hard to crack and keyword. Search is still important. So, most of the time, the solution is something hybrid. Where, you know, you get a query, you look, if a keyword search can answer and can answer it, if not, you go to semantics search,
 So it's not new but it's difficult and we spend a lot of time talking about it, so let me explain that with an example.
 when I say some vegetarian, when I say vegetarian pizza,
 If I have keyword search, then I would get items that have vegetarian, pizza mentioned either in their title or description, but Pizza Margherita. That's vegetarian. Maybe, you know, it's not obvious to mention vegetarian in the title of maybe the, the restaurant person did not mention it and then all of us are greater will not. So this is keyword search. This is the limitation keywords. Now let's say you move to semantics search.
 This can be solved with semantic search because then you have embeddings and then Pizza, Margherita embedding is very close to the embedding of vegetarian. It's all
 Now, let's say come to the example that I say, romantic dinner that itself that cannot be solved. So, romantic, you know, what is the embedding for romantic? Romantic is different for you, romantic is different for me. That cannot be solved by Symantec, and we see a lot of these queries, which are broad, which are ambiguous.
 Very fuzzy. So this is where again, you know, semantic search is the first layer, but we spend a lot of time building a pipeline of search. Where, when these queries come in, you have a step before search. And you have a step after search the step before search, we call it query, understanding query, personalization query expansion, Call It Whatever. Where, again, you use an llm. So, you already have an alum for the agent, but let's say now you have searched as one of the tools within the tool. You have a pipeline where the first step of the pipeline is already using an llm to understand the query. So romantic dinner, it understands. And maybe if it has my user profile, let's say this is what romantic means for Nishi or romantic in general and Anime says that. Okay, let's break it down into. I don't know. Cupcake is romantic, and so, on handle it. Yeah. Okay.
 Oh, you have different definitions of romance apparently.
 Ways that I would describe romantic dinner. Cupcake was not one of the. I hope my wife is not listening to this, but yeah, guilty as charged so anyway. Yeah, so there's a query, there's a query understanding step which breaks down your query, maybe into multiple queries, then you run it through your search pipeline, keyword search semantics search. Then there's a
 Re-ranking step where, again, you use an llm where, you know, initially you get a lot of candidates. Then this is, you know, re-ranking is again, re-ranking is an old concept from the machine learning world, where we had these algorithms LTR learning to rank and so on, but in the new world of llm. So those algorithms are still
 Important but they also in my experience, I've seen them fail. When it comes to these new kind of queries where you have a lot of contacts from the user and there we you can have another re-ranking layer where again you use an alarm. You say that okay this was equally from the user. These are the 1000 responses from the first two steps. This is a context about the user re-rank, you know, what is, and then you get these three, or these 10 options that you present to the user. So thats what are typical pipeline looks like,
 And, you know, again, I cannot stress it enough, it sounds simple. But search is something that has, you know, haunted me in each of my projects, it's difficult. It's difficult, it's messy. It feels like a new paradigm of search too. Or just like a building on what was already there and trying to leverage the new technology of llms. Yeah, totally totally. Now with the all the advancements in llm generative, AI agents, like agentic search and llm helping in search and llm augmenting search. So you know not replacing. But semantic, search is still important. But, you know, doing some steps things before things after, so that's the new paradigm. And so you write like search is really being
 reinvented with alums and I think I think very few people talk about it but if you're building an e-commerce agent so just fundamental and it's very important to crack that.
 let's talk to you, I
 Let's talk you very first, talk you, okay, so again, I have launched, many agents, many journey, I experiences. And typically, the way we launched them, is we a B test and
 I have burned my hands so many times. You know, it's again, it's it's there's a lot of pain there that I'll give you an example. You know, we want to build online shopping assistant.
 The first thing we do is, you know, let's build a charge GPT for everything, right? So let's build a conversational experience. We build that we
 Tested internally, I tested it. I spent a lot of time you know long hours at night. It's super interesting. It knows you it's connected to the catalog. You can you say that? You know I want to furnish my house. It it shows you Furniture, it shows you, you know, chairs so far separate sections and so on. It's beautiful, it works. You're so excited that you are so benevolent when you're using it, right? Yeah. You aren't thinking like how can I prompt this to give me free furniture? Yeah. How can I prompt this to get the right Furniture. You also know what prompting is so you probably yeah, explain it differently. Totally. So already seeing how this is going to fail. This. Yeah, totally. So we are so excited. I'm so excited. We launched it. We think we build the best product out there. We launched it. We tested our tests, like it really Falls flat on her face. We are like, really like, I'm refreshing the results. And it's, it's, it's, it's terrible, it's not even, you know, it's not even bad. It's terrible.
 Then you think, you know, what's terrible? Because nobody's using it or know that people are using it, but they don't like it.
 And they're giving you that feedback right away or you don't immediately get feedback but you can mount something, you can measure the conversion numbers. So A and B. So it does the magic of a b testing, right? So you have a conversion number on a, you have a conversion number of on B and you can see the conversion number is broken on this. Yeah. You look at the yeah. You look at the agent and everything is amazing, it's still amazing. So why does it not work? So, and this has happened, you know? Again, you know, I'm, you know, talking about failures. It's also important to, you know, Embrace failures and this has happened many times.
 And you know my learning here and you know again, I'm a technologist, you know, I'm not a designer, I'm not a user researcher. So I also learned it the hard way is
 the way I think about is, it is
 so there are two Dimensions one is technology. Second is user adoption.
 Technology. Like, I feel that.
 Technology today is more advanced than the use cases. It was not always like that. So four years four years back. I tried to do a project where I felt that I'm so ambitious with my idea but the technology is not ready these days whenever I try to do a project, I feel Technologies, you know, few steps ahead. How can I use it? That's the problem. So that's technology. But user adoption is not going at the same Pace as technology.
 Think about it, we all use charge GPT, you know, charge. Deputy it's been three years now and I think it's been internalized. People are now understanding, we're learning new ways to use it every day. We're practicing. Oh, this actually, I should ask chat GPT instead of just doing what I normally would do like ask a friend or yeah a doctor. Yeah and you and me we are probably maybe biased sample that you know we are both in in this field and you know maybe we're in a bubble but you know even my daughter uses it. I see my my mom does something with it when we go in you know Social Circle people who are not in technology, I see. So it has already kind of penetrated outside. So I won't say that you know it's a hype or it's in a winner bubble like it's going. So
 Chatbots for productivity chatbots, for these kind of use cases, General use cases. It's, it's a thing in the world where we live in today, and people more people know about it. But think about it, do you use a chatbot to buy stuff? I don't do. You use a chatbot to order food? I don't and imagine it's been three years. It's it's surprising, it's surprising, you know, that. And we all there's so many demos people are talking about it. We are at racism at the hackathon. People, build this booking.com. There, these are these assistants travel planner, all these ideas in our head, but as users, I eat food. I shop for things. I go for a book flights. I book hotel, but I so far and I'm in this field. I'm obsessed with this technology, but I haven't made a single of this, like, that's a UI problem.
 I think it's a user adopt. It's a UI and user adoption problem. If feels like the UI inadvertently, introduces so much friction and it's not the way that we're used to.
 Doing things on the internet for our shopping experience that we say, you know what, it's easier to do it the way that I know how to do it. Yeah.
 Exactly, exactly. So so after we fail we did a bunch of user research. You know, I have whether ever new found, respect for designers and user researchers, I think, now, I appreciate. So I work closely with designers, user researchers in last few years and I think I understand this field much better now and a lot of respect. So in fact these days you know when we put together a project you need a design and user researcher because technology is one side, you need to understand in the end you want to solve user problem, right? So after the tests fail, we did a user research, we spoke to, we did surveys, we you know, we we called in some users and some of our learnings
 One.
 So if you are the user and if I if I give you okay, now I give you a new way of doing so you are familiar with the UI. You you go use you use it every day and now I give you a new UI.
 It's friction.
 You will not use it naturally. You will use it if it's really solving a fundamental problem for you, it's really it's something fundamentally different. Maybe something you used to struggle a lot that I don't know, maybe you're looking for house and you had a lot of constraints, and a search bar was, you know, you were not able to Define with the filters and search bar. And now, if you, I give you a voice experience and you just spoke to it and it just understands you, then you use it. But if it's just incremental, if it's a better way of doing search. Yeah, and it's a whole new interface. Yeah. That's a pain in the ass. Yeah. I, as you would be pretty pissed, too. That you change the interface on me. Yeah, exactly. So that's what one of her first learning is that when you give people something new, when you change the UI,
 It has to be something you the value for them has to be very and they should know it immediately in the first 30 seconds that, okay, this is the value for me because if they feel that, okay, why am I even doing this? Is it? Why don't I just go to search bar and use my battery and introduce AI into it? Because it feels like, oh, these guys just wanna be able to say, to their stakeholders, or their stock. Yeah. Yeah. What do they call it? The shareholders. Yep, these guys just want to be able to say to their shareholders. Yeah, they're using AI goes up. It's so important to hand hold the users often we build tool and we just, you know, there's the saying right? So you build they will come, it doesn't happen. So you build then you need to handle. So onboarding guiding the users a great example that I always like is you know I have Alexa in my house.
 It's a black box that sits there. It's so inviting it says I talked to me about anything, then you talk to Alexa and out of 10 Things, I talk to Alexa about eight eight, it fails to works, but then that's a design problem. So, if and that's the case, also, with many conversational chatbots, we say it's a plain screen, right? And plain screen is nice neat. I like it, but at the same time, if I'm the user and you now give me this thing. And behind the scene, there's an agent which can do a lot of things. Maybe, there are 20 tools connected to that, I don't know. I don't build it. So you need to onboard me. You need to guide me and over the last few years, I've been working with designers and there are some kind of excellent ways to, you know, some and these are not new in the world of design there are you can you can, you know, maybe when I enter you can already have some boxes that I can interact with, you can have tool tips as I go so it can be a guide. It should be a guided Journey for the user. So that's a second learning. So I've tried both a chatbot.
 So, you know, immediately go for chatbot it fails.
 Then you try UI. You say that okay? This is the UI then let me try a new UI powered by Jenny. I make it different. You know, there are a few things happening on the screen. It's much more dynamic.
 Oh, that has better results than chatbot but you know that's also that that is constrained, that's limiting. Chatbot has more flexibility, right? So okay that also doesn't work. So I'm kind of coming to conclusion that the best interface is a mix of UI and chat.
 So these days. So there's this word that, you know, we discuss a lot in our, you know, in with my colleagues is the concept of generative UI
 Even the UI is generative, you know, let me first. Tell you what this means.
 It's like I'm talking to an agent. I'm having a conversation.
 Instead of just replying to the conversation, you always present me some UI elements. Sometimes it might be, you know, show me some items, some Cruz results.
 Sometimes, you know, related item and so on.
 And these UI components can be dynamic.
 So, the agent needs to decide based on my user based on my previous message, maybe, you know, with the design team, I build 10 UI components, and then based on the user requests sometimes you get similar dishes or, you know, similar items or, you know, sometimes item careers or sometime something else.
 And what we are seeing, we still need to test it more. But what we are seeing is that this experience because, you know, we buying is a very visual experience, you know, conversation is very limiting, you know, I just want to sometimes, you know, even me as a user. I want to scroll, I want to click. I want to swipe left, right? I make decisions about what I want to eat based on the image. You know, this image makes me feel hungry. So I so if you are
 dynamically creating these different widgets on the Fly.
 Only depending on the input that I give you from chat or also if I click on something then you can show me more like that. But that's a great point. If it feels like that is very mix of traditional machine learning and new agents. Yeah. In a way because if I'm clicking on something that's a recommender system problem. Yeah. Yeah. Totally. If you're clicking on something, that's recommended system problem. You know, the traditional World Is Still applies. And I think that helps in the, you know, that's how Tik Tok. For example, you know, you Tik toks, you're looking at these feeds you'll swipe left, right? And they're recommendation, will go them gets better. But if you think about, you know, again if you think about this movie Iron Man Jarvis,
 Jarvis is an assistant. Who's watching you? What you're doing in that environment?
 It Feels So natural, so we have built, you know, it's still not live but, you know, it's at a proof of concept where we build an agent who who's watching the screen. So it's not just answering you, but it's also watching your actions on the screen and then it talks back to you depending on those actions. So whenever you add an item to the card, it says, um so a good choice or you know, I knew that you would like this item or something like that and that interaction feels so much more natural. Then you saying everything based on you know, typing chat where you know it's just watching you. Yeah that's it's almost like you having to suck that idea out of your mind and put it into the interface is a lot of friction. Yeah. And if the agent can just watch you scroll and click that can be there with you and it's much more of a copilot experience. Yeah. And I've seen that in action, we don't have a product live here, but this is something. This is an idea we are playing with, right? But also the user adoption problem. There is I imagine you're gonna get a lot of folks that are
 Are like, I don't want you watching everything that I look at. Yeah, yeah yeah yeah. So there's that trade-off. Yeah. So we have we have not cracked it yet. Totally, you're right. So this is something which we are exploring and every user is different. So you need to, I don't think there's a silver bullet that I can share. You are people that are fully okay with that. It's like, I'm fully. Okay with that. My, you know, the way I think about, you know, data privacy, of course, it's important. But, you know, my mentality is that take my data if you can give me value. So, if I see value in return, I'm happy to give my data, so and different people think differently. So, last learning on UI.
 Before we move to next topic.
 is contextual so being contextual so often we
 We try to build a chatbot which is like the solution for everything, right? So if we give the, that doesn't interface.
 But one thing that we have seen much more useful is
 And this creates a lot of friction because it's a new interface, you don't know the capabilities and so on, we are used to regular interface. So one thing that we've worked better is that imagine your regular UI and imagine, you know, there's a maybe there's a floating button there and depending on what you're doing, maybe you spend five minutes looking for things and you know, looking for items, it pops up at the right time. Contextual and it helps you with a very narrow tasks, very microtask. I'll give you an example. Maybe I want to buy headphone and I'm looking at a headphone and it pops up. It says, do you want to compare this headphone with the latest headphone from Apple?
 It's amazing. That's an aha moment for me. If that happens, I would click on that, and you don't need an entire conversation. This opens, and the comparison happens using an llm. And, by the way, that's a very simple. It's you don't need agent for that. It's a simple little call connected with tool. Well, I guess the hard part is, if you want to put it into a table dynamically creating that table and what yeah, this checks this box. This doesn't check this box special on Mobile screen that doesn't look good but you know you know we have designers and you know, there are different designs. Yeah. The different designs with solve it but the point is instead of having, you know, just a chatbot that does everything and you user has no idea about its capability. If you divide some micro job to be done,
 And you helped to the help with the users at the right time. We in our experiment we find that much more effective. Maybe real fast, we can detour into how you make sure.
 It's coming up at the right time and giving you the right suggestions. This is where evals comes in ah, perfect segue. Yeah, this is very well it's coming. This, this is again this you can compare it with the traditional world of push notification. So all of us get these notifications from different apps at different times. You know, you know, I use Domino's app like pizza and Domino's sends me notification and a lot of times the notifications are bad. So and if the first few notifications are bad, then you stop. Either you silence them, or, you know, even if you receive it.
 your brain doesn't process it because you know it's bad if you get a good notification if you start with on a good if you have a good start where the notification actually maybe it's 1:30 p.m. and I'm in a meeting and I'm really hungry and, you know, the
 App knows that I'm vegetarian and it pops up that, you know, this it knows that I like Falafel, that's the context. So if it pops up at 1:30 p.m. that, you know, Nishi, you haven't had your lunch. I notice that you haven't had your lunch. I don't know how it will know it, but you didn't order anything from. Yeah. So we're assuming you haven't had your lunch. Yeah. So, but that's that's the thing that's the notification, which I would love.
 So again, I think this has been this is an old problem. It applies to push notifications, it applies to other things and now it's still true in the world of llm where if we pop up contextually. And again, the idea here is to a B test and try. So the idea again, the idea is that don't do too much, don't overdo it, don't send 10 messages but
 you can. So if we take food as a context, so there's lunch dinner. So breakfast, lunch and dinner, maybe you optimize you say that, you know, at the right time when it's lunch time, you pop up something and then you use the profile of the person to whatever information you have about the person to write something, which the person can relate to. And again, this is where llm, you know, we have done some experience with the llm and it does well. But again, you know, no Silver Bullet. These are few ideas, which we are trying that don't be don't over do it and whenever you do it make it personal you know people say don't make it personal. I say make it personal. I could see how you get people that are in my case. For example, I'm
 Checking out on one of the apps looking at some food. And at this particular restaurant, I've already added a few things to my basket and it says, oh, hey, I know you like vanilla milkshakes. These guys make an incredible vanilla milkshake and that pops up and just the right time. And do you want to add that to your basket? So again, it just reminds me of like traditional recommender system. Yep. In a new way. And so it's almost like,
 both with search and with this,
 Totally. You're doing these old tasks that we did in predictive ml. You're just adding an extra layer on top of it to make it even more personalized and even better. I do it. We talk a lot about this until that, the problems are still the same, right? So we are still recommend Netflix organizes a recommendation system competition. I don't know 20 years from now, ten years from now. What? So we still talk about recommendation, we still talk about search and recommendation is not completely sold, so it's not completely sold. That's sending notification, so it's the same problems. But now the toys are different. The tools are different, and those tools are helping solve the same problem in a bit of it. Yeah, okay. Going back to
 Evals? What is your take on those? Yep. So when I think about evals
 So we we talk, you know, processes also an investment firm, so I speak a lot of Founders and we talk a lot about moat. You know, what is the mode of the product? How do you differentiate your product? And, you know, a lot of time, you know, you see, people say that, you know, people don't share their system prompt, you know, system prompt is mode and, you know, I understand that you spend a lot of time to engineer that
 But I believe in evil so much that I think, you know, when I'm not a Founder, but if I would be a founder and if my system prompt leaks, I would not be worried. But if my emails leaked, I believe so much anywhere else. Then I, if I would be a Founder someday, I would say that evals is the real moat of your product and not your system. Prompt a lot of times when you launch these products before we launch them. How do you know if it's good enough, right? And again, this is the same problem that you can think of, in a regular software engineering world, where you build software and software development lifecycle is much mature. Now where you know there are this quality assurance, there's a field and I feel quality assurance. Testing testing in production, regression tests and so on. So there's the entire thing and now with AI applications are now when the entire product is about AI, we come to this new world where and special now LMS are non-deterministic so it's not your regularly. So, how do you know
 If something is good enough and we spend a lot of time with debate a lot. So there are, you know, people in the team who disagree and you know, everyone is passionate and you know, someone says that okay, we did not think about this case. It doesn't work. Other people say that. Okay, maybe you're being too particular, it works, it generally Works 80% of the time.
 So the answer to that is evil. So you can do that in a systematic way through evils where the idea is that you build a system to check, emails is nothing for me, but the system to check if what you build is good enough that the first case, do you launch it and second once you launch it,
 Once you start getting real user queries because it can also degrade. Users can also ask different things which you did. We are not prepared for evals. Is that the traffic light that keeps you? Informed so that you can. So there's a part of it which is pre-development during development and there's a part of it which happens like in production, I've heard it explained as again going back to like
 Traditional predictive ml, you have offline online. There's like the offline training batch jobs type thing and then there's the online like boom. Yeah, real time. Yep. Predictions. So few mistakes which I have made myself and I see a lot of people make so I'll talk about that in evils one.
 I think it's a mistake to wait for your product to be launched.
 To write your emails.
 Because you know, you want real user data, right? So evals need data. So imagine you have a chatbot and you need to see what how people are using it in the world.
 But it's too late already because you launched it, maybe your chatbot sucks and you will know it much later.
 So so this is where we do a lot of simulation synthetic data generation where before it goes live and it doesn't have to be. Again, it can be simple. It's like, you know, you get together in the team and everyone is playing with the chatbot and you create emails based on that or you use another llm where, you know, maybe you are these the lone engineer in the team, you give it 10 queries and then you use an aluminum that you know, generate 100 more queries and that's your synthetic data or you give it different personas. So a lot of people they wait for the, wait, a lot before evals, but I think, first step of evil the simple, it's even a person in the room, just, you know, firing queries, you create a data set of 2050, and then that's a data set. And then you see how your agent responds, and maybe you manually go through it. And if you go through it manually, you know, in half an hour, you'll realize that okay, these are the scenarios where it does, okay? These are the scenarios where it doesn't do good that influences. Your thinking that you know what metrics should I what metrics are important?
 Me. How should I use an llm as a judge? Llm is a judge, is a popular concept, right? For evals, because human labels are expensive. I'll assume the audience knows about it.
 Another thing that, you know, which I've noticed is that, people immediately run to llm as a judge. But again, there are a lot of low-hanging fruits. I'll give you an example.
 It's so llm. So these days, you know, we are a bit spoiled Solem as a judge. Let's just, you know, users and so you, you get the input synthetic data. You get the output from the agent. Now, you ask an llm, you prompted a bit that okay, did this input satisfy the user intent and it's easy? You can do that and immediately people run to that, because it's easy, but there's something, which is even easier, which a lot of people, Miss is a lot of time you have
 More deterministic because you never know that maybe the llm itself is making mistake, right? And then you have the final label which is a mistake. A lot of time there are more deterministic metrics
 so I'll give you an example from food ordering. So imagine a food ordering experience user comes in. You have the agent conversation in the end, if the user generates a cart and usually we send a link to the cart.
 Then that's, that means that conversation was amazing. It was positive, right? They bought something, they bought something. So the final,
 Metric that you're looking at is, did they convert or not did the convert? But even, you know, it's getting a little bit into the details but you know, sometimes people come to the card stage but they still don't order. So there's a so so not always yeah. But hey, but I'm saying that was their car. If you link up, if you think about a funnel that you know, a user making search a user going, a couple of steps, and if you think about a later stage in the funnel, could be about making order could be about adding to the cart could be about something else depending on what the use case is, but those are positive signals that this conversation actually went through different stages of funnel and it's a deterministic metric because I can see that, okay, I have these 100 conversations which of these conversations went to through this stage of funnel, and that's a positive conversation. So, grab that for the email, grab that for the emails, and that is so much better than using because LM as a judge will make mistakes. Look for those metrics, whatever those metrics are and don't run immediately to 11 as just a step. Before that, another thing I want to talk about is
 If you again if you look on Internet and if you see advice from people out there, don't evals a lot of good advice, but it very soon, it gets complex where people talk about, you know, conversation level analysis.
 Singleton, multiple turn analysis. Yeah, the silent failures on the silent failure. There are eval Frameworks. Yeah, tool calling. You know, did we call the right tool? Did we call that? Did we call the tool with the right parameters?
 Did if you have multiple tools, did you go? If you can even do a state management because maybe a user query calls 5 tools so you can even map that okay from Tool one tool to did the maximum mistakes so it can be very easily blow up and you'll be kind of paralysis analysis. You'll be stretching your hair often, you know, we, we present these emails to business folks and imagine, you know, you have a meeting and, you know, you have these 12 different, people get lost and you know, you lose the audience, you lose, you think so for me, you know, you know, for us what we have seen, when we build products that process, it's like the first level of evil is very simple. It's like, this is the query from the user, this is the entire conversation, you know, multiple queries, maybe there are 10 messages, exchanged
 You use and maybe you, try the usual metrics. Now you come to llama as a judge before you go to turn by turn analysis before you even look at what tools are called. Just give the entire conversation to an llm and ask some basic questions asked it. That did it satisfy the user intent?
 Ah, did the conversation went in a direction where you were trying to the agent was trying to close the order because you can in the end you care about making a sale.
 And these are so simple. And this business, people, they relate to it and then an llm will make a judgment, you know, true. Not true, partially true. And so on, and there's already so much information there if you now see that, okay? These are the things that were, these are the things that don't work.
 To be honest, a lot of time are evil stop there we don't because there's already so much information that there's already so many things for us to fix based on what doesn't work that we never go to level 2. Level 2 is you know tools. Did you call it? So there's, there's almost like a hierarchy. Yeah. And and you're as the higher gears first and foremost, there's like, looking at the metrics of did they try and actually make the sale? Yeah. Because that's our top level metric that we're trying to affect the needle on. Yep. That's really all that matters at the end of the day. So if they agent isn't closing. Yeah then yeah. Totally got to be who was it in The Color of Money? What was that? Who's the actor that was in it? You gotta always be closing and you got yeah, teacher agent put that in the prompts. So yeah that's one way to look at it but of course you know on a lighter side, does the agent, follow answer you. So if you were the user on the other side,
 The conversation went in a positive direction or, you know, the agent asked for burger and you give them pizza. So did it satisfy the user intent? Did it go in the right direction? So there's those are like very strong evals that you need to be focusing on first.
 And then if you need to then you can start peeling back the layers. Like did it call the right tool? Yes. Did it use the right parameters? When it called the right tools? Yeah, yeah. I can see that. Yeah. It's almost like there's an 80/20 here. Yep. That Brea's principal. Yeah we've got these emails. They're gonna give us 80% of the important stuff. Yeah and it's only these two evils, we gotta look at. Yeah totally so that has been our experience. So we also made this mistake where we already got into complicated evils. It doesn't resonate with business, it leads to analysis paralysis. So these days, you know, whenever we do evils, we ask it practical business questions first and then there's already a lot of insights there to act on and only then there is value in going deeper. Yeah, but it's not as actionable. Imagine, yeah. Like you say analysis paralysis where you're like, there's so much that we need to do here. So coming back and saying
 Is it answering the user's question is the user experience. Nice is
 The metric being moved like the metric we care about. Is it trying to move that? Yep. Yep. Those are very very basic and yeah, very useful. Yeah.
 There are two more. Yeah let's comes to evals one is about. So at process something we do. This is a recipe that we have tried many times and it has been very successful is labeling party. So we call it labeling party where you know, we invite you invite your team, you invite some stakeholders, it's important to invite. You know, businesses will you get 15 people in the room?
 You order some pizza. That's why it's a party. It's important, your book, one and a half hours.
 You spend time with these people to actually go through real conversation and then this is what the agent responded.
 Now, if I am imagine, and then the question you ask them, is that imagine you are the user on the other side.
 And then you ask a couple of these questions that we did the conversation, go in the right direction, did it try to close the order?
 And a couple of other things did it kind of break any guardrails or whatever. It depends on you know what the agent is trying to do. So you define a couple of questions, easy questions. So don't talk don't talk about technical stuff like tool calling and so on business related questions.
 And what what would come from that is that you spend one of us and at the end of one and a half hour, you'll have, I don't know. Every person will label 10 data points? You'll have 150 data points. And when you look at that, 150 data points and now run your llm as a judge, the prompt that you wrote and asked it to label, these 150 data points. You'll see that there's a difference between
 What elements is as human obvious, they'll always be different in labor data and as a judge and then you will see that depending on your use case, sometimes the llm will be more lenient. Maybe the llm always said that your agent is perfect or maybe they sometime the llm would be stricter.
 so, depending on the use case, I've seen both
 But now, when you see the difference where the discrepancy is between human and aluminum label, you look at it again, manually, all this is manual.
 And then the way to use this data, is you use this discrepancy to inform yourself that, okay? This is where the element is making mistake. You know, human is right here. Now, go back to your llm as a judge prompt.
 Give it examples. So few short labeling give it example that when this question comes in this is what you do. So
 I think often we also use, we take think a little message, very easy. We just write a prompt. We expect magic to happen and get something, but our experiences, you need to iterate on that prompt. And we're going full. Circle is Context, Engineering? Yeah. Yeah, it's complex engine it all these things are related, but you need to improve your llm as a judge. So we spend a lot of. So we do these labeling parties, we improve the prompt. We give it few short examples based on whether this and now your llm has a judge. You run it again. We see that. Okay, now, it's closer earlier it, they mismatch, I don't know, 30% of times. Now, the mismatch is 15%. And again, you need to keep doing it every two weeks, do a labeling parties. So that is a recipe that we've tried, and it's been very successful. So that's an actionable inside. Yeah, and it's a manual process and, but it's building. Its yeah, it's, it's rewarding, it's, it's nice as the Builders of the system to, you know, sometimes look into the data yourself and and hear from the business side of the house. I imagine, yeah. Hey, this makes no sense. Yeah, why would we
 Hear about this, why, why would the LMS say this? You discover a lot of other things, which, you know, you were not even maybe hoping to hear but that happens.
 Last point there.
 But it's actually kind of it's a little it's related with labeling party. So
 when you do a labeling party, how do you offer? It's very important how you offer people, how they can look at the conversation. And this is where, for example, we use a lot of these external tools landsmith length views. So, there are these observability tools where it captures the interaction. But at the same time, if you're looking at Lang Smith Trace, it's very user unfriendly. It's like you know, bunch of Jason's. If you have multiple tools it's like, you know, I love looking at it. You know, I'm a technical person but the business person gets lost. So one thing again, which we do which works very well is with now with very low foot by using v-0 versus. And so on you, create custom annotation tool. There are standard tools as well in market for labeling data, like label Studio, there are tools out there, they're good. But you know, our learning is that every use case is different. You need to visualize different things for the user, I'll give you an example. If I'm looking at Food conversation or a shopping assistant conversation,
 if I'm the user who's evaluating it, aside from the conversation, I also need to see that, okay? These are the items which were shown to the user that point these items. And I need to show it seed. Visually otherwise, how will I judge these items are from these restaurants? Maybe these restaurants are open or closed at that time because if you don't give me that information, so, annotation tool plays a big role. And what we find ourselves doing a lot is
 You know, with a couple of it's like hacking we just quickly put together a tool and a half a day, using v-0 vercel and people love using it. And then you get the audience is much more involved than going through a trace in Langs. So, again, it's small point, but it makes a big difference user experience. Yeah, user experience you