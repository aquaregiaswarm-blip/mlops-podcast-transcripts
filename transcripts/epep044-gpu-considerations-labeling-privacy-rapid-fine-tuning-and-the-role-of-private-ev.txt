I mean, these models are the quickest depreciating assets, right? They've got a half-life, the most expensive depreciating right. There are hundreds of millions of dollars that, you know, after weeks or months have lost their value. Because a better model came out,
 We talked with Paul and zhukov today, about training models, for agentic, use and evals for agents.
 Paul is the VP of AI at process and zhukov is the director of AI, got two hard-hitters and myself. He founder of the mlops community, my name is dimitrios. Let's get in to this conversation.
 Paul set the scene, what are you training in? How you doing it, right? So we're talking about,
 What we do at process in terms of bringing AI in particular, agents into production in that, in the scope of our work, we care about making sure that the systems work for our use cases that they are scalable and that they're reliable, right? And so,
 To do that. We have to do a lot of different things, right. And I'm going to focus on the AI part mostly because of course there's a product side and so on and the user needs that. We obviously also spend designing new experiences with AI but I'm gonna focus on the AI piece. Well, we need to make sure we understand how well models perform on our use cases and to nobody's surprise, the benchmarks that we see on this space are not good, you know, indicators for that, we saw recently even that the, yeah optimizing for the benchmarks, that's the oldest ml problem, right? Like, overfitting to the test set. So I think that is something under evaluating these models and they come and do most common every week. Now, very quickly for whether they can be used for a Polish car dealers, call summaries or for predicting ingredients in a recipe and in Brazilian food dish. Those are all the use cases that we care about how they perform. So evaluations is one
 Two is how much, how well do they fit to our use case? So it's domain understanding right? But also language capabilities, we Benchmark voice models and in our evil sets which we'll talk about in a second in, you know, Africans polish resilient Hindi and so on to see how well do they perform for our voice calls? Because most of our traffic is not English.
 And then we also care about costs. So once we believe that we found a good fit, we need to make sure that these models are actually affordable to scale to hundreds of millions of calls because we have, you know, that many users interacting with their products on a daily basis.
 So, how do you optimize for that cost? When you need to find either? You need to fine-tune models. Smaller distill them, or find open source models that you maybe want to host somewhere in control the full, let's say, inference stack on. So then you get into the world of how do you actually find to how do you just still them? How do you pick the gpus? How do you optimize for you utilization? So we'll talk a little bit about that and then
 the last piece once you've actually gone through this sort of ability to take the performance of a top-level, Ferrari level model and getting that performance out of a fine tune smaller model. How do you set up the entire inference stack to work for you with the same slays that you can get from and anthropic and open AI or any of the other commercial model provides and that brings in the whole new set of challenges. So, and we've had to solve things across this entire spectrum of problems because we care about bringing Jenny I or agentic systems into production at scale.
 You run the Gambit basically from Soup To Nuts. You gotta figure out from top to bottom.
 That's it. Yeah. So I don't know where you want to start, but maybe we can talk a little bit about the evaluation piece because we have zuku here, right? And we've went since the beginning of working together, it was very clear that a lot of things change all the time models, get better, they can now code, they can now translate, they cannot do it generate images, but one thing that will not change is our need to be able to evaluate how they perform on the tasks. We care about. And so zuku has spent a lot of time on developing the in-house capability for us to do that break down, how you're evaluating right now.
 Is the short answer, which is what it's probably good to give a little context. Actually, it's about Harvey sharer Evolution like we have many different world sets, touching different tasks from our group companies from our own AI team. And then we build pipelines. A relation pipelines, every time there is a new model out there, we are. If I mean, it's from credible resources, then we are putting that model in our Pipeline and I think mentor of ours, if not days, we are getting the results from that model on different tasks and we are sharing it with outside and the
 evaluation sets are
 Not public. Ah, they're not, that's the critical point, it should. Maybe I can give a bit more background to how we started like it. I think two years ago, we, you know, process on stack Overflow. And at that time, we really wanted to understand the value of stack. Overflow of data how it's helping because everybody was training on their data. So we want to say, okay, home how much we are really helping these models and it turned out a lot but that's a different topic. But what we were doing that, we are training the models and then we need to see
 Okay, how this models perform after training?
 on stack data and without stack data and we needed to really like a Toro evolve sets, see
 and also, you know, in stack data, you can really see different programming, languages, different,
 Type of questions debugging on debugging like explaining conceptually guiding all different types of this and then we want to go deeper in the evolution. We just didn't want to see you. Put some date on the model and have good. It's just writing proper. How good? It's answering python debugging questions, compared to JavaScript writing. So then we build all this evolved set with different attributes.
 And we were using this, basically to follow attract the models we trained, but it turned out. I done occasionally, you get some claims say we build this great model on very small, you know, it's just 3 billion parameters but it does better than jp2 for all.
 And hey then we are curious. Does it really do it? Then we get our revolve it we Benchmark. Not at all, not even close, it's just marketing. Yeah, and now he's a hey, there's a lot of hype that and it came a bit kind of, you know, more often. So let's then take this benchmarks since we already have them, let's put them somewhere. So we can also follow up to models because people have more of those and this then you said hey everybody is asking us then in our group companies, which model should I use for this? Use case, we said, hey, I don't make it, share it with and then that's how we started actually. Yeah, and you have a few different types of benchmarks, there's the stack data set and benchmarks but there's two calling benchmarks, right? What other ones do you have? I mean the stack Benchmark they have two benchmarks there. One is typical stack over. Cool stack. Overflow of questions, historical ones like a thousand questions we got there?
 And that one I kind of saturated so I can give you a good example. Like we were benchmarking, the models on this historical data and a stack Overflow, I hit some API agreement with Google and then after that agreement we saw that Google models suddenly picked on that data set very thick. They start training on it. So he said, hey I mean, this Benchmark is good but we need something more fresh, something really like some data from
 The models isn't seen, haven't seen in their training data, then we created stack on scene and that's, we are getting from stack Overflow, from the recent data.
 We hope models haven't seen those ones and so this is a bit more, you know, new like they're asking about the new libraries or new ways of solving the problems. So that became stacking, but both of them are Q&A.
 And then we have two calling for our agent. Thank you. Use cases of good. Models are picking up the right tool and using it with some organization and just to see how good it's following the instruction. I think that should also talk about because not every summer is same and we have open book and a where you give like, for example, that's very eleven for our use case, we have, for example, Finance colics, bringing large data sets, putting there and asking some questions. So we want to see if you give such a big context length of each model performs in answering those questions.
 And I think we have a couple of more like entity extraction.
 And what else we have? Q&A from token. Actually from token question and answers but that's also very new so models. Haven't seen that data. You need to keep the freshness of these emails up to date. How often are you?
 Refreshing the data. Yeah, that's a problem actually. Like they are getting outdated, they're getting automatically. So the elastic I think after six months or something, they all start, you get to smell. So you need to update it and so I can see. We are trying to do it every three months.
 For other data sets, we are bit slower, but we are adding more like, kind of and getting out some stuff, but that requires also resources. So I cannot say that we are doing it every month. Yeah.
 But I think every six months or something, you should show some attention to this, it was just check what's going on there. I mean, models are getting way better, but you also see some parts. They are not that still good. Maybe you put more emphasize on that. But also, depending on our team needs, like, hey, we need this type of like, for example, into Okami start using go. So it is important for us, which models are doing good in Gore, right sudden, it's a requirement for us, then we say, hey, let's make sure we have some enough coverage for go and we see which models are doing good there. So that's also based on our needs, we are changing scope of the evil sets too.
 And you can see what's really nice if you go to Pro 11. So for prosus llm that AI. You can see that there's basically this leaderboards on all these tasks that, you know, we've scored hundreds of models on because something will happen, right? So, deep seek comes out and of course, well, we are very cute. Like we see, oh, deep seeks out. Like, how? Well is it perform? And then like zuku saying somebody will claim XYZ performance. We run it through our basically evaluation the problem, evaluation Pipeline and within an hour, a couple hours, we know exactly where it shows up in the leaderboard and we know what it costs and we know whether it does better function calling or summarization whatever. And then it's like hey great. So now we should consider this model and move to let's say solving the inference piece, which we'll talk about the gpus and whatever for this model because actually we've got three or four use cases which, you know, which with five billion tokens each, right. So then we, but we have then confidence because of the quick evaluation rule to do is worth investing.
 More time and bringing this model into an actual agentic or AI product that we have life. It's great signal, amongst all the noise. And all of the hype on social media that you see, oh, this model got released as beating all these benchmarks, you get to see for yourself and I can imagine you did the same thing with the Llama 4 model and you probably were a little bit.
 Yeah, unimpressed.
 We can say that they have but also good point there. And now we have agent ex systems, right? Like there are many tools like summarization can be a tool there. I mean, you can, for example, if you look at the latest, Google framework, you'll see agents can use agents as a tool. So if you know that for some organization which consumes a lot of tokens are like it's very costly, you put up a huge tax in it and then you ask questions if you know that. Hey there is an open source model that can almost deliver the kind of same performance. It very expensive Flagship models. Then you can just replace that model there. You don't have to use the same model for all the tasks and their problem gives us this kind of freedom to choose. You look around for that type of task. We can use this model. What is the best alternative or we can cut here? We can get better performance here than this kind of. We can also do kind of rotting around one point to add about problem.
 You may ask, how do we get this email sets, created? Right, because we know that need is there. Like we said you all the time, you must you want to know, is it worth investigating further, considering it for moving in production?
 And so you need these eval sets, but this is one area where, you know, we found that there's a lot of Solutions on the market tools that, you know, claim to help. But actually they're not the biggest problem that we have for eval sets of revaluations. It's the eval sets themselves. And so, how do we get those? Well, this is we have a couple of advantages that we have like I said a big group loss of companies with real world data.
 So what we typically do is we'll gather, you know, whole group of people that are hungry and we organized a labeling party, we provide them some pizza and some other snacks and we put them in a room and explain. Hey, we're going to go through, you know, a bunch of summaries or we're going to go through a bunch of, you know, I don't know, listing on OLX and figure out what are the entities that we would want to recognize and we sit down and we manually
 With you know fueled by Peta curate these emails sets which we use and that's the hard part because that's laborious. You need people to kind of come in and do that and it's by the way it's been a lot of fun and I think we've discovered that there's a great way also to give people who are not so familiar with the work that is happening inside. An AI team is a little bit exposure of how you know, we work. And so we started organizing these labeling parties.
 Design labeling and then you have eval sets that we now have ready to be evaluated against anytime a new model comes in and maybe it's worth. Also talking a little bit about how we use LM as a judge.
 To very quickly, be able to evaluate a new model against that evil set. Oh yeah, tell me about that.
 yeah, I mean
 we have been using a m as a judge for a long time. I think the first thing we discovered that I'm for example, for stack Overflow, even like, stack on scene stack, create data sets, like benchmarks there, it's like, technical answer. So there are many ways of giving this answer, even if you go, stack Overflow feed, you will see different scores, you know, for different answers. So there's not one ground truth.
 But so how are we going to evaluate that? We cannot just compare it. So what helps is using whatever the ground truth is there. Like people you in stack, Overflow of case, we always have a true answer and we also have a couple of recent answers that get lots of good scores. So if you bring those answers to the model and then you bring the question and then you bring them answer from other models and ask the model basically. Okay. Is this answer? Correct or not? And here is the clue kind of hint how ground truth looks like. And then using that the model then kind of I think we got like eight to five now around 90% accuracy in evaluating if that answer model generates answer is correct or not. So as long as you have a kind of credible ground truth there I think models are pretty good in judging the Curious of the air on generation or generation of adul models.
 The part there, we judge.
 So you can also you know then you need another evil set. Like you need to judge the judges, find out which church you should use as a kind of Judge because every model performs differently in that and to the poll point then we organized a labeling party. We generated many labels from the models by putting each of them as a judge position and asked it asked each model to evaluate and then humans evaluated their evaluations. And from there we had okay labeled data set and then we use it, we have it in parallelism as a judge Benchmark and that Benchmark shows how llms are performing as a judge. And from there we pick up our judge and then use it. I think now at the moment is cheap before that one is our current judge.
 But if something else comes better, we will just replace our church. And how are you setting up the judging system, because there's a million and one different ways to have an llm as a judge.
 Yeah, what we do. I mean you always need to explain.
 The task for hunting, that's no brainer. Like what this task about, these is a technical crease. So you need a technical answer and there.
 I there are also many ways like, say, okay, you some people use from 0 to 10 scoring, you know, okay, give us a score.
 A model gives seven. Another model says 8, what are going to do with that, right? Like so in that the point we came what matters for the user like essay technical question, answering person. What's important for me is I get an answer and I stop searching so that's good enough. I just go and then continue my work. Hey we said okay but second also has something similar like except acceptance right, it's acceptable or not. So in that we want to bring in the kind of binary that solves my problem or not for technical question, but we also realize, hey models are easy to flip around, so we better give a bit room for them to make mistakes. So rather than 0 and 1, what we did 01,
 And 23. So if 0 means really, like completely off one is a bit more. Okay, it got some clue but this is still uses for me. Please, I can live with his answer. I don't need to and trees like that's perfect. So actually we put two and one there, it's a bit room, to if it flips, it still flips into the same area. It doesn't go the other side and makes it completely wrong. And we did that, I think we could get pretty good results. So it's intent. What is important for you for each task?
 If summarization, for example, then that definition changes. What's important? There
 so in summarization it has the following instruction if you ask for Tiki to
 Key takeaways, it should give you two not three.
 And things like that. So for each task, you have to go a bit deeper
 see what matters and then reflect it to your rubric basically one thing I think about because I am a
 huge user of Gemini. And if you were asked me six months ago,
 What model I was using, it would not have been Gemini. Have you been surprised by certain models gaining traction or losing traction over time? Because you've been tracking this for so long?
 I mean, in the beginning, openai was always there on top. So in every Benchmark like from summarization to technical question answering, it was always open AI models on top and that was a very big margin with the second best model if it is not from open air. But over time what we see now that margin gets narrower and narrower and now open a is also not always on the top. We see other models getting very close or getting ahead of open Ai and sometimes you just see okay for different tasks. Also like the models are changing now. It's a we got lots of more variety to know it. Google models it on traffic models and sometimes I can even Mistral like a small model was really doing great. Job on the, summarization better than the open. I have Flagship model and now we have the China Dimension, maybe Paul. You can also share your talks about Chinese model, what we see, them getting way better and better over the time, and that's all for helping us. So, open source.
 Community, like we are training a lot of models. And now with those benchmarks, we are just using them to see, which model can be our Baseline for different type of tasks, by looking at the parameters, okay? That model 32b, that can be a really like a great reasoning model and then from The Benchmark we can decide on that one. Or you look at the small models in The Benchmark. What is a small space model? Yeah. If you go back to this angle of building agents in production, let's take token, which we talked about are internal sort of platform for productivity and others.
 And over the last two years of building token, we've had continuous Challenger Champion, models to kind of see which ones are better at certain tasks that the agent, you know whether it's summarization or imagination and we've gone you know over 100 models that we've put into production and I can tell you with confidence that the models we have today in token that are answering the questions in six months. All those models will be different well, and so think about that as a product Builder, that means that we need to understand which
 Sick. You know in six months which models are going to be the ones that we need to be replacing with, right? Because there's gonna be tons and a zhukov mentioned, you know, initially it was actually easy. Like it was just open AI. That was the best one, right? Unless there was some costs consideration. And so on it was easy to know which one was best.
 Today, what's been very interesting is to see the rise of, you know, Neo model providers in particular, open source ones, coming from China and other parts of the world that have basically become very, very good alternatives for many use cases. And, and we are starting to use those in production which again then brings us to the point, which we'll talk about is how do you organize inference for models that are open source at that size that are available, you know, and the first open source options that were really the only competitive ones were essentially llama and mistral's models. Now, all the sudden you have, you know,
 When models coming out, of course, you've got the deep sea ones. And as we switched to more reasoning workflows right in particularly, for the agents, it's really cool to be able to get much more visibility on how these models were trained, what they work? Well on,
 And we see that, you know, because we've got two years of history of pro LM these leaderboards, you can actually play the the video to see. Okay. How many you know what is this leaderboard look like over time in the beginning was very concentrated, just three or four players and now you start to see that, you know, there's it dozen players that are in the top 10 that are continuously, you know, catching up to each other. I mean, these models are the quickest depreciating assets, right? They've got a half-life, the most expensive depreciating that hundreds of millions of dollars that, you know, after weeks or months have lost their value, because a better model came out. And so again, coming up to this product Builder perspective, we're agnostic to where the model comes from, you know, because we just want the best model cheap fast, whatever, whatever criteria used for best.
 and but then, once we know which ones best, we still have to solve,
 How do we actually Host this, right? How do we do? Is it a commercial? This API is a service or do, we actually need to solve for figuring out where this model is going to run on the cloud on our own gpus. And so on. Yeah, it's probably worth talking a bit about the infrastructure side of things and going out there and figuring out the gpus and I am fascinated by the GPU market right now because
 If you do decide to go the open source route, you need gpus. There's no questions. All other tools are kind of optional. We talked with Bruce about what you chose to buy and what you chose to build yourself. There's no way you're building your own gpus know, you might go and you might
 Decide to buy gpus, but that is a very hard way of solving this problem. So can you talk about this journey of getting out there and looking at GPU providers? There's so many right now so maybe you can break it down on what key considerations you had to look for when you were going out there.
 Yeah, actually say that are available to players there.
 And for, I think it's kind of talked to almost every leading player out there from Nvidia to core wave. Together, AI, the Mosaic about it became databricks later. So, we had a lot of exploratory phase. And if you talk to GPU providers, I mean, initially everybody is very open and then it's all everybody is, the cheapest. Everybody is the fastest about, once you start talking, it's shifting like I do, most of them are kind of asking, you know, some dedication like commitment. So you need to get reserved capacity for certain number of gpus for certain number of months and then if you do the math, hey, that's a lot of money and if you don't, if you're not going to use this GPS, immediately you are just wasting like burning your money and you are just idle there.
 And they are waiting. So that's kind of dilemma. We had initial hey. We know that we are going to explore a lot and there will be a lot of times. We are not training, we are just manipulating data preparing for another experiment and we will have, you know, extensive week of training, so many parallel trainings. So we need many gpus tools weeks like a peak times. But then,
 We again get the Lord experiment results, analyzed learn, and prepare for the next set of experiments until we are sure that. Okay, now we can do a big training because now we know which data to use and how
 everything is settled, but
 That part we needed on demand. So when we start talking is on demand, was not something. Every GP provider is very interesting about so that was tricky point. We couldn't, I think get it from
 yeah, almost anybody like
 Hey, if you want to do on demand and suddenly either price goes super high, it becomes wow. It almost you pay as much as a result couples.
 Or you get very small you know, number of gpus assigned to you. So you cannot really run any meaningful experiment so it was a kind of tricky point so with our current GPU provider so that was the differentiator for us we have a kind of Freedom we can use on demand gpus and scale quickly and then we need a very bigger capacity. We can reserve a week ahead and then get more gpus for bigger trainings married to add that. Because I think you've described as a very specific type of need that we have for gpus, which is when we train models for which for a period we need, you know a few dozen a few hundred maybe a few thousand gpus to run a training run, it's very spiky. Yeah. And that's a shorty on demand piece, right? Which is like then a commitment of one year or whatever longer is really hard to make because we just needed for that batch. A training run, right? And then, you know, look at the results and we'll run it again.
 Then there's other you just inference related where we also need gpus. Right? So, we looked at. Can we do? We need to let's say again, commit dedicated gpus or do we do on, you know, on on demand spot, instances or whatever.
 And on the cloud providers is an option, but they typically, are they have this quotas? They're very slow to respond. They're not great for the, for the setup because they tell you all you can have, you know, eight eight a100 or whatever, right? And but now it's sometimes we need 12 or sometimes we need whatever.
 so then,
 For those infant workloads. And we had the issue that if we had committed, let's say a certain number of gpus resources to this this this service, right? Which was doing classification of whatever or translation, whatever. Like in real products you also have a lot of Cycles like daily or even weekly where people come in and they post pictures or they order food. And and so you need to kind of adjust because if you don't adjust your underlying gpus for inference, you have a really low utilization and then your customer are the roof again. And that's, we're talking to provide us like together. AI has been interesting because that's more inference related for the training. We don't really have worked with them yet and but they've been great because they basically offer, you know, on demand or token as a service for open source models, with all the Privacy guarantees that we needed and so, depending on the workloads, you actually need to solve for very different
 You know, and other resources and so on the elasticity is key for you, if I'm hearing that correctly but also you bring up a great point that potentially if you can you're going to try and just go with tokens versus, I'm going to figure out how to put the model on to this infrastructure and then figure out everything around that to make that inference, very fast or whatever. So how do you break down the, oh we'll just hit the open source provider in a token API wave versus. We're going to go into the gpus and do it ourselves but we have our own models. So I think sometimes it's not like if let's say, let's take a deep sea example, right? We want to, you know, use deep sea for a workload. Then we could do a few things, right? So either go to deep sea themselves, which typically for privacy reasons. It doesn't work. So then we can say, well, we download.
 The models from hugging face and put them on some bare metal right in the cloud or other. Then you get into the issue. Well actually we still have that sort of day-to-day cyclical usage pattern so we probably can't get the economics to be very favorable because utilization fluctuates too much or too low, if you have guaranteed bare metal on your own. So then, you know, then let's say a service where it's token, as a service for deep sea, which is just the vanilla model, right? The model everybody has access to and together AI is great because then you just send the request and get it back and they organized the workloads.
 But we're also training our own models and so that's sort of training model. Like how do you expose that for people to use? Then you can use together AI or commercial provide, obviously, don't have that model. So then you still need to go back and say, okay, how do I provision make? This work is yeah. For our own training model. Well, and the other piece is on
 the when you're out there and you're looking for Hardware,
 I'm sure there's a lot of other considerations that you've thought about maybe in this specific training instance. There's it comes with its own set of considerations. Yeah. In the inference instance it comes with a whole different set of considerations. How do you look at what you need? What boxes you need to check as you're getting for the training? I mean, initially, we actually didn't know, we learned it to buy experience. So let me start training, the models, for stack Overflow. And at that time Mosaic I think was the only provider helping there and then we start using their platform and they were great guys. Helping me to support but we learned, we cannot use every tool or every model out there. So we have to just go with whatever they support and that kind of start being a limiting factor for us in later. So that's okay. Important. Checkpoint for us. Can we use the Frameworks and the mother ever model out there immediately with this GPU? Or do we need to wait? A kind of support to come to the
 Platform. Yes, make it real like so what you're facing is
 You know, llama to came out and we wanted to train that model on the Mosaic servers, right? And that was impossible because their their, their, their middlewares, and their framework. Can I think converge it was called? It was not compatible with that model yet.
 Or that model was compatible with it. So we had to wait weeks to get that model available. So then you would want, then we would want to have access to the bear, underlying gpus and run our own everything on there to be able to use their services. Well, it feels like it's almost the the bigger question is, when you're looking at the GPU provider, do they, what kind of support do they have what kind of software comes with the gpus? Exactly. And most of them are nowadays, supporting slur, for example, does that kind of default out of box? You'll get. You always need to hugging face kind of integration. We can bring every model from hugging face to the GPU. And then you also see later, hey, how are going to bring the data here? So initially you think, hey, that's a no brainer. Can you should be? That should be many ways but you'll see. Hey, not everybody is really supporting everywhere and most, are they in S3 how we are going to do the secure connection to bring to that this data, once this data
 Is here, how are we going to handle that data? Like what kind of privacy guarantees we get, can be removed. The data anytime we want, do you keep the replica? Like those kind of simple questions. Don't have the same answers for every provider so then they all turn to kind of checkboxes for us. After start talking to the GPU providers and then there are natural things like okay network speed and like, how is the scaling, okay?
 There is an autoscaling or not. So they're all kind of technical checkboxes. We also asked the GPU providers
 but one thing that was interesting is that availability was also
 hard, right? I mean, the shortage, we felt that sort of very real, right? It was actually hard to get access. And even some of the providers said, well, we only do you know millions of dollars dedicated capacity, of course and so. And then, of course, the quota is in the cloud providers. And so, that, that shortage was real,
 yeah, especially if you're certain type of GPU in mind, like,
 A, we give you a hundred as much as if you want. But for h100, you have to wait and for
 H200? Well, you need commitment, things like that and then we okay, you always want the best fastest GPU. But then you sometimes there's a big cost difference like, you know, it neighbors, for example. Now we got the same price for h200 so immigrated there, but that's also changing from GPU provider to GPU provide and I can imagine it's not as easy to swap out the GPU provider as it is to just swap out the API if it's tokens. So going back to the point of if we can. Let's think about just using the API if we need to go further because of some certain criteria. Then let's figure out what we have to do to make that the most efficient as possible.
 Like swapping our GPU providers actually is easier than you think other than maybe the data and so on, but like at the end of the day, we've got pipelines of code that we run especially if it's not tied to their Frameworks, it's just you know, getting the environment set up and and so on it's more. If you switch from an H 200 to a 100%, it's not super sticky, right? It's it's pretty commoditized. I mean you need to change your data routes like where data is coming and also like on paper able to supporting every framework. But practice, it's never the case. So we start with using a certain frame for framework for training our models. And then
 The rgp providers need to do some adjustments. Know if you go to another one I'm not sure if I'll talk about will be enough for us to continue this. Probably they will also need to do some adjustments.
 And but yeah, it's not a big deal. I know you also mention one big thing for you is the support that the providers give you. And I can imagine that if the scenario is, you're inundated with a lot of requests and you have to prioritize which ones you want to go with. You're not going to be offering support to the person who's saying, hey, can we get like spot instances? And we'll see. Maybe we use a little money this month. Maybe not as much next month versus someone who's going to Pony up and pay the actual big dollars. So did you find providers that would offer you support despite not doing these large commitments from the get-go? Yeah, we are looking and we see the importance of support later, you know? Initially you know. Okay. It's just a bare metal. We
 Take it. We put our Frameworks and libraries on the training.
 About the many things can go wrong. Like it's a huge network of the gpus Clusters connected. Not like there are so many reasons that things can go wrong and then you always need to support. And we were lucky with an abuse on that front. We had a slack Channel and the guys were always available anytime you ask a question, we got an answer back. That's how we could move fast. Otherwise I cannot really imagine it after seeing all this initially. I didn't think really we needed that much support but after seeing on practice how much we needed support, I think it's a big prize. I mean, I will definitely ask this one of the first questions up. Yeah, definitely. Okay. It's easy to forget how hard it is to run clusters of thousands of gpus and not all companies are equally experienced in that, right? And I think just having the gpus doesn't mean that we can run our training runs uninterrupted, right? That was a I mean sometimes we wake up like the next day and the run.
 For fail, any alert and so then, you know, you're losing time, right? So that is definitely something that that's important to give him, right?
 Yeah, it also helps that the right down the street from you so you can go and pop into their office, right? Just the next door so we can go not their door. That's helping I wanted to get in to
 the labeling that you have on the
 What is it exactly that you're doing? It's you you've evolved from your labeling with token answers. This is changing gears. All right, so let's change. Let's change gears for a moment because
 I want to talk a little bit about the output and the labeling that you've done.
 for token and the answers and how you are able to
 It's not evals you mentioned. It's what is it? It's so it's tagging it's and it's different than the parties like the labeling parties that you were talking about too. So let's change gears for a moment because I think you've done some Innovative stuff around figuring out how people are using token without needing to read tons of messages or a lot of output from token, right? And also that helps because then you have a bit of privacy. Yeah. I mean it's more than a bit probably will see. It's really a lot of privacy there so we cannot read any message and not even our team without the consent of the person. So token, messages are super private. Nobody can touch and read them even the database. It means. And that's when you have that requirement. And also we want to understand how people are using token, right? Where is helping them? What and also are companies, they come to us and they say, I want to know how my employees are.
 Using, they are really getting a kind of value from it. Or can you tell me?
 Any give me any insights. I have, we can support them. Make it more useful and things like that. That's the first point, like understanding how people use it but also is the Builders of token. It's important for us to see if toucan is fulfilling. Do you know the expectations if it is delivering a where it's great? I mean, where it's failing where it's performing great. So understanding for compare performance, the third thing is also understanding the impact where it's providing the most value and where we can kind of improve it and those kind of Dimensions. We want to focus when we get the tagging in place. So what we do there, we build a system that takes a token conversations, not just the question the entire conversation with the model. Like a user, question models, answer and follow-up question from the user and follow-up answers from the model and we give this conversation to our llm and we asked like, I think
 205 tax, for example, what is the domain of this conversation?
 Finance, for example.
 Or HR or data science it and what is the task type? It can be like coding.
 Debugging and what is the kind of use case? Because some but I mean it can be coding but it can be somebody from Finance writing, you know, some
 Analysis work for evaluating a competitor or for startup like and okay, this is the use case, this is a test type, this is in this domain and then we kind of make it very granular to see how exactly token is being used. And then we also ask the model, okay? How much time people are gaining with this task.
 And then in model is also like okay. The first thing is how people would do this manually without token says, okay manually. He should do this right Discord. Go check Google probably, he will spend 60 minutes. We took on now with conversation and answer. It took 10 minutes. So he saved 50 minutes. I mean, it's okay if 50 minutes saving, I think in average, which each token session, you say, like a 17 minutes or something and then in a day if you use it three times you say one hour by using to talk on but that's all so kind of you know, the metric we surface and then see. Okay, how much we gain by using token? That's one of the text.
 But I don't know if you want to Deep dive into technical. Well yeah, let's talk about the tags because I know the tags were a bit of a headache since it is. So open-ended, that's the point. Well, you have two choices there, you either.
 Kind of restrict the model to certain number of texts. And then you say, okay you have to choose one of those tags. But token is being used by different companies, different language, for different type of things. So there is no way to capture everything from that. You have to like, it will always try go beyond those texts and then basically what you do, you just miss cluster, your text, then
 You introduce, you know, in a curious into your system. If you do that, all you have 10,000 tags. Yeah. I mean we didn't want to go that way. We say okay, we are going to let it
 Keep it free and after but what we see them and there's no end to end, it's going all wild and the models all, you know, it's not always consistent. Even if you keep the temperature at 0 times, it calls it python. I mean, it's coding sometimes. Script writing sometimes programming like, you can get many different texts or the same task, actually.
 He needed to do lots of things there to kind of tame the model in that, we didn't touch the model, we kept it free but we did lots of post-processing and use the model again in this post-processing. So kind of we build new way of llm powered clustering here.
 If you go, I mean, use directly embeddings embedding so far, not perfect, you will get also in a cured clusters. Hey, then the idea with such
 Powerful models. Why don't we use them for clustering? Technically, you think I put all the text, poor all the text into model prompt and it will give me clusters out of packs. Nice, little organized text and I can use it.
 It turned out after 100 text models are failing. So some of them are not coming out at all like they're forgetting the text.
 And the Clusters are like in the beginning, it creates a cluster but intent it creates very similar cluster and a certainly it's not consistent at all again. Hey, we cannot use that. It turned out. We cannot really put more than 100, maybe new models change but like a year ago it was like around a 100. He said, okay, we use that model, but we do the clustering ourselves with the hierarchy of tax, we kind of created
 Good enough, clusters. Then use the model to refine those clusters.
 And that worked the best. So you get a kind of good enough cluster. You ask the model if it is good or not to model can say okay this tag is good. This kind of not that should be the name for this cluster. So we go over all the Clusters and after that we did another clustering because sometimes you just need five clusters to communicate the you know, audience who needs to know very high level. But we also want to go low level and see what people are doing. So we need to kind of tree level of clusters there. Deep dive into what people are doing.
 And it turned out. Okay. I know the cluster like you are writing
 You know, Python and you are using, I mean, for finance like somehow fight but what? Exactly? I want to know a bit more but it's also private. I cannot look into it. So we also kind of created another tech, kind of giving a bit more detail like the goal of without anything private, anything sensitive. But just if you were small, so if you want to read get a bit more idea, you can go read those texts in a kind of nice UI.
 So in that, we came up with a solution, you know, from all the way down, you can go all the way high in different clusters different on your needs. So we can solve it to our users, that can tell you how people are using token.
 Half token is performing and what is the impact? One of the reasons that we wanted to do this is because, you know, we have large volumes of data and we need to get some understanding in a way that skills. And in tokens case is compliant with the Privacy, requirements of the stock two certification and so on. So we build flow, which is our data analyst agent. And of course this is it's an interesting case because
 Token handles, millions of questions and the interest we had was you know, what types of topics with kinds of, you know, how helpful is it? How much time is it saving? So we build the float data analyst, it took a lot of time, right? But now flow analyzes these millions of questions continuously and provides this data so it's very useful.
 but it turns out this pattern of understanding, large volumes of data,
 is useful for a lot of different, you know, settings and in the group right people that are dealing with customer interactions or support tickets or you know other kinds of you know like sales calls
 so,
 You know, we are now moving more heavily into this AI Workforce as a topic and theme and creating, you know, AI agents that become part of the team to help things make easier. We now have lots of data analysts connected to all sorts of internal data sets that allow people to access data analysts questions. So we've got flow or data analyst but there are many token data analysts out there in the group in iFood and OLX and so on that are doing very similar things, taking raw customer data. You know, adding a layer of intelligence clustering tagging and so on that, then can be used by the teams to make informed decisions about what to do.
 so you did mention before that models are the fastest, appreciating asset and
 At the same time, I think I heard you just say that you're training models, and you have to go out there and procure, gpus, how do you weigh those two? Seemingly opposite ideas in your head.
 Yeah, I mean, I think you must train the models. If you are in that game and you are serious about it and then you have a really good domain data, you have to leverage that and in our case we have really companies having lots of valuable data. So we wanted to create specialized domain models, for this companies their own business in their geographies and language. Like for example, we are creating a large Commerce model now that can help the customers of this companies, way better than the generic models that model, both models will know their domain better and they can later build on top of those models. Their agents conversational assistance, even use it for their existing AI systems.
 And I think we should.
 Be mindful about the depreciation part there like what they appreciate is the base model. So we are not interested in creating a language model from scratch and we are just, we have our pipelines, we have our data. And if the base model appreciates,
 For example, it was when 2.05 a few months ago. Now, last week we got three cubic so we can just replace the base model and we continue and just enjoy the performance gains coming from the base model on top of on top of that with continuing and even further pre-training alignment. But that part doesn't really add that much on the course I think.
 If you think about effort spending on training, it is about fine-tuning and and pre-training for our specific tasks, on top of an existing base model. Like, zhukov just said,
 That we will see. And expect to see many more of our proprietary models going into production today, that is still, you know, small percentage of the use cases, we try and test open source models on that we've fine-tuned because we want to make sure we know how to do that. I think in the next 12 to 18 months, it's very likely that we will displace a lot of the commercial model traffic to our own models, for a very specific tasks, because it's higher performance and much more scalable, and in terms of costs,
 Yeah, the cost is a big one there.
 It's especially once you get it out to that many users.
 And you're starting to see, wow, it's not only our internal company, that is using it or are portfolio. Companies, that is using it. But now, when we start to push this out to their users, then that cost is going to go through the roof. If you're using just off the shelf API, I can imagine
 And also the customization Factor like you have lots of flexible to done. You have a model. You can train on multiple tasks there and you can use it not just for wanting now, you can also scale it to different tasks, like why don't we change? I mean, we have lots of statistical models, right in every company, AI infra host.
 I don't know how many XG boost models and other models then. Hey, we have a better intelligence here. Maybe we can also replace some of those models with that one. Now you start thinking, you know, in that way of how to leverage that resource that I have, I trained. Can we also introduced this task, maybe change the data?
 Structure differently but also start utilizing a better intelligence get a bit more performance game from it. And now once you have your model and the trainings, okay, we'll start also thinking around that. And then you also certainly find yourself coming up with some innovative ideas. Like, hey, I can also use it here, I can also let me try it there. Let's do this. Maybe then put these agent on this model rather than open a models and suddenly you kind of more well and interesting, it comes right back around the pro. LM like, you're not gonna know which ones are performing better and what ways unless you have something like Pro. LM, exactly. Actually also published that like a kind of evil driven development we maybe I use case. We always try to start from the end like we create you all set for parallel. We put it there. What is our Baseline? Okay? What we expect from the model, we also do initial labeling for some evolved set, if you don't have any labels and then after that, we started developing training and experimenting, but we always say the bench.
 There in Portland, you recently created private spaces. So for that, those are not public. It's only for our team or a process companies. So when you start a lot of project, every time you do a benchmark, you on your privates, you can see which models doing how which technique work best, which prompt work best. You can do the kind of follow-up on that space and we iterate over through the benchmark.
 Are you allowing folks from the portfolio companies to add their own eval test idea? Yeah, that's cool. Hey everybody can use it, they can create their iPod space. They can create their own leg space and put to we give the pipe through the pipelines donating. They need to come up with the evil set and then they put the website and everything else is automatically execute. Ran through all the models they want to test. Even, we can also test fine-tuned models, so they will be able to see it.
 That's all we've got for today but the good news is there are 10 other episodes in this series that I'm doing with process deep diving into how they are approaching building AI products. You can check it out in the show notes. I leave a link.
 and as always,
 see you on the next one.