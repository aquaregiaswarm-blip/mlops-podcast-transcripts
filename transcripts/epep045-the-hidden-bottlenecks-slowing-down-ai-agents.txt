the evaless one, typically the bottleneck is actually not the tool itself, it's about having the ability to generate those eval sets and having a product feedback loop
 which you can then take data from to measure against, you know, new models and improve and sort of have a real flywheel. Let's say
 we approached the builders by question when it comes to building agents and what kind of vendors you want to onboard in the Enterprise setting Bruce and Paul talk us through this Paul is the VP of AI at process and bruises an AI engineer. I myself am Demetrios the founder of the mlops community. Let's jump into this conversation.
 You're just saying, hey, we have this culture of, we should buy it, we should build it. We don't want to think about building it. Yeah, despite that. Yeah, as we'll hear from Bruce later, it's hard. Yeah, it's so immature at this moment, in time the tools that you have. And I kind of went step by step over the main areas where you have these categories for me, well, maybe what I can emphasize is
 You know, sometimes the tool is not the hard part. Yeah. So let's say
 If you want to have an eval solution, you want to know whether your models are good, that's the problem you have and you want to know whether they get worse and how they perform and where they do worse and where did they do better?
 The hard part is not the tool the hard part is you need an eval set that you curate, you need to have real users that give you, you know, new conversations that you can measure against. You need to have that capability. If you have, if you don't have that, it doesn't matter what whether you build the tool or yourself, or not. And so, they're in that thing, the tool itself, at least in our setup, right? In our setup is not the thing. That unlocks our ability to do evals better and human time is expensive. Yes. What I think about though is there's an argument for if you have a tool that makes it much easier for humans to get that eval set, then you're saving time, right? But I think that you're absolutely right. But the, you know, what we do in the team to give you an idea, is we have this labeling parties, right? So we buy pizza, we invite folks, everybody's welcome right? Not just focus on the it or Engineers, like anybody in the company is welcome.
 And well, for example, let's show them. You know, these are certain set of answers for problems set so we'll do like, I don't know images for food as one example or we do code. Obviously, if you do code, evaluation you need the folks who can can rank or you know, score or evaluate code Snippets. We will do. Let's say a customer conversations and then you need people in different languages if it's polish or or Portuguese, Brazilian or whatever. So we'll do these labeling parties to create data sets, right? And these are the eval sets that we can. Then score. You can see that on Pro llm, which Google will talk about in another episode I think. So the the hard part is getting those eval sets.
 And it's not that if we then all of a sudden had a tool and we share them all right, with its spell book or human Loop or orc Ai and so on some of them are really great. And by the way they offer 15 things, one of them is evals the evaless one. Typically the bottleneck is actually not the tool itself, it's about having the ability to generate these eval sets and having a product feedback loop which you can then take data from to measure against you know new models and improve and sort of have a real flywheel. Let's say one thing that I think is Wild is how you're actively encouraging the team to go out there and buy tools. You would prefer that they do that because as Bezos said back in the day focus on what makes the beer tastes better and creating your own orchestration system is not what makes the beer tastes better. It's not going to make toucan a better tool. If there's something off the shelf going, grab that and also you have the angle of
 if you find out about a tool, then you might want to become an investor in it.
 Despite that you have had a lot of trouble per se, I guess that might not be the best word, but you've had a lot of difficulty finding tools that you can actually buy. Yeah, less adoption of outside tools for our agents in production systems then I'd like or then we'd like, but yeah listen. We're all like we're time and and people are scarce resources, right? So you know, I always say to the team if if there's a tool that we build that we could have bought outside there's no you know there's no brownie points for that nobody like there's just no value in that we could have done you know other things moved faster or whatever.
 Um so we're very actively push the team like hey check out this this new whatever open source solution, check out this new, this tool out there, for doing evals, we're doing observability for doing whatever and I I do think our team was a little bit in the future in some ways because we're so like our job is to test this, the latest and greatest and then figure out what makes sense. We have real problems that probably other folks will have, you know, 12 18 months down the road. And so it's not just, you know, I want my team to like have all the time that they can to spend on the things that make a difference. And so I encourage them to buy and used tools wherever that makes sense. It's also that if we understand, hey, this tool out there, these guys build something for a problem. We know we have and others will have down the road, we should partner with those guys. Like, let's make sure everybody in this group. We've got like I said you know thousands of Engineers all over the world that will have those problems already have that we should.
 Then expose, you know this tool too like hey did you know that such and such as you can now, help you with authentication in the agentic flow or they can help you with whatever observability of such and such part of the stack and and that happens and it also means you know because process has big investment arm. We can also help, you know, partner with those Founders if they're raising money. To say, hey we want one, we're already using you, we want to use you to, you may want to have other users in the group because there's a big global company. And third, if you need help, you know, Investments and so on, we can provide that because we use tools in my team to try and solve real problems that we think others will have soon. I tend to trust your opinions about what's real and what's not a lot, I would heavily weigh them because you do live in the future a little bit and you're able to tell me what is hype and what is real. And so when I see
 You saying, oh, this is great but it's not for. It's not for us that kind of is a signal for me to say, there's something wrong with that piece. Why is that? What is it? What? And the fact that you haven't on boarded a lot of these common tools that you hear about every day,
 Is it very big signal for me? Whether that is an orchestration thing like a lang chain length whatever or an observability tool like a length fuse. Yeah, insert your favorite laying. Yeah. And and eval tool that for me is a huge signal. It makes it
 all the more important when you do onboard, a tool that I
 stopped, and I listen to why you chose that tool.
 Yeah I think sometimes like I said I do think in some ways we live in the future because we're early than the many and trying to build things that scale and production agents or other other, let's say AI powered products.
 Um, so I I think one thing that we do understand was one of the problems that others will have down the road. We're also a bit of a strange team. And I've come to realize that as, you know, sort of look back is like, we are because we're much closer to, you know, we train our own models, we work with a bigger Labs out there, we have our own evals. We're we are not representative of many users that are coming down the line. So if you have other teams that already, you know, they have ml models or maybe they don't, they're smaller teams and a smaller companies, they will have different needs and they, you know, even if you fast forward them, 18 months, they will still not be same as a sort of full AI team, right? So, some needs in the ways to solve those needs that say the problems are common but the ways to solve them for a team that is less.
 AI experience, they will maybe favor more Dragon drop solution, for example, as one, right? So those I'll give you on. Like there's a lot of observability tools out there and we looked at all of them.
 And many of them had to sort of drag and drop so solution.
 Right. And it turns out that for some people that's great because they, they don't want to get into the intricacies of the coding and so on. But, you know, my team would say, well listen, I need an SDK or I need this to be headless or basically be able to kind of interact directly with the underlying things with the debug it. I need to see the logs and so on.

Um so those things that there will be, it's like if you look at, let's say the orbital ability tools in the the ones that create these chains of Agents, you have zapier.
 Right. Which is just anybody can use that can have has access to a keyboard in the mouse and then you have all the tools. Like I don't know the things that are native in in gcp or AWS, right? That kind of help you with observability and change these things together and those are very different end users. The people that access the AWS environments and the gcp environments and in your environments, are your SRE your engineers.
 The zapier users are, you know, anybody and so you need to solve for those different needs as well.
 Are you using agents?
 Yes we are. We started already using them early on and by the coding agents I assume you mean things like Devin and Nana and many others. That's something that you haven't built yourself. No we have not know. So that's a clear area where we see, you know, amazing team, shout out to the cursor team for example, who have really focused on, you know, the Persona being the software engineer and how to really change the user experience for them to be able to use these things, right? So we don't have, that's not that's not, we will products that are consumer-facing, right? So we would never build a coating agent, but we use them all the time. We test them all the time, very excited by all the things coming out. We're investing in some of them as well.
 But if you think about, let's say the first generation of tools in this space,
 The first one we're GitHub, copilot, like and they were the first ones to come in this space. They of course, with openai
 And we could already see the early days. This is actually pretty good.
 And then soon after you, you saw, let's say newcomers like of course, cursor Source craft codium. Yeah replay. Yeah. And so on that came and started to do this slightly better.
 And then they evolved and actually in my view they overtook let's say the let's say the first movers, in this case co-pilot to build. What are now like software native sort of agentic flows and this sort of conversation and the interface that you had that cursor, build where they train their own models to actually executes has was very quickly, like it was good that was better. And in fact, you know, my whole team moved to cursor
 And now, we are sort of at the next level. Think of it, like autonomy levels at them or children care level, one, two, three, four. Like, we're we're now at the next level where let's say Devin like Asians, who can
 Don't they just don't, don't just do autocomplete, they don't just edit code, they can create entire environments, right? And executed, tasks. And to end,
 We were playing with them. I think started about a year ago when the Devon announcement came out.
 and it was a little bit under very promising, but under
 Now fast forward, you know, almost a year. We see that it's much, much more useful and
 You still need to look at this different.
 Let's say use cases. So if I'm in my hobby project at home and I need to create a little, I don't know, a little app, or a little dashboard to track whatever household chores to say something right by myself. I can use Devin perfect now today and it will work basically, like first time right before I had to try five times. It'll work for something simple.
 Great. But how much of that it really happens at work, right? So, yeah, Legacy. Yeah, I need to commit stuff. I, you know, have maybe smaller projects, some of them are more demo like there. You can also start to use Devin but what happens there is if you do what my experience at least today is that if you let it create the entire code base from scratch for the thing that we need to actually work on together and maintain.
 Like you create it in ways you didn't like the architecture isn't exactly as you prefer and so it's not to your taste and it's not easy to maintain that's where I don't see it working yet. What we are. Now finding it works really nicely isn't that other type of projects where you actually already have a code base. That isn't, you know, a codebase at 3,000 Engineers are involved in but it's a code base. It maybe dozens of Engineers, maybe hundreds or involve them, but it's, it's, you know, clearly documented repositories where you already have a cicd pipeline things are, you know, already, documented better, because you got more people than you can, you know, individually.
 See, so that agent actually comes into something.
 And you can ask it to do these tasks that, you know, now are, you know, a card on a jira board, write a ticket. Like, I don't know, make sure this becomes XYZ compatible or insert this feature. I have given give it a shot and that's where these things in my experience are now becoming great because it's an existing code base. We already thought about how the architecture should look, but you're making tweaks and modifications that are some incremental and the agent. It's not that large that the agent can't handle the context.
 And of course, all these features that we're seeing now is, you know, like Devin creating its own Wiki, right? And indexing the code base there, you just repost together. You can give it instructions and then it knows for anybody else. Working with Devon on that code base. That's cool. And we're tracking sort of how Dev and now how many PR's
 is Devon actually making on the team and
 Kpi. Well it's it's definitely like we want people to have to just do. Now we're basically saying Devin Duty right? So somebody's on duty this week and on Devon and this week is Devon but the next week it'll be Madness or cursor or whatever or replit. But the goal is that during that Devin Duty
 Any task you pick up from the jira board? You should first think let me run it with Devin. Wow cool to see right because then otherwise how do we know if this thing got better? And where do we apply it? And so that's the kpi that everyone needs to try that. Be on duty to sort of learn and discover. And then eventually we may set like, hey, 10% of the PRS, you know, should be Devin, right? If we believe that's feasible or 20% or 25% or whatever. How do you ensure the cognitive load? Doesn't just go through the roof because people are submitting a bunch of shitty PR's. Yeah, well we had this right, so like we actually built our own like a PR reviewer, right? That thing was so bloody verbose, right? It would people turn it off after a couple hours because it was just answering a ton of comments and then the cognitive load of you having to read those comments compared to the submitted, you know, PR like it's easier just to look at it yourself and you can you can spare the the commenting from this AI robot say, I think
 So that's the question, right? So, where what's the sweet spot? Where the work done?
 Offsets, let's say the cognitive load. You still need to additional cognitive loading to put into understand that ai's work. And that's what we try to figure out and then and so is it, you know how many PRS are worth it to sort of give to a to an AI software agent? We're going to play with an SRE agent soon. So, there's we're just trying to understand like where is it useful. And we're not some to, some extent is also a team thing and individual thing, more senior Engineers, or some more juniors. Are you familiar with the code base or not? And so, on, for onboarding, for example is great.
 Tell me more like I mean so for me like I don't spend a lot of time coding, unfortunately. Let's say committing like production level code but I want to understand what's happening in the coaches. I can just like integrate a code base is using cursor.
 Right? Or any of these other tools new hires have the same. They come in and they're like, hey I need to work on this new repository remove people from projects to projects and they can come in and say, hey, tell me, describe to me, you know, how this code base Works, what are the key, you know, endpoints or the key Services, what are the standard, you know, what are the utils? And it'll just describe that to you. And so it's a good way to get people up to speed or people like me that are in sort of day-to-day in the code base.
 I think we should bring on Bruce now to talk about what tools, you all have tested and where you decided to build your own and why
 Great.
 You're here. Now I am. Who are you? And what are you doing here, Bruce. So I'm Bruce, I work at process in the AI team, you know, for two years there now and I work as an AI engineer in the team. What I spent most of my time on this building token which is our own agent or agent platform and we just distribute it to the portfolio companies. So I work as a like, AI engineer to software, engineering mix of both of those and you spent a good amount of time trying to incorporate different vendor tools that will help you build took on faster. Hey know that.
 You've been working on token for years and so there is this.
 Maturity factor that goes into the tools that are out there that you can use and buy even if you wanted to pay money for something.
 Maybe it's just doesn't exist or it's not at a solid maturity level.
 I typically see folks using.
 Tools in a few different places and you can tell me why, or why not, you ended up buying a tool in these places and we can talk about which areas there's actually value and they're which areas you potentially see value in the future if it matures or there's no value.
 One is.
 Prompting tools or prompt.
 Tracking.
 Yeah. So for I think for a lot of products out there and also for what we are doing, problems are very essential to your system and storing that somewhere else I guess is.
 Fine. Of course, we also stored in a database but we're not storing it in an external service at doing the evaluations for us because
 Feel so essential to have a very good problem, Dad. I think it's something you want to spend time yourself on and the evaluation piece.
 So we've build our own evaluation flow for that two reasons. One, big one is that within the AI team, we are also have a team that do evaluations that have a leaderboard. So we have the knowledge to do those things. And a second big reason is, if you're gonna use an evaluation tool, you're gonna need to send all the conversation data to an external party. Which usually is
 not it's hard to convince our legal department that we're going to do that so you bring up a great point there on
 the fact that maybe you wanted to use this tool, but to actually get it through
 And get the okay from legal is a whole nother Beast.
 Yeah.
 Yeah, I guess so, it's not only like getting okay from legal, I guess.
 You also want to be confident that something like that goes. Well and if there's a very new startup companies out there, oh, you can just send all the whole conversation. I'll keep everything in track. It does feel
 A bit strange to, to send it all out there because it's Court to your product that you do well. Oh, when you change prompts, for example, and
 let's say I'm talking to a colleague at another company and they asked me this question, I would think like, oh,
 I'm a bit hesitant to tell you how this dude is because they are probably not gonna like if we send out all the pii to to these evil sets. I think it really helps then if you have these companies that provide that that if they have a self-hosted version. So the data still at your
 Place. Yeah, that's a common design pattern or vendor offering where it's like bring your own cloud or VPN and so then you don't have to send that data anywhere. No, that's true. But even still, I can imagine legal is one vector that you're thinking about when you're thinking about that build versus buy. Yeah. And so you said we're going to do the prompting tools and the evaluation tools in-house. Yeah, those are two big ones in my mind that people will pay for the other one is orchestration.
 And so these are Frameworks like the Llama indexes in the lane, graphs, link chains, that type of thing. So I have an example in our code base where we build our own orchestrator and we just added vertex Ai. And we noticed that sometimes we are throwing a timeouts. So the request was taking too long because we set the time out, it was five minutes, so it's very long. So yeah, because we're doing the request ourselves. Where did the API implementation? Ourself we weren't using their SDK. We were pretty close to what the code does. So we easily edit metrics there to see. Oh what time, how long does it take to get the first junk from 30X? So it was super fast so we still didn't know what's happening here.
 And then we added an extra metric which did the time between chunks. And then we noticed, all of a sudden it's sometimes the time between chunks is super high like minutes which is probably just a yeah. This is probably just malfunctioning or verdict here, which is fine, but if you then using something like library to do that you're probably missing a way to fix that. So now because we did do that it was very easy for us to First at a special time I'd saying okay. If the time between Chong's really large, we just try again with. So the user can get a reply quicker and because we're making the whole API motivation ourselves, we saw that in the headers. So as a request study that fertile say I send us, we could just easily at that to that metric again, and it locks that we have. We exported those send those to Google and they easily can then see what happens. And I think
 You need to be quite lucky. If you accept precisely want to do this because it's quite nice but it is a fixed. But I feel like using an orchestrated are probably wouldn't have that or he would be very lucky and would pass it all the way up top but yeah it's that transparency piece you have so much more control of what you are able to see and what you're able to do. And you do say that this is a very Niche and specific situation, but it can be generalized to if it's not this specific situation, I'm sure you encountered five more like it.
 Yeah.
 Yeah, and then it's nice that because it's okay, one of our main advantages of using us, instead of using a different orchestrator or agent framework, is that we take care of that, we're very reliable on the reliability. Yes. Do you think that,
 The orchestration Frameworks, although they give you a lot on the abstraction and they're able to make it much easier, they're able to make the Prototype experience. Nice, the reliability experience drops, and so it's like you have to balance those that trade-off is very clear, you have
 Speed to prototype is very high but then reliability is very low. Yeah, definitely reliability. On those Frameworks can be super good as well and if they just have what you need, then it's perfect. Well, explain why they didn't have what you need, what you needed. I know that you had mentioned the different sdks and specifically, like a lot of these Frameworks are in Python. Yeah, the, yeah. So we use go. And there are sdks, of course, out there that dude, this kind of stuff.
 Um, but even Google and anthropic say, okay, this is our bada version, which then funny because entropic says, in their documentation, use our sdks, if you're streaming because the API the direct API implementation, might not work as nicely. But
 So it's not only with the orchestration or the evals or these other tools, it's with the foundational models too. Yeah, definitely. I think a lot of these for they like these AI products out there. I really focused on python doing very they're probably very good to building the SDK.
 but yeah, if you're not using bite and yeah, it's pretty hard to, you know, what are some
 Downfalls or what are some pitfalls that you've had? Because of that? Not using bites them. Yeah, we were using biting bites and two years ago or even a year ago still I think
 yeah, so one thing we're missing is the sdks
 because we're doing sometimes like these small tweaks that I just talked about, you could do that in with monkey patching, which is also not possible and go.
 but,
 Using go was such a nice transition for us because it's compiled and there's already almost already so much uncertainty with what comes out of a model. If then the code is, you know, okay, this is fine and and you didn't look into using something like paid antic.
 We did, why didn't that work? Um,
 are you ready for like the models that come in like the objects but then, in the goats, we
 I think it just requires a lot more effort to make bite and fully fully Fail-Safe where it would go. It's way easier to do something like that. It's almost like it comes out of the box. Yeah. Go and in Python you have to do this extra work. And super, if when I switch from parts and to go, then it was felt super restricted. But in the end now it's feels way faster because I just know. Okay, I can't do this, I should do this way something, new comes in and just easier to switch.
 It's funny that you it's restricted in a good way. Yeah, I guess so. Yeah.
 No.
 You do.
 so on one hand with python you are or without python
 You're able to have that reliability and you get all of these things out of the box. But then, on the other hand,
 When you're working with the different foundational models or orchestration or any kind of tool vendor tool.
 You have to do extra work. Do you see it that way? Or do you see a little bit different?
 Yeah, it's a little extra work.
 Yeah, I would think that's the case. So,
 Of course there there's Frameworks out there and orchestrators that are super hard to replicate because they do something very good then.
 That's a reason why. For example, we use llms that are did we don't make ourselves for this for this agent?
 Because that would be too much work and then for agent tools as well. They those are just too much work to build the whole OCR by plane, we can just use something that's out there.
 But for this for the score from now these like an llm orchestra in your agents, I guess.
 It. Yeah, it's a little more work maybe but
 Maybe that's in the beginning, but then in the end, if you're trying to find that time between chunks, it's way easier if you have the coat, and it's, you don't have to Fork anything and change the library, or if you're invited to monkey pet something to get that. That one thing you need. Yeah. So, I see it a little bit as you're investing. More time up front but then you save it on the back end.
 and when you have these edge cases, that's where you really see things shine because you can go and debug much easier and we try to figure out what do we need and then
 it's way more easy to to build something yourself and then maybe in six months we feel like oh this needs to be expanded.
 Because it needs to let's say we do memory memory, only use a level. And first, we just build it on what's the user and assistance to say. But then after a couple of months, okay would also be nice. If you remember, how did talk also because we're now integrating a lot of all tools and maybe ID if I say. Okay, I want to send something in a slack Channel and I always call this channel operations, but then actual ideas like doggone operations would be super nice. To that, is Remembrance, then there.
 But then if you didn't build that whole memory thing yourself to begin with and they don't provide that extra feature, dating gonna need to rewrite the whole thing. Again, instead of, if you have builds to yourself, you know what you want. You just added
 And of course, you can do feature requests. But yeah, I just like that if something so Court to your product, and you want to expand the feature, that's easy to do, instead of having to heck around, or a change the library or for the Lost.
 For the newest feature, right?
 The next piece that I think a lot of people end up paying for, or that there is a lot of attention around, is the observability piece and so tools, like lengths or I think Lynn Smith also does this.
 Obviously insert your favorite word after laying and there's probably a tool that is called that. Mmm,
 are you all using a specific observability tool for the agents?
 Not specific for the for the llm. So, I think most of those laying something are all for agent specific observability, but we use data for our service ability. So,
 I guess those Lang. Probably, I guess those linked tools, do a little more things out of the box for, for the for, for those agents for observability on the agents. But
 Yeah, we started using data dog and it's also easy just to add the metrics there and the logs there that you would probably also get at other observability tools.
 And it also depends a bit probably on your back end as well. So most of the time with those lengths products you can also rerun if you make a change and to your prompt. That's something we have in the backend ourselves that's how it builds with event streaming. So
 Yeah, I guess I guess you could use also traditional tools but if you don't have that in place and it's probably nice to do something like that as well. And you don't feel like you're losing out by using a data dog, which is not specific for it.
 Um,
 No. Yeah, maybe a little, I guess, a little over of course when there's a new model coming out, takes a little more effort for us to to change to, to rollout, dead model, and check, whether it's still performing as well.
 Because we don't have those tools but then we say that we don't need to send all the data to the dead tools as well and then we have a data for sorry, we have a vendor tool that does the llm logs and traceability and then we have dated up, which more more does this? So for engineering side now, it's all just combined.
 Which is also nice to work with.
 And so, it does go back to this idea of
 to bring on a new tool is
 A bit of.
 A journey.
 You need to get it through all of these different things, whether it's contract and pricing and legal, etc. Etc, that when you look at what's out there and their capabilities versus what you're using in-house with the data, dog. You say? I think if we squint, we could probably make data dog. Work up to 995% of what you get from all these other tools and we don't have to go through any of those whoops? No, yeah.
 Yeah, I think so, I do. I always get to myself. If a new look at these new products, I'm super excited. I'm saying, oh, it can do this and they can do that. And it's better than that at all, because you can do this and convince everyone in the team, we should do it. And I make the pr and I we tested on stage and then oh this. And then we find out either something is missing or we could have just done this in August. It's not that difference or like a more traditional
 A software engineering tool not that it's traditional. But in a sense it's not AI.
 It seems like that's a little bit of a trust issue also right? There's advertisement and what they say they can do. Yeah. And what they actually do and you don't find that out until you test it out yourself. Yeah. So
 I have another example for that we were looking into adding a provider like a vendor tool. Sorry, we were looking into adding a fender tool that could help us roll out tools more easily. So we have a bunch of like, more General system tools, we call them Socrates, image Generation, all that kind of stuff that you expect from an agent. But then we also wanted to do, oh, off tool. So going to Google on with the user's credentials and maybe creating a dog or going to get up and list be ours, which is a lot of work because you need to go into the documentation of Google, go through the API and be creative. And think, if I have these Scopes and I have this endpoint, I could probably make a tool that's called to create blank document.
 Which is nice to fight. Why can't this just be done with?
 Oh, it can be done. We want to do with oats but like finding out riding the tool descriptions coming up with what a tool should does, she do? It's not like in the Google documentation. Say oh and these and points are super nice to create an agentic tool for
 Kind of looks like I'm talking about mCP now. Yeah.
 What is that back? So we were looking to accompany that would provide all these tools for us that would do the oath flow.
 And we could just provide a user ID saying, okay, this user ID, these arguments in this tool. So we found one a composer and they had a huge list of tools available. So I was super confused. Like this, is it? This is super nice. I tested a couple of them, they worked. So I made TBR went to staging. We all started, testing it and then we noticed that,
 A lot of those weren't working and I was just minor minor things each time that needed to change. Like, oh, you need to request an extra scope or yeah. It was something you needed to add in the dashboard but you didn't do that. I was also not really well documented because they don't companies like new new startups. Don't Focus that much on documentation. Sadly, I wish more did. Yeah. So yeah, that's only example of where I got really excited. I was like, oh this this was a little too, that too soon to communicate to the team. We should do this.
 Because then the teams looking at you like dude, come on.
 It's more. So I'm saying okay this task is done at the end of the week because I know we have 20 tools and we have 30 tools and then I tested five of them and then you test the other 15 and then you need extra Scoops and getting certain scopes at Google is so much work and yeah, just things you didn't expect because it wasn't in the documentation. But I do like about the startups though is that they're always very much listening to you. So if you're if you would tell them,
 Something like this, they would probably help you out and they also love the feedback I guess because they feels like a feature request. Because yeah, this new anyways and the roadmap is not dead defined yet, but yeah.
 so then you scraped it out one and you decided to just go with mCP,
 Know, he did. And so a lot of our users are not working used to working with AI or are very technical people in HR are using. So just we're just disturbing our agents across the portfolio companies. So
 So far from hr's, not gonna go online, download some MCPE. I hope that it the the docker runs instantly. I don't even like a regular person easily can set that up. So yeah, we considered it and in the future it might be nice if
 For example, Google themselves are going to offer mcps and it just, maybe you go to your Google account and you say you can create at mCP, link. That would be nice, I guess. But if the users need to run the mcps locally themselves, it's gonna be too hard for them.
 So basically you check that one off the list.
 Yeah. And then another thing with the startups they we really tend to also look into the legal part and the Pea in a privacy because we need to be compliant and that's kind of thing. So I'm used to if I go into Data, I won't find P if there's Pi. We get an alert saying. Okay, just there's probably bi edits. We never look Beery Century. The same thing. If I want to look at you as a data, do you need to consent that per go conversation?
 But then when you start using some of these Fender tools, so when I started using compose you know and some users edit their accounts so we could test it out. There were like buttons where you could pick one of the tools, for example, search documents. I could pick one of the user IDs and just execute the tool.
 So I thought you had god mode. Yeah, basically. So I thought, okay, maybe this is a one-off, it's not that bad. Maybe there's gonna edit my future later or I feature but then I looked recently into memo which is like a long-term memory service.
 And did a bun added that to our system. Just locally to test it, went to the dashboard again. And then I saw my whole conversation being there like on the front page of the dashboard saying like, oh this is your recent traffic. And then I went to the memory section. I saw all the memories which is it is basically all pii because it's about. Okay, this user like this, and this user prefers if you do that. Wow. Yeah. So those tools are not really made for like made in a way how they respect Pi in. Respect the data, there's probably feet probably buttons and setting out there, but it's there was something I needed to get used to as well. A little eye opening. Yeah. It's like you aren't trying to look but you had to. Yeah. And then all the sudden I could see you looking at you but I mean to see that I promise it does beg this question of
 You want this capability? Inherent in the memory capability is you need to know things about people, but the way that the company is setting it up.
 they now have to think oh, if we want to go to users, with various users,
 We want to make sure that we're not.
 Inadvertently making their lives hard with doing things, like keeping pii or making it front and center. So it's like the typing, you need to be able to see that quickly, right? Yeah. But
 Yeah, out of box. Seems just so different than like the more traditional for engineering tools where you definitely don't want pii and places like data dock. Yeah. And it's like you would have to add this extra layer of getting rid of all this P. So adding that extra work makes you then. Come back to the question of. Should we just try this ourselves? Yeah. Yeah.
 And the cell phone. So first, and then a really good option as well. Yeah, it feels like you're taking two steps forward and then one step back or
 Two steps back and one step forward, sometimes with a different tools because they give you certain capabilities. It makes your life easier on one vector but then when you are looking at other vectors, you're saying, oh my God to actually get this introduction and to get rid of all this pii and to comply with these Norms that we have set up, we would have to do so much extra work, it's not worth it for us know, and
 If it's easy enough to do then, you're probably gonna save some time in the long run. If you start from the beginning, just experimenting and building something like that yourself.
 And do you see this becoming?
 More common. Or is this just a maturity thing?
 No, I don't still think these Fender tools out there are probably gonna stay for a long time because, as always, people building prototypes, and you don't always need to be super compliant, you don't always need to be able to scale it indefinitely. So, do you think these Fender tools out there are perfectly fine, just not, if you're having aged in production and you need to be compliant, you need to scalable Etc. Let's change gears for a second and talk about how you're leveraging different coding tools. Are you using windsurf cursor? Devon, a little mix of all of them took on even. Yeah, we're doing a mix right now. So of course we're using our own tool but now since the IDE assistance, got so good. We all so used as quite a lot. So I use cursor. We use Devin.
 And we started using get up co-pilot now to do PR reviews. Yeah, for example, the pr reviews, it really took away that first step where you may be made in spelling error. We didn't get caught by a linter or you copy paste. It some Mongo adapter and he needed another Mongo adapter, you just copy paste the whole thing and then you forgot to change one little name and the co-pilot that gets a co-pilot's. PR Reviewer is pretty good at catching that we did try other ones out there as well. But we stopped using those because they were super chatty.
 And very confident. And now with the get-go pilot, you can collapses comments that things. It's not confident that that's the case. So yeah, really helps with like if I made an error, send it off to my colleague and I see. Okay, already said, okay, this is, I don't think this is right that saves a lot of time because it's a time that I asked to call it instantly goes back. Okay, just to comment there, I need to fix it and I need to wait again till he has time. So that's super nice to to have. So this coating tools.
 add to this piece that I find fascinating, which is
 you've been building a lot of
 tools for building agents like the eval tool or the orchestration tool you
 Do not go out there and buy something, but it's not because there's a lack of push from the team. I was talking to Paul and he was saying that he's really trying to encourage you guys to buy stuff. Mmm, despite that you come back and you say yeah, it's just not there yet.
 And there's this Vector, maybe that is worth exploring in your eyes on before. When you thought about building it yourself, it was with one scope in mind. But now that you are using these coding tools,
 Maybe you're a bit more ambitious, on being able to build it yourself. Do you look at it in that way, where you say, yeah, I can probably build this myself with the help of cursor and Windsor for whatever in a weekend. Definitely, definitely. And if something comes by, and I think I get, I get how it works. It's so much easier to have something like cursor. Indeed, it's explaining what I think it should do. And then also me and Kershaw, trying to find out how it works if curse. It wasn't there. It would have been so much work to try to copy something. You think is worth copying set of buying.
 Like, copying the functionality.
 Yeah.
 Yeah, really. Yeah, I really think that
 tools like, cursor, enable you to quickly build these these versions of existing tools that you think, okay, they're lacking something like this.
 We can probably do it better but if curse it wasn't there. It's super hard. It's you it takes way longer to experiment, for, for a very small feature. Definitely. Yeah.
 That's all we've got for today but the good news is there are 10 other episodes in this series that I'm doing with process deep diving into how they are approaching building AI products. You can check it out in the show notes, I leave a link and as always,
 you're on the next one.