I think that agents collapse on a simpler structure as the models get better. We actually had our first agent Frameworks all come out during that period and we got like Auto GPT and baby AGI and none of them worked and they sure enough to show promise then we got land. Grab Cloud. That's trying to become datadog and then we got AWS is probably doing both. I still am not sure what it means to be an agent in the cloud. One argument you could make against these agent Frameworks is that
 Here we go, let's go. This is particularly close to my heart because I started in Tech as an SDR and you are working on, making that job obsolete.
 Well, we don't have sdrs at 11x, so that's, that's true, really? Yeah, we actually we only have account Executives and other roles within the revenue award, but we don't have and their calendars are supposed to ours. Yeah. Wow. Yeah, yeah. So for context, I'm Tech lead and engineering manager at 11x. And 11x is a company that's building digital workers and our two workers today are Alice and Julian. They are both in the revenue or so, Alice is our AI SDR or sales development representative product. And then there's Julian. Julian is our, AI voice agent who actually specializes and inbound sales and speed to lead. If you're familiar with that concept and I have, you know, I'll copy off by saying I've got some voice AI experience at 11x, most of my role is focused on Alice but my previous startup was a voice AI company and talk a little bit about about object. If that yeah, Optical was YC summer 21.
 Company. We built one of the first vertical AI phone calling solutions for the healthcare market so we were
 Automating calls to insurance companies on behalf of healthcare providers, and provider groups. Mostly for verifying patient's Insurance eligibility, or for securing prior, authorization or for claims follow-up. So, you know, a lot of the operations that are relevant in the back office and billing department of these provider groups are actually, can only be done over the phone, it was gonna ask and they don't have a website for that. Yeah, they have website short story is that there, they're not that incentivized to make it very easy and one of the ways that they make it hard is by only allowing you to do certain things over the phone and we had been working in health insurance, medical billing for about a year and a half when chat should be T and the GPT 3.5 came out. And we had already had team members, who operations folks, who were doing these calls,
 For our customers as a sort of a service as component of our platform. And it was like immediately we realized we were going to be able to automate the calling to end using Ai and so we just pivoted a purely doing the voice AI solution for provider groups. But what happened, why aren't you still doing that? We I think we're a little early on the technology Technologies. I mean that's summer 21. Yeah and I actually interesting I watch the 11x team work through a lot of the same engineering challenges that we were working on while we were at HopCat kind of doing it from this, the sidelines. I mean, they're definitely taking things to a new level and they're Way Beyond where we were previously and way more sophisticated. Those guys are probably as sharp as it gets when it comes to building voice agents. But it's funny to see like the industry is still grappling with how to do that. Well and actually voice agents are really different than regular text based or multiple agents. I talked about this all the time. There's so it's a rich medium but
 They're when we type something out.
 if you and I both typed the same thing, it's
 pretty much understood what we're trying to say. Yeah, if we say the same thing,
 It can be that. I'm saying it with a certain tone, and I mean, one thing and you're saying it, and you mean something completely different. Yeah. And so how you pick that up with these voice? Agents is very difficult. Yeah, those inflections and those paralingual cues and obviously make things a lot harder and then doing that in real time. Yeah. With latency constraints is awesome. Really challenging. Yeah. So but so then let's let's move over to Alice and what you're doing there. Yeah. You probably know the SDR role better than I do. I'm an engineer. So yeah, I was founder previously and I've tried to do some outbound, which is why I have a little bit of appreciation for help paying for you on the other side of it, right? You probably get hit up all the time. Now it's all vendors coming out. Me and ironically, I'm, you know, I'm building like the Canon, right? Imagine they're using Alice to hit you up about that. I wonder if I could suss out or recognize an ALICE for an email. I, fortunately, I think they're like Dynamic enough that I wouldn't notice know you talk about.
 Alice being an agent.
 What does that mean for you? Why is it agentic as opposed to
 Just the program. Yeah, that's a really loaded question. I think getting to the definition of what an agent is and a hotly debated topic.
 I think that, you know, agents the characteristics that they have are they can automate complex and ambiguous tasks tasks that really were not. You could not automate previously, so they're sort of unlocks a new category of things that can be automated with software. They use tools, they have, sometimes they have memory and generally belt with one of these new agent Frameworks, whether it's land graph, or the new open AI agent, SDK, and effectively, there's a, at the center of the agent, there is a loop that involves a language model and in that Loop, the dhn is planning about what to do. And then taking action usually through a two call, then observing the results of that action on their environment and then reasoning about whether it should continue to execute and take another action or whether it's an execution and inside of Alice, this looks like
 I have this data, I'm sending an email or I'm scraping a website to find out more about this person and then I'm personalizing a message. And then I'm sending the email. Yeah, that's essentially, right. So the, this main campaign creation flow, which is the core part of what Alice provides is starts with sourcing. So helping you build a list of people that look like your ideal customer profile. We have an agent that specifically helps with building the Right audience. Then there's a research on the individual leads that are in your audience. So, we'll essentially have a deep research agent that like, creates a really comprehensive report on Demetrios and Ontario and anyone else who's enrolled in your campaign, using some of our data, and also using web scraping and web search tools. And then following research, there's sequins and message generation the sequences the different messages that occur or in the Outreach process for this leads. So you know, I'm going to contact Demetrios on day one day.
 Read a 7, this is something that agent decides. And then finally, the contents of each of those messages, you know, what am I going to say to Demetrios and each message? What's my follow-up gonna be? You know what value props am I going to emphasize? What personalization am I going to call it? I'm gonna say like, hey mlops I saw you guys. You recently broke well like a million views and one of your videos they call that and yeah, congrats on the woods say congrats on like breaking that Milestone. Yeah. I bet you have a need for, you know, more growth tools. Now that you reach this digital dude. You know it's so funny is back in the day when I was doing the SDR work, I thought it was magic. When a friend showed me a tool that gave you the tone that you should speak to somebody in when they had this list of folks. And so you could add your leads and then it would say oh yeah this person we know that judging by how they speak on the internet. You can talk to them in this.
 This type of tone. Interesting. Yeah. And now for me was like, Wow, incredible. Yeah, this is
 10 steps further than that because now I don't need to even know what kind of tone or what to think about and I can pull in relevant data on recent things that have come up and I don't have to craft it. Yeah. And I think one of the product challenges that we have is, you know, we're selling this into sales organizations. People like you who actually know what they're doing, some of them a lot. I mean, basically all of them have previously been sdrs have opinions about how these things should work.
 And a balance. We always need to strike is that we think that Alice
 can do this very well, Alice is great at sourcing. Great at writing messages, greater research knows how to pick the right. Tone knows what aspects of your product and business to emphasize when reaching out to this particular lead and sometimes the user thinks that they know best. And once to, you know, once the messaging to have a particular structure once it to only ever reference like this case, study wants to Target a specific type of lead, even though you may be, you know, we don't think that might be the best lead in terms of performance. So we have to strike this balance between providing control for the users and providing results. And the way that we think that we get results is by effectively putting on autopilot, and letting her do what she does best, and that's something comes up in our product conversation and with customers a lot. Know, we talked about the
 Reliability of this and you mentioned. All right. Well, we chose to Outsource the hosting of the agents in a way. What
 Were the trade-offs. How did you look at that decision of? We're going to go with and you're using Lane graph, right? Yeah. So you said we're going to use line graph
 Why, and how did you come to that decision? Yeah, that's a great question and controversial recently. Yes, I forgot that. And yeah, there's grant music, why didn't you? We're mostly typed from typescript shops. So probably not the right fit for us.
 But yeah, it's been hotly debated topic. The agent framework Wars recently. I'm sure your listeners know, like the blog post that Harrison published in response to openai's new agent framework and they're opposed to their guide to building agents through a little bit of shade on the idea of workflows and graph structures. Blank graph is the agent framework that has graphs. In its name, the name went all in on the graph. I did. Exactly. And actually into their credit, I think it's an extremely flexible structure like land graph. Yeah. And graphs, generally for representing the agent and you know, with line graph, everyone can build an agent that gets to production and I think that is in part possible because of the graph structure that they chose now, a lot of other agent Frameworks are much simpler, they model around chat and maybe are there. Pure agents in the sense that this is not a workflow. There are no predefined code paths between nodes
 they're really just one node and its looping through this recent act, observe Loop, you know, the react agent that people are from here with,
 and that's a super powerful structure. I think that as models get better.
 All agents start to collapse on this simpler form factor but the reality is like, you know, models have limitations today, prompts aren't perfect. We are evals are not that good.
 Like some things are expensive, you know, and you want to create structure in the graph that ensures that like a certain step performs. Well and a graph, a structure allows you to do that. Well yeah, and if you're going out and you're selling a product to folks and you're claiming that it's going to work, you can't have it works some of the time. Yeah. Right. And so a graph gives you much more reliability and you thinking about it from that system's angle, I'm sure that was one of the things that you realized right away. Was we need this to be reliable.
 Do we recently rebuilt Alice from scratch over like the last six or seven months we started in October of last year. So October 2024 Alice for context was launched in January of 2023. So a lot had changed. We actually had our first agent Frameworks all come out during that period and we got like Auto GPT and baby AGI and worked. And they sure enough to show promise you're like, wow, this is something cool here. And then the link chain people are pretty Forward Thinking so they launched land graph and really what happened for us as we saw the way the landscape was changing. And then we saw some of the new agentic products that were being released specifically, the replied agent left, a really big impression on our team. Are you familiar with the replication? Yeah, but I don't know why it would be different than like cursor wind survey agents. Well Devin replicate agent was the first of these tools of these coding assistants to really be a Gentech, you know, cursor had table which is the one of
 Their main feature, which would use the language model to predict where in the in your code you're going to jump to next. And what the next change that would be and you could sort of just press tab to yeah, to apply that they didn't have an agent. And in the platform is autocomplete, it was effectively really, really good autocomplete that made GitHub copilot look like a joke and to their credit, like all done on like custom language models. Some small language models involved, really impressive, work that they did. Meanwhile replied was the first one was really in my opinion. The first company to bring this AI coating agents that is looping through multiple steps calling tools to build an entire project, the agent is your co-pilot and it's actually helping you build the project and experience was unlike any product experience we'd ever seen before and they did this way before cursor composer, which is the, you know, the cursor agent, which is now the default mode with in cursor and so you
 Saw that and you thought that's where we need to go. Exactly. We, we saw that. We, we knew that landscape had changed products. AI products, we're gonna look completely different and it had to be agents. And like, we just basically made the leap based on that conviction. And it's interesting that you're reassessing, I imagine you have to reassess continuously because it moves so fast and the landscape and what you're doing is
 One-upping.
 All the time. Yeah, right. Yeah. Keeping up with models is really hard. Well, not just the models but like you're seeing okay now, we're not in a baby AGI World. We're in a lane Graf world, so we could potentially do what we're trying to do. Now a lot easier. So models are improving. I've mentioned that. I think that agents collapse on a simpler structure as the models get better.
 One argument you could make against these agent Frameworks or against more complex ones that allow for more complexly, like, landgraf is that they actually, they incentivize you to create structure and Scaffolding to accomplish short-term performance gains, right? Like I can add a few extra notes here and create a new Loop within my agent and that ensures that this step in my campaign creation flow. Now, consistently Works. What happens is that you do this for to expand the capabilities for your agent and a lot of places. Now, you have a very complex graph. Then a new model drops and you're like this new model I could have accomplished all of what I just accomplished.
 Using a new model. And it's much simpler structure I could have just waited. You could just wait a damn model came. And so you're doing this constant analysis of whether there's premature optimization you're like, is this is, am I making is this investment in My Graph structure or an engine into the framework of my agent gonna go to pay dividends before a new model? Comes out that renders it Obsolete. And then there's cleanup work to be done to simplify your agent afterwards, because it actually you what you do is you become sort of sclerotic where the agent is like locked into this structure, that no longer really makes sense and prohibits it from doing other things that you look at, as you're saying, hey, we need to be Forward Thinking
 In three months, we don't want to have to go and rebuild this from scratch. Yeah, that's a really good question and I imagine you can't tell the future. So obviously like I can't tell the future barely keep up with a present. We try to obviously stay on top of all the new model releases and at minimum we will you know when we get access to a new model, we'll have our team sort of Bosch it and and like regular chat clients, get a feel for how it behaves. Maybe use it in cursor, a little bit to see how it performs in a, in an agent Loop. We will drop it into our agent and see what the performance impact is. To be honest. It's a lot of Vibes today. We we have an agent that works and, you know, agentic product, that's pretty successful. Customers are happy with you just lock it in there. Then I wish we could we could just say, you know, mission complete and we're done. But we I think that there's a lot more that we want Alice to do and our our North Star metrics are revenue generated
 It for customers or pipeline generated for customers meetings booked, some of these are hard to do attribution for, by the way, positive replies to to any of the outbound messages and anything we can do to create lift in those metrics, justifies like the investment on the engineering side. So we're just kind of keeping an eye on those metrics and on some of the evals that we've set up on on the hnt and when we feel like there's a model that changes the game, we spend a little time investigating whether that is there any, it's a time where you've been creating in creation mode and you thought
 I could do this right now.
 But I think the next model drop is probably gonna make this all not worth my time to invest in it. Yeah, that's
 that is a good question.
 Or we could do it retroactively and say what did change in the last Model drop? Yeah.
 We're still mostly on three, five and three seven. Sonnet
 We've obviously got access to some newer models, from openai sense.

I don't think that we've seen like a big enough performance left from those models. There's also a cost and latency consideration. So it's a pretty complicated analysis at any given time to understand whether
 We should upgrade. We need to know.
 Is the performance. First, we got all these lagging performance indicators. It's so it's hard to even know what the performance lift is. And once we know what the performance lift is, how does that relate to the to the increasing cost? And whether what does it mean for products or end user? Excuse me in product user experience? When you're using that with the agent or interacting with that? Does it feel slow when it's generating those messages? And meanwhile on the background we are doing like, 100,000 of emails per month. What's our throughput? What's the speed impact, their
 so,
 A wish we could say we're more systematic about it, maybe the best thing for like the mlops community to say here is that we don't have it down to a science and it's a lot of vibe decision making there. Yeah, it does. It does feel like
 You're not the only one. Yeah I'm pretty sure. I know the industry right now, we're not researchers either, you know, we like we
 I'm pretty proud of what we built with Alice and how, you know, how far we are as a company and with 11x but we're really focused on commercializing AI. And so we were happy to to rely on the, on the labs and on the other researchers to in the benchmarks to help guide us. But we're very pragmatic and focused on creating a better product. And so that guys, a lot of our decision making. What did
 You decide on when it comes to Lane graph again.
 You told me that it's hosted. What does that even mean? Because I'm not sure I'm clear with a hosted agent versus what. Yeah and why?
 That there's a lot of places we could go with that. So, just the main decision-making criteria around, landgraf, specifically, September of last year, landgraf is one of the best and most mature options for an agent framework. And you can argue whether that's still true. But at the time, certainly it was and there was those guys are leader in the devtools and an agent. So we really wanted to partner with like a team that could educate us and that was leading the way and had solid, adoption wasn't gonna disappear in six months, like many agent Frameworks, that's their job and probably, well, yeah, you have a job for that. That's a great.
 Axiom to be thinking about, yeah, yeah, we use a length change products, and a lot of ways we use their land graph agent framework. We use land graph Cloud, which I think is now called langra, landgraf platform. This is their Cloud hosting solution for AI agents. We use Lange Smith for observability which is coupled with landgraf Cloud. That gives us visibility into the actual agent runs.
 And then we use some of their other sdks within the agent itself. I still am. Not sure what it means to be an agent in the cloud. Yeah, well we could break down like, what the agent actually is. So, we talked about like the agent is this Loop, right? It's really an llm and a loop and the llm is saying is, is deciding what the reasoning about its state and its environment. So give in like some input object, which represents the state of the world and a prompt which explains who it is and what its objectives are it decides, or it reasons about what action, it should take. Then one of the actions, many of the actions. Typically are tools that I can call. So if a call a tool then you have something that executes, the tool. Now the llm is an executing the tool, it's just saying I would like to call this tool and then you've got maybe it's some case statement that says, if llm return tool call get tool called type.
 And based on whatever type was called you would then invoke a function which is implemented in the in the code or maybe a more modern way to do this is with mCP. Yeah. Have they have access to it? You had to go and upgrade shit because we're not. We're I personally love MCPE very interested in it. Have built some side projects, we're not using MCPE and production today. Yeah, I understandable. It's a very new technology. This would probably not be like the most pragmatic thing to do. But super interesting and things does solve a lot of problems, related tools, we can get into that. So then it calls the tool, so it calls the tool. Something executes, this tool, it could be the same. Let's call it a node process because we run typescript. So our agent implemented in know, and this node, processes running a calls the tool, as a function, returns some results. Then then you update your state to reflect this the new state of the world now that the function has been called and then you repeat.
 The loop and eventually, the llm says I either stuck, you need to and should exit the process or I've completed the task and I should exit. And so that whole process is done on the link chain Cloud. Yes. So we use. So link chain provides this agent framework length, which was some create, some nice abstractions for defining the agent that I just described, and what that ends up being is a, and a node or a JavaScript bundle that runs with node in a container on Lang chain servers or Lane Graf servers. And there's a there are two things that sit in front of us that link chain also provides or Lane graph. Also provides the first is a is an API that allows us to call our agent from either our backend or our front end. And we do just interacting with an API, exactly right agent. Becomes an API endpoint for us and as a side note, link chain.
 Is also created this protocol called the agent protocol, which is like a set of guidelines for the for how that API should be structured. What are the different endpoints that it should have? What are the different entities and resources that it should expose, which HTTP methods should be used? What are the payloads this was pre mCP? It's not. It's not for Tool calling. It's really for like exposing your agent. Yeah. If I want to deploy my agent as an API and have, I mean, Alice is a constellation of Agents. But if we want to take Alice and turn her into an API endpoint that you could use and using our dashboard, then one of the ways we could do it is by implementing an API that adheres to this agent protocol, that Liang has defined, so they have this API.
 In the API, sits in front of a queue. Basically, every time I call this API endpoint, I think an example endpoint would be like, runs creates, like post runs create now, I'm creating an agent run. There's right at the front of the queue. Something is holding my connection. Actually depends on whether I'm streaming, whether I want to stream the results back or whether I just want to fire off the run a simpler example. As if I just want to fire off the run, I don't care about streaming it back. I call this endpoints, the my past, the parameters like the whatever the agent is that I like to call whatever the initial state is. And then a, you know, a fanciest Place onto this queue. The there's a consumer at the other end of this can queue that sits in front of the node process. And for every queue event, that is received. It starts the node process and passes in that state. And so, the nice thing about having, that cue there, is that, if my
 If I haven't like totally scaled or if Lang chain, in this case, hasn't scaled up all of the agents so that it can handle Peak load. None of the none of my agent execution, requests, get dropped or lost. They will all eventually get processed. Yeah, just slower but just be slower. Yeah. And you can scale the queue or langshan provides some ways for you to scale the cue. If you are hosting this yourself, you could also have you do some autoscaling or well, let's talk a bit about that because I guess one of the huge value props here is that you don't have to deal with that scaling issue. You don't have to worry about, so nice. Yeah. And
 But what are the trade-offs? There's always gonna be something that you're
 Well, always is there's no there's no free lunch.
 Yeah, there are trade-offs I mean so this is like a lot of infrastructure. We didn't have to build. Yeah, and it's infrastructure, we could have built but you're with Alice is also like a SAS tool. I mean, there's the agent, but then there's the dashboard. And then there's, yeah, we have other background processes that are happening. So, we're building a whole application. And then we're building this agent that needs to hook into the application, and you rebuild it in October when you already had something out, you can't just turn around and spend a year and a half building out the infrastructure. I imagine so that you can get this agent going. Oh, yeah, there's a whole meta here, which is that our customer base was growing or old product was starting to crumble. You're going fast, we're going to go fast. Go to market was not slowing down for us. We had to get this thing out really quickly. Yeah. And it was it was quite the crunch. It was super painful but we're on the other side of it now so I can laugh about it. Somebody I heard somebody talk about that being type 2 fun. Yeah. Yeah, that's exactly right. It was type 2 fun. It was a lot of late nights.
 My skin looks a lot better than it did in March or whatever February or something. You got some sun. Thank you. Yeah. Not as pasty. I remember the first time that I met a little more Pasty.
 The let's say the trade-offs. I mean, so it's super advantageous for us to do this just to get off the ground really quickly and to not have to worry about building that infrastructure.
 The I think some of the trade-offs were, you know, when we started using Lane Graf Cloud, it was a newer products. We did run into some scaling issues at times, there were some limits on the extent to which they could scale and individual, what they call it deployments. So in fact, we actually had to create lots of deployments those cues or those. You think about this is like you know if you're deploying a service and Amazon you like deploy it you see. Yes service. If you are deploying an agent and Lane graph, you created a deployment and we had a lot of volume, we had spiky volume and that led to us running into, you know, connection timeouts during those Peaks. And
 One of the things we could do to make this make, this better is to smoother or request traffic, which we have done. And that's we take full responsibility for our query patterns. But we were also, you know, the we're exceeding, some of the limits of what they could, they could the extent to which they could scale an individual deployment. So the way we got around with this is we created 10 different deployments and then we had all of our clients is round robin and across them. And now it's been fixed. So we're Consolidated again. And then other challenges, we have a deployment pipeline, that deploys, all of our backend services. Our code is stored in a mono repo, including the agent itself.
 So usually at any given time, the main, whatever the head of Maine in our mono repo is represents the state of the world. What should be in production and our deployment system works for all of our backend Services. You know, we've got a separate set of infrastructure now for our agents. So if we want to also Auto deploy an auto roll back our agents, there's all of this additional deployment infrastructure. We need to create to support that and our deployment system gets way, more complex and then last thing that comes to mind is on the observability front. You know, agents are software, just like everything else and we want to see the logs, we want to see the traces and we typically pipe all of our observability. Telemetry logs traces metrics to data, dog. And this is something laying. Which, what's the service laying language? Yeah, and this is another small, small complaint about using link chain.
 Ecosystem is that they have link chain link Smith, everything like graph the year of the land is the year. It's no year of the landing men. There's more. Yeah. Ironic because it's hard to remember. Yeah. And it's all about memory and then there's two sets of
 Two sets of documentation for both Python and typescript. Oh, so you're like, am I on the right documentation page right now? Painful. Yeah, we've gotten better at navigating that I think they've gotten better at positioning there and figuring the products. Yeah, call something different. Yeah, the links have been exhausted. Yeah, that's the get the calling card. We on the observability. Side Lang Smith offers a lot out of the box which is great. But
 I'm a former infrastructure and observability engineer, like one of the core tenants of that role is like, all of the Telemetry should be in one place. So that when something's going wrong, it, you know, don't have to search. You don't have to go looking for it. And if you're not, if you're a product engineer who doesn't know what the whole observability stock is, you can just like, jump into that tool, and it's got everything. All of the logs are there. Yeah. You don't need to remember, you know this, there's a customer that's complaining or there's an outage or oh, there's this is everything except for that, except for that one thing, which by the way is where the bucket is. All right? So getting some of our logs and other Telemetry and to data dog from Lang Smith, what is was not that easy and still a little a little bit painful. I think I know that they're working on better Integrations with data to make that and make those pipes that feel that seems
 Relatively trivial I guess it's a huge problem for you, but it's like there's nothing that's blocking them from making that easier process in total like six months, you know. It depends on how where you think these agent framework companies are going. Oh yeah. I think sort of there's an incentive for Lang chain to own everything to I mean they want to lock you into their platform and they're all of their agent Frameworks are open source. A lot of their other tooling is open source. So the the value the way that they capture value is by getting them by using their hosting solution and by you using their observability solution. So making the pipes easier, so that I can get to datadog, may not be like number one priority, but they do want to make their observability tool really good too. So it kind of ends up becoming a question of learning too. Yeah. Do you think that agents are different enough to require a different cloud?
 Will we, will we end up using dedicated clouds for agents? Or will we just deploy agents? Like we deploy other software to existing clouds? Yeah.
 Is it complex enough to Warrant its own cloud?

I think that's
 I'm not sure about that, I think.
 We talked about the infrastructure pieces that are required to host an agent. I think that you
 You can imagine all of Open Source versions of all of those or a single open source solution that combines those components, something like an elasticsearch or a Kafka that is, you know, everyone knows how to work and everyone knows how to deploy and AWS is a one. Click deploy for
 So I don't think that that's out of the realm of. I'm surprised AWS. Hasn't stolen Lane graph actually and run it as a service on. Hey, tell me why is now that you say that they have they have their own agent framework. Yeah. And they have AWS Bedrock, they have better Arc agents but their agent framework. It's one of those ones where it was older, right? And I don't know anybody who's using that, I don't know anyone who's using Bedrock agents. No offense to the to the Bedrock team doing great work. We use Bedrock for inference. Better Rock has a has a partnership with anthropic where they can host some anthropic models. So, but why Bedrock as opposed to just going to anthropic because of the reliability? We originally switched from from anthropic to bedrock because of rate limiting, we couldn't get a high enough rate limits through anthropic. You gotta know, someone, he gotta have an angel and so Bedrock gave us heart rate limits. Now, I think there's a difference in cost between the two platforms and
 So, this is like, where you get your procurement team involved because sure you don't want to give us these better rate limit. Yeah, well, that's a great point. So then,

oh, it's just a ghost, yeah, just
 Bidding for this room. It's super cool space. Yeah, it's cool thought about getting a bunch of cigars. Oh my God. That would be cool. But I wouldn't be able to like step out of work to come to the podcast. Yeah. No we don't have to smoke them. We just chew on them and look cool with them. Just have one like smoking and I try. Yeah. But they would be very excited about us doing that. We do it after hours thing. Yeah. Well, what else, man? There's a few other things, right? Like, I like the trade-offs, with laying graph and hosting an agent there, laying Cloud link graph Cloud. They missed an opportunity to confuse the shit out of people more. Yeah, and just call it Lane Cloud. I think. Lane Cloud would have worked just fine. That's the next iteration. You actually, you know what, I would love to talk about. Is you, as a
 Observability guy. How do you see observability being different now with agents if at all,
 And it can be total of you. If you say nothing. It's same, same, it's still software. No, no, I think
 this is a rich question. We might have to not noodle on it a little bit.
 Yeah, so my background and observability is I I was I was second infrastructure engineer at Brax and had like an amazing Mentor who grew me a lot of practice. Built a lot of cool production infrastructure there and then at some point established, the observability team at Braxton. So we had a couple of folks who are specifically focused on developer tools related, to instance, and understanding, how systems are behaving and production in practice. That's like we set up data dog. We set up Auto instrumentation for all of our services. We had like 30 or 40 different microservices running in our kubernetes cluster, all of them are sort of uniformly and automatically instrumented. We had instant process and and tool that you would use for, for kicking those off and for resolving them. And, and some other ability tools as well for things like fraud detection.
 And a client side. Observability is actually interesting area of area too.
 The means like like capturing errors that occur on the client as a post on the server.
 Things that, you know, for example, there's like exception that occurs in your, in your web app and its happens in the user's browser.
 How do you send that back to a server somewhere so that someone can look at it? Someone knows that something went wrong. Yeah. There are a lot of tools for doing this today. Like I think log rocket was one of the first ones. Post, hog has a session recording feature. It's pretty cool with all the browser. Dom events you can reconstruct exactly what happened on the on their experience. So it's they're not actually recording. They're your screen but they might as well be because you look at the recording as it's the exact same thing that happened.
 I think.
 Observability with agents can be really interesting. One is, I think that just agents
 So there's sort of two separate things we can talk about here. One is like, what does it mean, how do we observe agents as we more of our production systems become agents? Like, how do we keep track of what's happening and make sure that they're performing reliably and, and reconstruct things after the fact? And then the other one is like what is observability for just regular software. Look like when a world of AI agents to tackle the latter one. I think agents like have a lot of potential to automate the sort of production and infrastructure or site reliability process. I mean, so much of what I do in my day today is
 Someone says there's an issue. You know, customer reports an issue or customer success, person reports an issue, I get like an organization ID, maybe a URL, and then I can roughly, I guess it happened within the last hour or something like that. Maybe I get a loom or a session recorder. That's crazy. Yeah. Um, and then I take the, whatever piece of it, whatever Clues, I've got to take them to data dog, or to another observability tool and or maybe our, you know, I can query our production access to our database. So if I can look up a record, if it's seems like a record, might be in a bad data State. And I have an identifier, I can query for that in our postgres database and data dog. I can plug in like a trace ID, or that organization ID and get the logs related to that request org and and I start piecing together. What you probably asking people on slack or it's funny that you mentioned in this because we had Willam in here yesterday and he's working on AI SDR? Yeah, no. Sorry.
 That's what you're working on. He's working on. AI SRE and I built one over there. Yeah. No. I want to hack a call building it. Yeah, that's awesome. So funny he's so he's doing that he built a knowledge graph on.
 All the stuff that's happening in slack. All the stuff that's happening in the logs, all the stuff that's happening in your environment and how do you connect all of that and know what's valuable tell the agent. Sometimes he was saying that the agent will just go and do its thing and come back with nothing. But it will tell you, hey, I searched these places and so then you don't have to go and search. Yeah, that's yeah. You have to really trust the agents doing its job. Well especially if it comes back with nothing. Yeah. But you're like are you sure about that? I don't believe you. Yeah, I'm gonna go check. It wasn't like our agents just they want to please, you know, language models want to please. Yeah, that's probably rare that they come back with nothing products like that. Have a lot of potential.
 It's a it's a it's a clear set of tools that you would integrate with clear set of data sources and you're just correlating this commit with this error log line with you know this bad record in the database over and over again. And then summarizing, what happens, creating a linear ticket out of it. Reporting your responding to either the customer or the internal user and agents can can automate a lot of that process. So we see them automating like the code, the process of writing the code and shipping at. Yeah, we don't really see them automating the production. What's happening in production? Yeah. Root cause analysis of what's going on. Why did that fail? And and even preemptively saying, you sure you want to put that kind of scale on that alone. Yeah, that's another piece. Actually a code review, which is habit somewhere in between those two things and there are some cool tools there. I know someone there are lhotsky from from graphite is a one of the leading AI code review tools and we use graphite and love it. Pretty consistently catches things.
 the balance you need to strike their from a product perspective is
 how do you make sure that you are catching things and providing useful suggestions without being noises because nothing does hate more than the noise that alert fatigue. And I know a friend that plugged in an llm to their GitHub instance. And then they turn it off like six hours later. They're like, this is not gonna work. Yeah, yeah. So really recognizing what's valuable and what's not? And that's a huge thing. Now when it comes through the observability of the agency. Yeah. Yeah, I think
 So I think that agents are kind of complex and scary and new. It's important to remember that agents are just like software. They're just regular software programs and so observability in a lot of ways is the same. You know, you should you should be instrumenting them with metrics logs and traces. You can do this using open source libraries, we'll link chain and Link graph. Have a lot of this stuff out of the box. If you use that, you get that in like Smith. But open Telemetry is the standard that has been used for most software monitoring tools thus far and you can use a lot of those tools to instrument your your agent. So, yeah, depending on the agent framework that you use, they might already have hooks for like admitting open Telemetry compliant traces at each step or or similar things for for metrics and logging. Otherwise, you might have to do a little bit of your own build, a little bit of your own middleware, or maybe raw page and framework. So that it
 Instrument set, but all of those tools work just as well. Now, if you use a Cloud solution, like land graph, you are restricted in terms of what other things processes you can run alongside your engine. And that is something that you should consider when you're thinking about observability for the agent. Because, for example, a very normal way to collect metrics logs, and traces with traditional software, would be to have a sidecar container that runs like a an agent process like a stats D agent or dated dog has a dog statsd. I think, which is their flavor. You can't do that in a cloud environment that you don't have a lot of flexibility or control over and just so you might only be able to do that in something like AWS or gcp or Azure. So
 something to consider when you consider, when you're thinking about where to host your agent is what options does this? Give me in terms of observability, getting the, those just metrics out of restricts, your architecture decisions. Like you can't just go and have a blank canvas. Yeah, you go and trawl in the lines. Yeah. And none of the agent clouds are as sophisticated as though as the big clouds. So are you linking up? These observability metrics with the evals or are you fully separating them or they kind of mixed and matched. How do you look at those? Yeah, evals
 I think a lot of companies most companies are not doing emails today, everyone's interested in evals, but it's all Vibes, but it's all vibes.
 There's a lot. I mean a lot of vendors are talking about how they have support for evals usually those features kind of take the same shape. It's like you can create a data set from some of your production traces and then you can run your agent in times for in items in the data set and it will tell you. Oh each time you run your agent it will run a set of heuristics against the output and for each of those who are you get like a score or a label? So it common one is to detect hallucinate hallucinations. We actually use this and our message writer. So we have an eval that actually automatically runs on some percentage of production. Oh, nice traces for the agent that generates the massages and this this is all so done with laying Chung. This one is done we've implemented and Link Chain offers this capability. This one we just we were trying out another tool called a rise which is another
 LM, or AI observability tool. And they also have support for evals. And so this one we implemented using a rise, I don't strong opinion or the oh this this is using their core platform. Okay. Although Phoenix is really interesting and I think
 I've been interested in Phoenix for a while as a way to deploy sort of like a production grade observability tool locally for all of my developers on my team. So you know, we're running these agents we can run our entire application locally using a single command, which is Sweden, starts a bunch of services starts at the client starts at the agent server, which is built online graph. Yeah.
 What would be really nice is to also start up this Dev server, that allows you to view all of the agent traces that are happening on the, on your local agent and maybe even iterate on some of the prompts through that server. I think Phoenix might allow us to do something like nice, okay, maybe a much better local development experience? Yeah, building the agent. All right so sorry I got you distracted with evals and production of the main one we have running or one of the ones we have running today is detecting hallucinations and messages. Obviously super awkward. If you say like, hey Demetrius congrats on passing 10,000 subscribers and you're like, it was a million. It was my ego is hurt. Exactly. Actually on my generator spots? Yeah. Not the kind of response you want, probably. But actually I was telling you this earlier like I imagine a lot of the stuff that AI generates or Alice generates specifically is a lot better than the human stuff where I was saying
 Random shit in my that's so yeah. We and we get scrutinized a lot on our messages too because you know users will log in and they'll be like you know I saw this slightly weird message that you sent to one Prospect and maybe the prospects LinkedIn says they work at like into it and our data set, you know, our email says like Intuit accelerate or something like that because yeah. So it's a little bit off. Yeah. It's not a little off or it's like when you're spelling, is it dessert or desert? And like yeah, it's the same. When you look at it and you don't know how to spell like me. And so you're like yeah I said dessert, right? Yeah. There's one other thing that's really funny about this is, we often
 Ah we try to show sources in our in the research alongside the message that we generate when we do it in the product and oftentimes we'll have customers say like why just say like we you know, maybe the message says we helped a similar client and the healthcare services industry. Yeah. Save, you know, 50,000 dollars in the last year, using this solution and our customer will say, where'd you get that? Like doesn't sound true and then we pull up the research and sure enough there's a case study their website that says that so many. Yeah, it's, there's a lot of that, that happens in the day-to-day and it can be really delightful when it does happen, but we have also, you know, we're held to high standard for that messaging. I think certain customers are more Savvy when it comes to sales and understand that a lot of times, sales reps will bend the truth or or say things to provoke a response. And also can do that. If you want her to you, you can nudge her there. Yes.
 We effectively have sassy mode if you want to it enable that we call it brand voice. Oh, interesting. But but also, you know, we want to, we generally like want to be factual everything that Alice should say, should be based in fact, and it's, it's a product design problem, to make sure that the, the sources are alongside the messaging so that you can correlate, you know, if a user wants to double check, something they can do that usually. So then back to the evals you're saying, all right. We are doing emails. With Lang Smith were also doing emails, just kind of on Vibes and figuring it out as it's coming out.
 And you have this.
 Cycle of whatever 20% did you say? So. And so we do this in a rise right now and turns out on which, you know, we can talk about which of the eval platforms. I like better. But interesting, so you're doing it, both with links and or just link Smith only do observability Lang Smith has eval offering and we've used an earlier version of it. We don't use it right now and our team has been experimenting with arise, which is another element observability platform and they have also an evil offering, which is playing around. That's fascinating. So you have
 Evals in Lane Smith, but you're like and we're just gonna stick to the observability piece of it.
 We we tried an earlier version of it and we weren't satisfied. Felt like the experience of studying up was a little too hard and, you know, we, our team is very curious, about AI tooling. Yeah. And we're always kind of shopping around for whatever the latest and greatest hits. So we test drive, a lot of products and for evals right now we're test driving or us very cool. This eval we sample like 1% of our total
 Yeah, of we have too much volume for because we were running an llm in our eval.
 So LMS, a judge. And so that could be expensive if we ran it for 100% of our traces, we only do it on like 1%. But what's cool is it gives us this this number between 0 and 1, which represents, you know that the average across all of the traces in a period of whether they have hallucinated or not. So as we, if that number is close to 1 means like we're not hallucinating the numbers close to zero. It means we are generally hallucinating
 Exactly. Yeah, you're burning that, right? This SDR is running emails from their satellite connection, you know, from deeply. Yeah. So that said that's what we, that's a number. We watched to sort of as a Bellwether of whether or not like a recent change to, the prompt has made it less reliable or something like that. And what the research or what the evalu is doing, is it's comparing the inputs, which include this research report, which we've assembled and the outputs, which is, which are the email that we've written, basically saying, hey, is there anything in the email that doesn't match the research, okay? I have a very clear picture of how you're running your agents. It's your hosting. It with Lane Graf, you've got language Smith as the observability. You're exposing an API in a way for that agent that you can then build services off of you are relaying whatever that output is. You're taking one percent of that and feeding it into a
 rise right now and saying we've got these evals and you're matching it up with
 this is what the research says and this is what you said.
 How accurate is it, right? That's effectively how the evolves. Yeah, we send send all of our traces to arise the same way that we send them to Lang Smith. These are the agent traces. And, you know, we run a bunch of agents for Alice, one of these agents is the message writer and the input for the message. Writer is the research report, some demographic info, and the output is the message. And so, for the traces that are message writer traces, we for 1% of those we run this hallucination, eval wait but then what is because you've got data dog, right? And dated dog is your observability like the system observability. What is linked? Smith doing that link Smith. So some of these things are duplicated, I mean, we have a lot of metrics and traces too or the agent for the only what's deployed in line graph Cloud. I see, which is our agents now. It's nice to get all of that data into language or into data log impossible because that's where
 it is for all the services. Yeah, I remember you saying. No, that makes a lot of sense because it's like data, dog. Can't cover what's happening in links laying Cloud. Exactly. I mean, there's two walled Gardens. Yeah. And interesting. I think the length trained team. I don't, I don't want to publicly promise anything on behalf of them, but I'm pretty sure that they're working on some kind of integration that allows that type more of our data to learn today, but they're not going to let you put a data, dog. Instance in Lane cloud.
 I don't think that they're ambition is to become a general observability platform. Yeah. Like to have our backend services and their logs to language that would that's probably outside of their scope. Yeah. Yeah. That makes sense. Yeah. So then why would you want data dog in there? It would be better if you just took all of that from
 Lane graph. And just said, hey, we're throwing all the lane graph traces, and whatever, whatever your cloud is doing. We're thrown it into dated dog. Yeah. And that, I mean, that would leave them their value out. But then be like running the actual agent hosting that API and cue that sit in front of the agents, which as we talked about earlier last. Yeah, may or may not be needed. Yeah, so everyone's being squeezed right now, you got dated dog which is Big trying to become landgraf Cloud without the hosting, I guess. And then we got Lang's trying to become dated dog and then we got AWS is probably doing both and so they're all kind of trying to carve out more of the Rolling infrastructure that needs to surround the agents and own it, but there's definitely a lot of overlap between these different levels. Yeah and then you have a rise that's doing the Horizon, sort of a lang Smith competitor. Exactly. And they're really focused on llm observability specifically. So just
 More of a better specifically with Lang Smith and datadog although datadog has their own llm, observability Suite. Yeah but arises an API that you're hitting or you're sending data to it's like a cloud a hosted version. Also arises open Telemetry compliant. So oh so yeah. Okay, in the city, you know, we can just instrument our agencies, using some open source, libraries and all of the data flows into a rise, interesting. Yeah dude. Listen, they're awesome man. This is I hate you being so forthcoming. We'll see if your PR team allows half of the shit that we just talked about to get out and to the world. The technical stuff is right? Yeah. Oh, we could have gone on for longer, I think. Yeah, I think we can. Well, we'll continue the conversation tonight. I'm