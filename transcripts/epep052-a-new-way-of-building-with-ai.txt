But assume that the AI was the main driver of producing and maintaining software. While she designed, we actually asked the eye model. You know, if you wanted to add a table or should do and AI model like yeah, I like to have a function that at Sables. We're starting to see more understanding and the ecosystem of how to connect AI to software. Now, a lot of sandboxes, they are usually like Jupiter style kernels. They run they disappear and it's like, oh very far more. Like the status quo in the variables are gone, it's not great because that's not how we interact with each other's systems. We try to do it. We come back to many hours later and keep going again. Oh, we go take a nap or wake up in the morning. I'm gonna make coffee like going in like, oh, that's terrible.
 even though there's so much activity,
 It was hard to figure out what's working. Yeah. But then That's so exciting. I guess, it's like almost the early days of, you know, just new technology, people adopting it, you know, getting it out there. And so I think for us what we are seeing at least from our users out point of view has been, you know, this really kind of people just wanted to work a lot of AI that's really fancy. Generates really cool things. But end of the day what we are seeing is that I just wanted to do some work for me know, parse my emails, read my invoices, get my air table in shape, you know, stuff like that, right? And sometimes the
 even though we are seeing all this amazing fancy things out there,
 I think the basics actually important, like, can it do the basic day-to-day operations that we care about, right? And I think that's, that's not what we're seeing. Like people just want things that work. And I think I think that's kind of like come back to the ground. Again of like this height. That's reality. Let's bring that together and expectations are aligned. Yeah, because you can get into trouble if you're just going off of the hype and yes, then you're promising the world and not delivering on much. Exactly, exactly that. There's all these different potential use cases, but the ones that actually work are human far between. Have you seen some
 Pretty common use cases that are reliably. Well, so I do try, what we do is that we are building a TI agent that completes tasks across your apps and also be like this January agent that works, you know, understands how to work across different software, use emails, CRM, spreadsheets. So on, we're seeing we're starting to see actually, a lot of people understand how to use it and get a lot of success with it. And I think the nature of the task that we normally see working all those that you know, yes data and one system, you could pull it out, you go to transform it in some way using AI normally and they go to put it back in a different system. For example, get my invoices that are coming through my emails, pull out the different 99 items, get it into my accounting software or expressing working with QuickBooks not very and it turns out that, you know, it's a thing about that. That makes a lot of sense because our software services are separate and silos, right? They don't work well together but it turns out that the AI has the ability to Now understand how to interact with each.
 Piece of software we work with and be able to glue them together, right? So, besides to see that that kind of work that kind of process work really well. So, the thing I just described is actually a process that works really well on Nutri. But I think as we start to, you know, but I think what happened to see is two things in there. I think one is, that model is actually the current generation of generational models have caught in a really good at understanding software.
 So code generation cursor, wind Services a lot. But at the same time that ability to produce software can now be something that everyday business users can harness to automate work. And number two, I think it's that we're starting to see, you know, more understanding and the ecosystem of how to connect AI to software, you know. Like so we have been working on this for a long time and I think at the core of it is figuring out what is the right AI computer interface is. No, I don't I don't think it's going to be the point like I don't I don't think there's gonna be the raw apis. It has something in between that is abstracted away for the AI to work with you know, so MTP things like this coming to bear. So I think when you have all those pieces start to come together and click, you know, things are starting to work quite well, dude, it's funny. You mentioned the
 ETL process. But now for AI, I'm sure you thought about that. You realize like, oh, it's just the same ETL stuff we've been doing in data engineering, but now we have more ability to decide what we're extracting and what we're transforming. And then where we're going with that, maybe give an example for like some fascinating use cases and down the lines. So one of the things that we found is that
 The challenges that's your point, right? We used to be all stuck in tabular data to us. You need to have a schema database around it, but a lot of real world data is not tabular. It's not in the schema like that. So here's one example of a anti-wind process that we see some users trying to do, which is I have a sheet of people sign ups, you know, maybe in an error table or Google sheet, there's some missing data in there, you know, some of the form completely with, you know, maybe the name their addresses. So, yeah, you know, budget for something. You know, and then I got to email this 100 people say give me that data point and actually, you didn't put in to your sign up form, this cannot be no and now is the ETR process. Of course, what you do is that you read the sheet, then you produce emails drops for each of the individuals, based on, how are they filled? And they send it out. Now, then he replied. Now, the different ETL process, you take the emails that they replied to you extract out using AI, the data that they're giving you.
 now, and if it back into the spreadsheet and narrow table where you need it,
 Now those actually both ETL processes but if you think about it, looks actually real work processes that we do every day. You know, getting some form of a table of data missing data you follow up. Yeah, exactly. So stay fascinating that you start thinking that concept and saying because AI understands how to work with different applications. So you don't have to program and which API is to call is a share the tools, you can use and number two because it can take that unstructured data from like email content and get the right piece of information out. Yeah, you know, you can set you really eating things, you know, extracting know extracting being, you know, figure out who's the tools transformed being AI, transform the data and then shove it into a different system, like I made that verb. We are ET yelling, a whole lot of stuff. Now, you're not confined anymore. How do you think about integration?
 Makes sense.
 so,
 The way I think about the integration is actually what is the right level of abstraction actually to give an AI, so that it can be successful. It's going to. So, I think a lot of Integrations that we have today, apis were not designed for AI, they're designed for developers. Now here, all the error codes. If you do read it, you know, program it deterministically to do all of these things. You know, and then for an AI model, sometimes that doesn't make sense. So give it to maybe two examples in here. One is that we worked a lot with the Google Docs API because people want to create documents and edit them at the, you know, is AI to write documents for yourself. It turns out that there's a way to get the Google Doc API to add a table.
 Into the document.
 Now if you can figure it out, kudos to you because it was impossible to figure out a lot of thinking, and what he had to do was that, you know, create a table and then kind of like backwards compute how to fill in every cell and it's like insane, you know, like, you know, it's possible. But you need to like, sit down really work through the details of that. Yeah. And any AI model is going to struggle with that. So the abstraction that we want to get to for an action, like that would be one level higher at a table to the document and behind the scenes, we as Engineers developers figure out what all the garni details and doing it. Right? And so, as we doing Integrations is really thinking about that, right, why? Because that's just one use case, right? When you say, and
 There's Millions this Millions. There's millions of Google docs with adding a table instances that you have to backwards engineer. Yes. So it sounds like that. I don't think it's so it look at the level of getting different outside structure. I don't think you're gonna go all the way to such fine detail. Like we need to support this table that table we just about tables, right? So it's a bit one level higher than the raw apis but not so far abstracted that. It only does one task. All right? So just one of the level would get to the the thing about that is that the way we think about it actually, is, like, kind of in a kind of funny way that instead of setting down and designing and API for the AI, we actually asked the AI model if you were to wanted to if the you know LM you know, if you want to add a table or should you then a model? Like yeah, I like to have a function that at Stables. Okay, great. Let's do that. You know? So almost observing like, if we give it a set of Integrations and actions, what are the inclinations of the model? What
 Does it prefer to do? And then we kind of actually adapt the way we design the
 Actions to what the model prefers. So we actually designing around AI preferences, which is a very different thing to do. One of very one, surprisingly non-trivial thing, scratch sheet filling.
 Yeah. Okay. I'm very common tasks. I have a sheet. I need to fill in data for every role. Maybe do some research on the web Fullerton. There are many ways to go about doing that. What is the right abstraction to give an AI model to say? This is the way to update a row. Do you give it just the function to say update a cell with coordinates gonna say maybe? Yeah, turns out, if you do that, you're gonna get off by one errors, it's going to be like, oh I think there's a header roll, plus one, everything. Oh, no. It's not. If you give it an abstraction in, which I'm gonna give you the input data, that represents a row. And you tell me the columns to update based on the, you know, header as keys.
 Now it's gonna work no more off by one errors, no more issues in there. So once you did, once you figure that out, for example, then the arrows are the models. Make go away because sometimes it's impossible to be the same Iris.
 And then you can see like, you know, there's a model like this in some ways, right? Because you give them. All right. This is a way to update. Is it getting it right? Or is it trying to do something else?
 And this makes me think about is how the
 you're giving.
 I always think about trying to give the llm, the least amount of scope is possible and really directing it and almost ordering it to do one thing and do that one thing. Well, but you're
 coming at it with this vision of
 What? Let's work together on it. You have a certain level of understanding and capabilities that work fairly well. And so take that into your understanding and do it, you're just giving it more scope than I would have been if I were to attack this problem. Exactly. So, this is actually the one of the key principles we use in developing like,
 AI Integrations. It's like look at what a model is doing. The model is not doing what you think it would do. Don't go against it. Yeah. If I take a step back and go like we should redesign out in the face, so that, that thing that you want to do works, and then she becomes very easy because like, you know, not fighting the model, now, they're just doing what is really naturally programmed to do or learn to do in data and it turns out that everyone turns on the same data, all the model. So it's the same tendency is that, and so we see that across the platform where, you know, once we get it working and, you know, one more company, I the same system, then it works. Also with entropic, models, and Google models of the Tendencies are very similar and all of them. So then what are some other ways that you've almost created?
 Workflows or programmed with the model, they have many different ways to get models. Interactive applications. Two very common Frameworks are react which is to get it to move function calling over and over again. That's the most popular one and then I think they don't want is called code act. Yeah, you know, you get it to write a bit of code and you run it and see the outputs. So we are a big fan of the Codex approach. So we've been doing that actually, before the paper came out, oh, that should be. We saw the first demo. So gbd 4 was about know, I think Greg Brockman is on stage. You know, demoing, like coach Generations. Like, wow, that's gonna work. This is like very early days. So we have always been big on that, that approach
 And it turns out when when you take the Kodak approach, just a lot of things you can start doing now.
 So for example, you say, hey model, I want you to produce code and I need the code to respect pipes. So when you call this function and this function and that function and you know, pasta no data around and you play have a variable that goes around it, we can infer the types because it's just cold. Yeah, we can see that tight must be X and we get past it down, has to be a string. Oh, at this point of time. Good competitor number. And I don't.
 And that approach, you can say, produce a little bit of software called we can check whether the types match up, and it doesn't match up, like not good. Go back through it again. Yeah, right. And then you can see them and the model will fix itself.
 And so now again it's a sprinters on and setting the model. Like you gotta stay in the gut bills and the guardrails come from external system. That is well understood type tracking. Yeah programming. Yeah so the model gets better but it is over time too. Yeah and that's actually a really good thing that that works. Really about practice. Are you using? Yeah, identity AI for this because they, that's a whole value prop that they have. We are using pandemic for a lot of like type tracking and typing things for sure. We do it a lot for schemas and serialization of data, we use some degrees of pyrite, I think for type tracking. But then it's also interesting in that
 Doesn't balance to, right? So here's a fun party. Yes, type tracking is great. Get models to do type check code, and turns out that in Python variable shadowing, which is reusing a variable name that you once you use, you know, it's a very common thing, you know, like, you know, you use a index and Loops or something like this.
 Um, type Checkers, don't like that. Like you said it was a string. Now it's not a string anymore. What's going on guys? You know it has to be the same time in the whole function so it turns out that going back to our first principle, the model keeps producing code, that doesn't respect that particular rule. Yeah, great throw that rule out the window so they rule out of the window but keep the rest of the rules. Yeah, right. So there's some rules that you can be kind of like also guided on like a great guy rules like that, but this guy real, the model really hates it. You know, then you kind of revisit like, is it a good country real? Yeah, it's not a good real way, right? So it's like AI first, you know, like if I first system design. Yeah. If you were to design a environment for coding for execution for software to run but assumed that the AI was the main driver of producing and maintaining software what she design, right? Yeah. And that's kind of the mindset we have. Wow, over here. Yeah.
 Having the guardrail. But if for some reason, it keeps not liking the guardrails, see if you can
 Format. That guard rail differently. Yeah. McKinnon something that's such a like no valuable for the model, right? So like yeah, that's a lot of the ways to think about it. Like, what does the AI model want to do listening to me reasonable. It's got real a guy wrote, that's really important and you putting that up. Yeah. Because there's probably like Hardline guardrails that you cannot change, but then there's other ones that you just have their. For some reason that you thought it would be more useful. You thought that
 The model output would be more consistent or more accurate. And if for some reason it's not like in that and well how can we work around it? How can we maybe work with the model as I post to try to put yet another guard rail on it. Yes. So I think when you see I mean the two approaches right? You want to go back to the problem and pump it more to do what you want to do, or change the way you set up the system. So that just accepts what the model is doing. And the latter turns out to be a lot more effective we found because it's, you know, fighting the model anymore, right? So just really figuring it out. I think it's been a journey for us actually because he was like the models. Keep changing, keep evolving, getting good and bad in. Something this episode right here is
 Sponsored by the good Folks at mlflow the open source platform for teams looking to track manage and deploy ML and generative AI projects at scale. If you spend hours, keeping track of different llms, prompt versions, debugging agent, hallucinations or filling out. Spreadsheets of experiment results. Mlflow is the right tool for you. This year, the ml flow team launched mlflow 3.0 the first major version update in four years along with a fully managed version. Now, available to everyone build and maintained by the original creators of mlflow. Shout out to Ben Wilson, we see you, man. Managed mlflow gives you access to full featured, Enterprise ready, ml Ops platform with zero setup required. You get effortless experimentation tracking automated model versioning, evaluation and seam.
 Employment, all that, plus The Cutting Edge features four gen AI workloads. Like
 Automated tracing agent evaluation and one click monitoring dashboards for your apps and agents with zero infrastructure. Headaches ready to try it out for free. Well, Visit mlflow.log. Let's get back into this episode. Every tool has its own way of working and I can imagine you
 Are constantly thinking about, we need to create more Integrations with more tools. How do you look at that? And I know you mentioned mCP before, is that something that you're just like? All right, well, cool. Soon enough. We're gonna have mCP servers for all these different applications, and we won't have to think so hard about the integration. So putting things in there. I think I'm CP is very new and we do see a nice potential long-term future, right? That's become a standard. Lots of people adopt. I do see a world in which every company does API right now has MTP server next to it. That is someone like the API, maybe a bit different and that is no molded around how AI should use it with problems and set a functions that he curate.
 Um, but it's not that right? And we're still figuring out what that should look like. So, for example, mtps today, they're not output schemas and we think figuring out the right output. Schema of mCP calls really important. So, the way webbing thing about it is like one we have become MTP compatible at Ultra so you can actually use omcp service. It will try today. But two, we want to make it possible for people to create Integrations today without waiting for MTP server to be available.
 Turns out that what our Integrations Integrations are a translation of how, you know, I external system. Normally apis work.
 And how that should be adapted to our internal way we integrate with AI.
 Translation are very open to being AI translated. And so what we have build and ship in the last last month, actually was a way to take API docs from any system. Have a service that looks at our documentation, are how we set up things, how we configure authentication, also generate an integration all the code needed for that.
 And then immediately instantly tested out and then upon testing it out because it's important part, it's going to always work because of that edge cases and everything be able to have the model. See live data come in.
 Fix the integration and I roll it out. So just something that we've been doing it's it looks really well with like super based databases. You can bring into lutra Crut base apis, just about the box right now.
 It gets a bit more tricky when you start to work with really complex systems, like crms, but we're getting there. Yeah, so when you know what? It's so funny that you're saying this because back in 2022 or 2020 early 2023 right after chat GPT came out and there was this. Boom. And it was before folks, were really doing stuff with code. Someone was in the MLS Community slack and they said, hey, I built this framework and all it is is you talking to an llm telling it what you want. But in stead of
 The llm.
 generating code, The Prompt was
 What would this look like in code in your language? Oh, like so the LM was just output its own basic gibberish and its own code. It's and then it would
 Create its own language in a way. And then it, you could re-put that into an llm and for some real weird, really wacky. Reason it understood it? Yes. I think the model is actually have some Tendencies, right? They're like, like to process data in some ways or structure code or some things in some ways. Yeah. And I think that when you describe that, it actually reminds me, a lot of like, Chain of Thought reasoning, right? The first chain is actually to produce some pseudo code, just like this, internal jibberish. And then after that, you know, all maybe a plan and after that produce the actual thing you want to do and then it kind of understands that too. Yeah, so we see in, we seen that a lot. I mean, it's fascinating that reasoning models work that well today, because that whole process is now embedded into their reasoning model. And, you know, we definitely see like a big step up and so I moved to that. Yeah. And so, how are you using different reasoning models? So we what decided to be pretty model AG
 From day one. So we use models from different providers and all back. And if I we stop between them and like, falling back when my operator's not available and it actually works pretty well. And and the way I think about reasoning models in particular, is a models in particular. Is that every model? What he does, different strengths
 So, it turns out like, for example, Gemini flashing models are really good at data extraction. Give it a bunch of data really fast, really high quality data. Extracting comes out nicely, even the multimodal data even more exactly stuff. It's really, really good at that. But you know what, you know, I don't think that they're great at writing, you know? And then these clot models. She write really nice content. You know, tragedy now is the best image model. And so we think of it, as can we help users or dynamically figure out the right model? For the right tasks?
 When they need it. All right, so if someone comes to platform and says I got a bunch of PDFs, I need to extract the data and put it into a spreadsheet like ding, ding ding. I got just the model for you. Exactly, you say, like, we're going to use Gemini flash, to do all that extractions. We're going to use sonnet to figure out how to map, the data. Causes a lot of reasoning behind the data. And then, you know, if you run into any image, doc maybe use a different model there, right? And so that's what we do in practice and then the reasoning models that part really comes in critical understanding how to tie different systems together.
 Right. So like okay I need to book with system. A system be decided to set up the API sections. I have what is the way for me to tie to get a reason about it? Write some code run it?
 Now maybe it works great, maybe it feels and if it feels reasonable, why it feels try again go. So that reasoning is really important and orchestration
 And that's why we use mostly. So is it also, orchestrating, what models to be using? Ah, so yes in practice, yes. But what we do is that in the back end, we actually we do our own testing and pretty fine like no. Did I traction? We like Gemini flash. Now we're looking at them, you know? But and the top level is figuring out. What is the task? It's a task, creative writing report generation, where research or is a task, you know, data extraction, and then beating on the task we have hence elected the models based on just evolved sets we have. Yeah, that's a great way of doing it and all so not sticking to just one because you're like, oh well, I like Gemini, the best know. It's Gemini's great for these tasks, exactly. So let's use them, let's predefined that. We're gonna use them for that type of thing, that's right. And then if we need others, then we'll go and find others. How are you thinking about?
 Super complex and long-running jobs. Yes, we the longest tasks. Someone has to try to do to, I think two or three days to run really. Imagine you press the type of progress and there's like three days, it comes back, no way, but I think that there are different kinds of long running tasks that the kinds that one kind, which like you're doing a bit of it. You need to see what is it? You always re-planning really, you know we or renting around the outcomes and then doing it more and more and more. This one kind of long range task very high complex, knowledge work, correct that. Maybe a d research report about something and you got to make talk to even people like data that that's pretty hard, there's another kind of long-running tasks which we think AI is well suited for today, which is do this same damn thing but over 100,000 rows
 They're the same thing over repeated over many, many things but I just know verify me that's doing the right thing.
 So right now where we are editing is a very good at that, right? And then over time the former will get really good at 2 but the latter is actually where we are today. So it's almost like, yeah. If you can do it once. Well, yes, you can do it. You verify that it can be done, and then you just do it a lot a lot. Yes. So, so for example, the tasks in this case was to some extent, I have a spreadsheet of accounts websites. I want to research on, maybe all the people attending a conference, maybe all the nameless or something. I got from somewhere. Every single person in ml Community. What are you doing right now? Pretty big task, like, go to every single person represent them.
 Look up on LinkedIn. Look at up on Google Scholar, put all the data pieces together. Maybe even scared that and give me a final score on like hey should this person be a good candidate to interview? Yeah on this podcast, I need that but now it's like, okay, I got a 5,000 people or maybe 100,000 people in my community, run it on 10 hours, go running on people right now. All right. So now this is where interestingly, this is very tight back to a bit of like, how do you identically automate work? And this is where a bit of, you know, why we think metals like cold act Shine versus react, because now, if you're producing software code, you can go like, I want to write the function that does it for one rule.
 I'm gonna run that function a few times for you. Sure the output that's great. Now I'm going to write the for Loop that goes through all the rest. I just keep running until it's done. Yeah, it's not imagining. Now that's only like four to five model interactions and you're often races. Now, if you do something more, like the react framework, every single row is a single llm call.
 And never going to finish this because it's like, yeah. So much more expensive. More expensive doesn't make sense. Yeah, right. So this is where, you know, we have already been focused on that approach because it scales very nicely to many different kinds of real-world tasks that people do. Yeah. So the thing I just described is actually a task, people do like all the time right now, 100%. And if you're in sales even more, you know, think about that as like,
 Why is it input a Sprint store on a particular list of Target accounts? I carry about?
 Can it just read my data from that system? Directly do this work and notes back into the CRM directly?
 Go. There's no difference. Conceptually from a CRM and a sheet. No hide. The high level. Yeah. Right database. Yeah.
 Um but a hot pot traditionally has been. I need to figure out the apis and Integrations that but I was like oh wait. I have to if I have the Integrations ready to go into AI understands it I'm just a prompt the way. Yeah. So we have hotspot Integrations today and that's actually something they will do on their tasks. Yeah, I love that. Yeah, you're thinking about attributes or whatever it is, these custom fields that you have in HubSpot or Salesforce or whatever object with properties, like could be Nest that could be, you know, arbitrarily complex. What you want it to be. Turns out that this is where Integrations get hot too host. The more system allows you to configure things to a very complex degree. The model, AI needs to understand the schema
 Behind configuration. So we spend a lot of time on and as well figuring out like okay, how do we communicate use a Define schemas from external systems into our AI agent? So that agent operates, it's able to understand the schemas
 For example, if you had spot or airtable, one of these things, they say I want to update a drop-down field with the value. You know XYZ it's one of the drop-down options, the system we rejected. So you need to know the schema to walk with it.
 and then now, you know the I'm going to make the mistake of doing the wrong name, maybe it puts a space between, you know, qualified lead and
 The thing in hotspot often have no spaces is qualified lead without spaces there. So how does your AI agent then? Understand that error comes back fixes itself and going in right? All of those things. I think that we spend a lot of time thinking about an architecting for. So simple, on our end, we look at it. It's like qualified lead. No fucking space.
 And then for the agents, it's a little bit different. Yes
 Man, but that is that is so cool to think about how how much it can unlock.
 when you don't have to,
 Say export as CSV. Yes. And you can just let it run loose directly. This makes me wonder about
 Taking.
 this data and mixing your processes of the agents steps with
 traditional data processing and how you're looking at that because I imagine you have you, you mentioned
 You have certain models that you like to use for certain tasks. Obviously, there's certain tasks that are better done with pandas or things like that or spark or whatever it may be.
 How do you like integrate the two of these? So this is where it's actually really, like, it's really beautiful in some ways when you tend to use
 Going back to get the Kodak framework.
 so, if you're in a world where you are orchestrating between apps, not just by function calling, but by code generation,
 Pandas numpy. Whatever can just be part of the output and it is so today, when you use Lutron, it actually produces, you know, pandas pipeline. All the stuff that you'd like, it is called, as well. So you could go be like, okay, for this part, do this thing. That calls the AI function that extraction for this part use pandas to do some kind of analysis. What is this part? You start small to do a regression analysis for this part. I generate a HTML file using a model. So that's a nice pretty website that you can present the date at all.
 So now I guess that composing across all your existing tools, you like that do their processing with the AI tools that are new and bring it all together into one place, right? So that's really nice. The hot pot of this turns out turns out to be actually run thing.
 Yeah. So, if you think about the most of the time in the world, when you see people doing a code execution approach, the code runs in a very tight lockdown sandbox. I could interpreter from openai, that's been around for a while. You can use when they are gbt saying, write some code for me, do that in Python that python environment and lockdown sandbox. They can do nothing. Yeah, that's it runs on Pyramid, right? We cannot call apis. You cannot call out to other AI functions or or external systems. Now,
 That's where we took us that back in like how do we then make it possible? No, and how do we then consider or the security implications of making a Sandbox? That can access the external wall? Such that the AI is working in a sentence box but it shouldn't be able to access that he's key secrets and he keys but it can call external it. Can bring data back, well, that looked like right? And so that's why we've been turned the last time designing on and then
 Second thing is that, now a lot of sandboxes. They are usually like Jupiter style kernels. They run they disappear and it's like, oh, we're very familiar. Oh yeah, like the status gone, the variables are gone. It's not great because that's not how we interact with ejecting systems. We chatted it, we come back, many hours later and keep going again, or we could take a nap or wake up in the morning. I'm gonna make coffee like going in like, oh, your state is gone. Now, that's terrible. But because we cannot keep, we can also know reasonably, keep all the machines running all the time.
 So how do you make an environment that is more stateless, right? So we design, we have designed HD more of a stateless and environment, so a stateless security sandbox iconic.
 that enables the AI to operate and
 So that's actually the core of the, the cord at the core of it all. Yeah, there's so much that you just said there that I don't want to dive into a special on this this stateful and stateless part and how you're enabling that because there's sometimes that you need it. There's sometimes that you don't and
 If you get the wrong time, you can either lose data, or you can lose processes or a long-running job.
 or if you,
 Keep it on too long. You can just spend a lot of money. Exactly, exactly. So this does a lot of design decisions in that one, but at a high level, if you, if we just, you know, abstract ourselves into that go like what
 A computer systems, you know, like what's the state and they were talking about state food, stainless, right? The state is actually like what's in memory
 really, and one instructions in memory down execute against
 That's all the state that we need. We don't have to keep the machine around what we have to keep around. That's what's in memory this day? Yes, memory State and the current instruction State just what's in the, you know, instruction stack. What's on my functions on the stack? What can I run, what I can call, but all the variables on my stack, right? One easy way is to try to zeroize the entire machines did and say not many machine load it up and go again kind of slow and might imagine how that works and whenever you chat in the Finish just shut down. The machine says the state away and kind of solution there. And other way is to, you know, be able to, you know, with strap machines and load updates, much faster, and other ways to rethink a whole runtime. And like, let's design a whole new runtime that is designed around being stateless and then, you know, execute coat Rana writes it to memory but really controlled the entire
 No, interpretation of the code and how you start in the variables.
 That's another way to do it, right? And so we are much closer to the lot that approach where we actually you know, we don't just exact know without eval eval. The python code that comes out. We actually inspected, we look at it, we run it.
 Yeah, we actually integrated in the way we want to.
 And that gives us.
 A good amount of control.
 So for example, one of the challenges that we ran into when we deploy, this was someone knows up their 100,000 rows spreadsheet.
 Go through treadless, for me, that's running.
 If you ran python out of the box or any programming language, it's like there's no output, you know. It's like the terminal is like you know like there is this running and like is it running I don't know you're on top and see like CPU is running so it's good. I mean it's been going but you really want your execution runtime environment to give a feedback. Yeah. How long has it been running for what actions is it taking what is it doing right now? I didn't want to see a real-time indication of what's it doing. Yeah, and the UI. Yeah, that would be brilliant. So of course just like we see the streaming of the Chachi giving us the answer back, but in the court runs the stream, too. Yeah, but for the execution of the code and then maybe you even won an estimation.
 Can we you know, do static analysis and say oh this is how the code is laid out. This is the loop, it's going to be you know 9000 iterations. The situation has been running. Now it looks like it's about five seconds for one iteration. Oh yeah. So like 9,000 times 5 Seconds that's not only will take. Yeah. That'd be nice, right? So this other things that women, you know, looking at thinking you know, designing around and so if you just to try today, you start to see hints of that. So you when neutral starts to run a piece of code, you actually see real time live streams of actions are staking you start to see indicators of, like, time estimates, you know, and can make all of that better and more exact. And then, more importantly, yeah, I functions are expensive. So, this thing is running for you nine times, and in the middle is calling some model. It's going to record a lot of Hogan soon. Yeah. And so estimating not just the time but also the cost. Oh I love so this might cost you, you know, fight 1000 credits, AI credits to run because you're going to consume that many tokens, right? So that's all the things that come into the picture. When
 When you really start thinking about designing around, you know, AI operating software. Yeah, yeah. I love that so much on the cost. And it also feels like you have a chance to
 Warn people or set up alerts or do something where maybe the first couple iterations, you don't do anything, but you take a pause and you say, hey, by the way, if this is gonna go 9,000 times, it's gonna probably cost you this much. Are you okay with that? Exactly. And then you can have that human say, yes, which if you don't get in traditional Sequel and stuff, I remember a horror story of a friend who
 did some kind of a join and he was working at Spotify and then
 a week later.
 Somebody knocked on his door was like, hey, why did we just spend 40,000 dollars on something with the like this, you know, our database bill just Spike and he was like oh that was a big join that I did. I didn't realize how big it was you know? And so something like this, where
 It's conscientious and the alerts are building in.
 And you're ready to say you can say, if I am about to spend over x amount, yes.
 Warn me. Exactly. Exactly. And then I said, that's just the exact 30 things that we think a lot about because we had versions of neutral early on that didn't want and he was like, I send one prompt and when I created a God now, what happened like yeah, shoot. And because it's just a prompt, right? Yeah. But meanwhile, there's a whole lot of stuff going on in the background. Exactly. So we are entering a world in which a single prompt
 You know, could set off a whole chain of processes, you know, today, you know, I think that's having reports and articles like can do, you know, no minutes hours. I work for you in the future. I can do like day, so work for you months of work for you without thinking about it. Wait that's. That's a lot to design around that. Yeah, because if one problem can just do that, you want to have all this you know ways to help users understand the implications of that one prompt, right and today you know and that's where we're designing your own to. It was really thinking hard about. Yeah, and I can also Imagine a world where you would want to try and give some advice or help say where maybe it's like
 Here's what we've decided. Right? Here's the choice and you can click in.
 And maybe you're like, well, maybe I'll not all 9,000. Let's bring this down to 2,000 and we'll just see if I can find any value there. Exactly, like start start small the future 100, let me review it first or even better, AI. Can you review your own work? And if it's good, let's do the rest of them, right? And in fact, I think, you know, the application developer, this is really nice because, you know, you know from a you know, even like a business model perspective, a lot of AI I think would be more usage base, right? The more you use it, the more work, the AI does for, you know, the more you on the charge for that too, right? And so if you can't help people understand the value of the work and the magnitude of the work, it also means that it's a very natural business thing to do, where such know paying by the dust. And so it's really nice and aligns expectations, right? And you do not want it to go off and do 9,000 rows of work if they're all going to be really bad. Yeah and that's the worst thing you can you know and
 you're paying for it. Yeah. Like you know that you hired it interns like doing a terrible job, working really hard and it's like, oh good, we do everything. That's not good, right? Yeah but I think that's, you know, that's all
 Challenges to design in the space right now. Especially when, you know, when you say one problem can do days of work. Yeah, it's just long-running jobs, man.
 Wow. You are thinking a lot about system design with AI. How do you feel like system design will change in the coming years and especially if we're thinking?
 AI is only going to get better. The models the underlying models when I say AI, that could be interpreted as many things, but if we're looking at the underlying models, getting better,
 And being able to understand systems better.
 How do you feel like system design is going to change? I don't View.
 Calls you motels in the few Dimensions, the first very obvious one is contacts with no line but it's very important. The reason for that is that you know, back in the day of GPD 4 when it was like, I know, was it 8K or 32k? I don't actually remember. It is just tiny contacts with those, and we had to do a lot of workarounds. All this work around rag, was actually a lot of it was because the next Windows was too small. So I was at very small but now it's like shop, the whole document and the PDF, I'll go with it, right? So that gets bigger, that really changes the way we think about things, and what we can, give the model as context if it can reason about them really well. So again, I think it's only enough costs.
 No cost, then equality and equality and equality of the outputs. And the reason why cost is actually very interesting and important is that
 It's most sealant actually, if we compare it to maybe a year or two ago, would you be for it used to be the single call to gbd 4 was? I don't know on the order of like 60 dollars for million output tokens, only 120 like some crazy amount and now it is on the audio of like three dollars to five dollars. So it's about 10, 20 x reduction,
 So put it this way. If, if you would ask the AI to do a task, it had to make 20 calls to figure out the right things to do, and each call was about no 50 cents.
 Right. 20 calls 50 cents, ten dollars to expensive. I don't ask any AI to read my emails and summarize them with all this. I just did myself, but if that's a few cups of coffee right there, by the divided by 20 Sunday, that's, you know, that's way cheaper, right? Like 50 cents, right? Yeah, 50 cents. I would do that. I can do that. That's great. But then, when something about that, why do you want to do that? It turns out that a lot of the times, the models are not perfect, they're not one of single shot. Give you the right answer. And the more you can get a models to
 Decide like it, I'm going to do a task.
 What do I need to get somebody can now Based on data, that's what I do next. The iterative way of using a model over and over and over again, is very powerful. We know it works well, it's very agentic, but it's a ton of holes. So if the cost of models keep going down, then more developers and more users like myself will be very encouraged to call them a lot and calling them a lot. Actually, improves the performance a lot.
 So cost is actually like a big thing right now because I think the most I really powerful right now, like if you put on Cross by 10x today, I suspect we will see how know a lot of games are just, you know, like how people do reasoning because just keep calling it more
 that I think it's just like no party of outputs reasoning traces and so on, he was going to be better to. So
 A lot of the work we've done just looking back has been working around those things. How do we work around contacts? Windows being too long. How do we work around being you know saving a number of calls we make and then how do we work around on like you know trying to be smart about Which models. We use from a quality perspective. When all right, but moving forward, those are not considerations. Then should just be using them. Like, you know, it's like we'll start worrying about what to put in the context window, just put as much as we can. We'll not worry about how often to make calls. We just call as much as we can and then quality wise, I think it's going to be no increasingly better or worse like to. And so that's kind of like the things that they treat things, like kind of like pit attention to home, like a very high level perspective.
 Do you think that's where the biggest bottlenecks are right now?
 I think so. Yeah. So I think it's like the quality cost trade-off curve. This, I think the thing that everyone's pushing on so does this Frontier? Unlike you know what? That parallel. No quality cost Frontier is I think Google study Frontier right now so kind of cool. Yeah but I think that's a that's so much more we want to use it for and I think I think we'll just, you know like for example you know, when we generate Integrations today but like okay we got to go and forgot the API docs for the new system on an integrate, we go to figure out, Lucas API, docs on what we are doing and generate code in the middle.
 Okay, test and run that code?
 Self debugged a few times and then produce an output. Right? We now have to be very prudent on like what we sharpen as the API docs from the other system to show me everything. Wholesale, do we need to strip things out? Do we need to format it nicely? Do we need to be very careful while we put in there.
 If they contacts windows are very big and if we don't really have to worry that much just saying we give it, we'll give it more contacts than he needs. Probably because it's not a concern to us same for the outline documentation. Now when we produce,
 Some kind of integration. We need to test it runs in the loop, you just continuously let it go into your super confident and I was like, okay, when it's testing itself, we want to give it debugging information.
 So it's going to make an API call an external system, observe what's going on and say it's great, that's fine. It was that we want to give it as much debugging traces as possible, right? Not just intended and that era, they're not just like, you know, what's wrong? But actually like
 The the traces in the calls itself.
 You know, maybe the headers that coming back, maybe hen all the details that you don't normally see as a user or even in the terminal, right? Search, whatever stack Overflow for this. Exactly. So
 Will be much more open to just grabbing all of that. Something identity contacts too. No, but that requires the models to be able to accept long contacts.
 The reason about them and a good way and be cheap enough for us to do that because it was going to cost us a dollar each time when I'm gonna do that. It doesn't cost us like five cents. Great once and holy are no brainer as much as you want you know? Like it's just yeah it's funny that you mentioned that because I wrote a blog post probably six months ago on how the
 Price per token is going down but price per answer is going up. Yes and it was that very fact of okay per token cool, it's dropped its plummeted but now because it is plummeted that leaves us creating more complex systems and sending more calls and so no longer are we just doing a back and forth? One off. Hey, give me a poem in the style of Bob Dylan about my last earnings. Call were
 Creating these complex systems. And since we have these complex systems and there's many calls being made
 our price per answer has gone up exactly like it's a I mean Joe bonds Paradox. Yeah, it work that you want to use it and then
 Price per second token goes down, but we stuff so much more contacts that same price again. Like we don't do more so the price goes up. Yeah. Right. And I think they're just like, you know, just all demand for the cement. So I think it's great in that.
 The Connex input convex Windows have gotten very large things like pump. Caching we use a lot as well. That's super important and I think this was a trend now that the output windows are getting quite big too.
 And that tells you know that trend is only going to keep going. Are there any other trends that you think you're going to keep going?
 Say multi-modal it's a big one plus you know? I mean a year or two ago like Google Away by Dolly. Remember like Dolly first came out. Oh my God. What is image us? And now we're like oh yeah, GPT image one, touchy things like of course like the table Stakes. That's like well it's like just the bar just gets higher. So quickly multimodal video, all that would just keep coming to I think
 We will see a lot more, agentic type things out there, a genetic systems, I think.
 Were.
 Just from a quality cost perspective, not feasible. Definitely, two years ago when gb4 first came out a year ago was really expensive. This year. We're seeing in price prices plummet in a way that they become feasible.
 That too. Um, what about you mentioned? How your using gemiini for like the image understanding or extraction?
 Do you ever try with videos? Yeah. So we but we don't know it works. Yeah I haven't tried that either but I realized that oh that's something cool that potentially also could be I don't know what the use case would be. I don't know if it's just like cool and then it doesn't have any real applications but that's fascinating one too. And imagine like no take this podcast recording. Put the video and I think the support like
 Treat to 4000 frames out of the box which is like the video the audio to. And then you can say create figure out all the parts that will quit and just cut them out and he just produces for you then give us a minute long video and I know an hour shit, but he's like, okay run it. And then came out for other parts that the audience on the bad, I run it through some of the filter to improve the audio. So it's imagining like, hey, someone just things that we do manual at that thing and everything. The most thought, the reason about it,
 And as fascinating areas. So actually one thing that I always love to do the real professional podcasts do this because they have a whole staff and team behind it, but they will, if you have an interview, they don't
 Give you the interview that you see or I should say this, a different way. What the end product that you see is is not the interview, how they recorded it, they slice and dice it, and move this part to the front and move that part to the back. And oh the story fits better like this. And so I never do that because I never have time to really think about it and get that creative and say, oh, that question actually should have been here and said, of, in the end of it.
 but,
 That's a potential use case. You could ask the model to give me like, Pine points to like slice the video's. You know, I've always liked this generate a description where every slice cannot give me a reordering, a lot of spices. And then with this narrative, go better here. I think this question or this answer, we I jumped around. I was talking about Integrations, then we talked about something else, and then we went back to Integrations, or whatever it may be. So he tried for this video. Yeah, that's fascinating.
 But yeah, I think those are the things that I think. Also, I think there is this bigger Trend thing that I think people want to do solve as well, which is, you know, we hear a lot about computer use.
 I think it's very early. Honestly I use operators like also but you know it's really like super high like oh this is so be so cool. Like I ran it was like yeah to do my thumbs kind of kind of getting there. Needs my help. Now can get it there then. After like a day, I was like stop using it. But I do see a world in which, you know, this systems, you know, they, they can be trained to understand our world, as we see it. You know, how we use computers, how we navigate around the physical world, and everything.
 so just think most had to see a lot of developments that too, very early days but I think definitely some promise and how that works and oddly enough, I think it just goes back to the fundamentals which is
 I don't in contact with those big enough to put in all of the frames or history of interactions.
 To reasonably predict what to do next.
 All right, so the time of interesting, I think the other interesting part on that is I want to be using my computer. So would it be some VM or background process that's happening in a different sandbox, or a different almost like it would be some different VM there.
 And then when it gets stuck and it needs my help, it comes to the Forefront of my computer, totally. I mean, like I can, I mean there's so many ways we are in a
 This article right Yesterday by Pete Kumar night thing on a AI Hostess carriages. That's in that one. No that's great. I think we are in that world right now where this we all most of the AI applications are like horses carriages. You know like the guy in front of the trying to like on the carriage and the AI says, like the machine, the engine's machine in the back. Why is it there? It's like a bit retro fit and ways that we don't expect it and
 It's such a reimagine, you know, five ten years from now where the technology is cheap enough.
 To be deployed, everywhere voice.
 Recognition. This amazing multimodal was amazing and then you go like
 You know what should the UI be for computer now? I think
 the GUI that we have no that came from, you know, I remember like fondly, remember Windows 2.11 days to 95 and everything today with Malibu designed for humans to operate, you know, Windows like start buttons like so, you know, layers on navigation and all that. Very humanistic somebody to help us understand how these things work you know and but he doesn't need many of that, right? The AI needs operated more fundamental level you know right software and operate that.
 Um so I think we're still in this horses Carriage world where we haven't really figured out what the real interaction model should be yet. For example, if we wanted to do this task of getting the machine to cut up your video for you,
 What if?
 You said you give it a task, you give it to mp4 file say, go do it. But
 you don't want you don't want the AI to work without telling you what it's doing showing, you know, kicking around the UI and trying to get it to do what it's kind of nice. It makes what you're doing very understandable but those are not very inefficient.
 What really should do is like, run some impact commands, do some slicing run another model but other stuff.
 But what if on the flight it generated?
 Its own UI.
 On the flight because, you know, the models can produce websites, you can write his own software, what if on the Fly produce a representation of what he's doing?
 To you.
 That is not your video editing software. That is not just the raw commands as running something into me. It represents what they is working on so that you can understand what's happening. How would that be interesting now? So, not even the VM, right? It's like it's almost saying,
 Create your own one-off experience to explain the task of working on in a visual way so that a human user. Can't tell what's going on. So it's like, okay, I'm gonna write
 A little mini app, electron app to show you how I'm cutting up the videos.
 Okay. There's a column this way now. Oh great. I'm not gonna edit them. I want to modify my electronic on the fly to show you how I modifying the videos. So seeing that goes like so you're not watching it. Use your
 Current applications. You're watching it. Do something really rich.
 but then showing to you, how it's doing it in a very intuitive fashion, you know what the problem I have with that is though, yeah, is the because I've thought about it and I've had all kinds of conversations with people and
 The, there's certain applications that we use that.
 Are very specific and very Niche, like, video editing the pros, the pro video, editors, they know what words?
 Describe certain actions inside of the camera and they know, all right.
 When I'm editing, if I want different colors, if I want different Luts, all of that, even Lut for someone who doesn't really video, do a lot of video editing. They're just like, I just want it to look better and maybe
 He the AI can give you something.
 But if you want just one little thing changed and you don't know the word to describe it to prompt it, then it's really hard. But if you know that, all right, I can go in and I can either normally what I do, as a very, very amateur video video editor. I'll just go and type in YouTube. I wanted to do this and I can't see. And then I'll search around in YouTube a bit and try and figure out what I'm looking for. Potentially, there's a world where, yeah, I don't search you, I just impromptu back and forth saying no, I want something more like this and I, but usually it'll be me going through different YouTube tutorials to see. This is how you do it. And
 There's these specific tasks that you want something. One thing changed,
 And I'm sure you've had it with cursor or an any experience where you just want one thing changed.
 And that one thing, it's changed but the rest is blown up.
 Totally know. It's, it's funny like you remind me of the conversation, I had my designer and my team. Recently we use an image and models to create like you know, illustrations for some of the automations people may come neutral and then he writes up this really long problem that really well and clearly, you know, he understands how to express a certain kind of design, language way better than I do. Because when I try, it's cool. But it's, but it's but, you know that point, right? It's like that. There are few tokens that we use, and what was we chose that? Have a huge impact, you know, if you were set up a particular design pattern to cursive like I know what you want and that design pattern for you. Yeah. And so this is interesting. Interesting thing in the I do one there if
 I mean, at the thoughts on my mind here, I guess, the first thought was, what is the role of people? You know, what do we do with AI doing all those book for us? It's turns out that I think one is that having good taste
 but and even if you have good taste, how do you describe your taste? So it's not a describing your tastes having no tokens. Yeah. Like everyone now knows the studio one, right? But so many other ways to express stall, a form or design pattern that I think is really hard. Number two, I think is
 I don't wonder as well as like,
 can can the machines help us understand the Styles feel like
 Alright, so for example, you know, you look through all the YouTube videos and you're like, I like that kind of style.
 I mean why not just fit it on the videos and say, I like that style. Don't like the style. That's my mood board. Tell me what how she described this. I think it actually do a pretty decent job at describing it. Now the hot part is still finger was tells you like right. But in that fine tune in to where you have those that last mile of oh I need this but I do like that. I think I've seen a tool like that where it's almost like a mirror board but in stead of it being a random mural board the stuff that you put on the mural board are ways to give the prompt more in real. Yeah. And it's more enriched prompt. It's not just the same the words but you know I mean back to the Horseless carriages thing, are we in a world where
 Their problem that we are seeing is that most of the software today, has been pre-designed for us to use. So, whatever notes that exists in our software, some developers are like sit, like those notes, I'm gonna get up to you.
 But what it does not give to us was generated on the Fly.
 What if your video editing software looked at your videos? Say, oh, this is the podcast here to find Ops you want. Oh, this is a, you know, very a video recording of a commercial on the street. What is the fine Ops you want? Yeah. Oh, this is an interview. Someone is a fine option, one. And one of those knobs that it gives you are Dynamic based on this whole knowledge. So, what if you don't have to know the words, but the words come to us,
 all right, and that's not like now I'm just ideating because I'm like, that's a different world and where software
 software isn't just, you know, one size fits all design and ship.
 Software, adapts to the contacts.
 And don't think this kind of software yet just have not. And so it just I don't ice, I go to bed thinking wondering what that would look like cosim. I can imagine, you know, when we say the word personal computer,
 oh, I think the word personnel actually came from the Genesis of like Computing that that's what we want to do.
 But this day's, the personal computer runs the same software for everyone. It's not very personal except for Hawaii having my data and right, but if it's truly personal Computing and you know,
 It AI that looks at what we're doing and then makes suggestions to us.
 You know, it's like you wanted that particular style, you don't know what is it called.
 You don't even need to know about it because the AI has looked at 1000 podcasts and say
 Yeah, that's a stylish you consider and you're like, click on it and say, okay and then he's like, that's not the one I want inspiring me, show me five more, and then you said to learn, right? The fact of the matter, I think, is that the models have seen more data that any of us
 His humanly possible can write, they're seen every single piece of data.
 Out there in the web.
 In the world. If ingested, it all the tokens were talking about taste tokens are all in the model.
 So what better system to tell us Educators and we help us understand how these things then the models himself. So
 While football I guess is like I don't really know if that's gonna happen, but yeah, exciting wolves. I love you.