We actually tear all the different machine projects over into four different tiers with the Tier 1 being the most business. Critical project can measure some proxy. Metrics how many models we train the last week? How many miles is deployed for those users? We provide a way for them to directly access the info layer. Michelangelo is the machine learning and AI platform for Uber. All of the AI and ml use cases are powered by Michelangelo.
 Which is a feat that's freaking crazy thing to say just because of how many use cases you have. So we are. Yeah, we actually open source in my kitchen. Do you want to touch this? No.
 We are what holy shit.
 That's a lying on something first. Yeah, that is the definition of AI because, you know, now does everybody working out. So I think when
 A lot of people out there talk talk about AI to think of you know tragedy. Yeah. That's what AI means to most of the people out there, but from the AI platform perspective, AI is not just tragedy, AI is. Now even just the larger models were generally AI, which is technology behind the trgb. AI actually covers the whole spectrum of machine learning from the very simple linear models to trivial 3-ways models, like random forests and actually boosts to the traditional different models that combination neural network and recovering neural networks, and all the way to generate. So let's keep that in mind and then let's talk about my tangential. So
 Ubers Ubers AI Journey started back in probably 2015, when a few teams, like Maps, pricing and risk. They started thinking about, you know, he's if he's feasible to replace the rule base systems with the emotional learning with the machine learning systems,
 So one this team started, there are build their own one of, you know, workflows were in for us to support their machine needs for like training models with deploying models. These are you know, the random ad hoc python code, seating notebooks. So there's no books were very hard to manage.
 They're very hard to share. So it's basically non-reusable, we're not shareable and ownership of exactly, very hard to production. Yeah. And impossible to scale, which leads to, you know, the inconsistency and performance and also duplicate the efforts across teams at Uber. So that's actually where my angel comes into play. So Mike, Angel provides this centralized platform for machine learning developers to manage the whole machine life cycle and to end without worrying about the underlying influence system complexities. So that's why we actually start building my kanjo back in 2016. Yeah, get that scale ability and I think there's something super cool that you pointed out in the recent blog post. Well recent like now a few months old. Yeah, on from predictive to generative Ai and how Michael Angelo has had this journey. And you talked about how
 Folks.
 You want their own way of doing things. And so you have this centralized platform that's supposed to be helping them and streamlining things. But then you have those people that are pushing the boundaries and they're pushing the edges and they're asking for, hey we want more deep learning use cases. We want to be able to support deep learning, use cases and then you had to go and figure out how to make Michelangelo adapted for that. And so the flexibility on that platform. Can you talk a little bit about how you thought about that? I think one of the last says, we learned along this journey is that
 you should let the developers choose which to the one they want to use.
 so, what we do is, what we did is that we provide a
 Abstraction layer on top of all the infra, you know, all the infra complexities we with the pre-built templates and pipelines for. I want to say more than 80% of the users.
 For them to easily build machine learning applications.
 And for the rest like, 20% users, these are the advanced power users. As you mentioned, they won't build, highly customized workflows. They want to use different trainers to train different models for those users, we provide a way for them to directly access the infer layer. For example, we have a tool called uniflow, which is orchestration framework python oxidation framework, which allows the users to write their own code to customize their workflows and directly access the, The Rake clusters to run their own training and then deploy models on my calculator. So that's how we think about this 80% and 20%. Yeah, and speed that process up so much I can imagine from that time.
 That you have the idea to actually like deploying. The idea is unreal, is that something that you look at as a key metric? If we talk about Northstar metric for mechanical, right? That's let's go back to to the mechanical here because the success metric is supposed to measure how well the product meets the product go. So Michelangelo go is to provide best-in-class machine, learning capabilities and tools for movers Machinery. Developers for them to rapidly build and iterate, high-quality machine and applications. So there are two keywords here. Why is the rapidly? Which measures the developer velocity? That's what you just mentioned. Yeah, the ideal Matrix is time to production basically from the idea of a project. How long does it take for their project actually launched in production?
 So that's a that's a very nice metric on paper. It turns out very hard to measure in practice. So funny that you say that. Yeah I can tell you why, why right? First of all
 Use cases. They are all different. There are use cases.
 You just need a linear model. The other use case. You probably need to train a large amount of scratch. I'm just gave, for example here. Yeah, so you can imagine the time takes for these two different projects to launch. It's totally different. And the second variable here is the
 How do you say the team capabilities? Probably a engineering team with five machine Engineers. Everyone has like 10 years of experience.
 Probably can do things much faster than team consisting of one like applied scientists, which who just graduated. Yeah. So we also see this kind of different experience unless they're going on Vibes. These days, you can take you a long way. Yeah, Jokes Aside. Yeah, I didn't mean to do. You know, it's true that you do have these very different teams capabilities and maturity levels and how they attack the problems and then you also have these different use cases that if you're training a large language model, it's very different than if you're just using a random forest model and you can't take the average of those. And then think, oh, our time to Velocity is decreasing or over the last quarter. It increase because we trained some large language models, you know. Yeah. It's really hard to systematically measure just metric. Yeah, what we do here is we do two things. The first thing is we work with individual individ,
 A teams just watch how them how they actually use my calendar to launch their project. So we get some anecdotal feedback from these teams. For example, our Rider pricing team
 They shared feedback saying by using my candle, it actually reduced their engineering Cycles by 80%, compared to them building everything by themselves. So interesting. Yeah. So we have such like anecdotal feedback subjective in a way subjective and each team has their own way to measure speeds, which measure the engineering Cycles. The second thing we do here is that yeah, these North Star metric is really hard to measure, but we can measure some proxy metrics.
 Things like, you know, how many models we train the last week? How many models are deployed? Yeah. How many variation pipelines, we run? How many evil reports are generated, all these proxy metrics can be an indirect indicator about your developer, you know, speed developed with velocity here. So we track all this systematically on my casual level. It's funny now. It's probably a good moment to bring up to when you talk about evaluation pipelines, you're probably talking about the predictive ml evaluation pipelines less than new generative. You're now you're talking about both actually both okay but that totally different. Okay, that point I see where you're coming from. Yeah, totally different. So in a way you're looking at all these proxy metrics, you probably have a page or a spreadsheet of him and you can see if they're going up, we're doing more is good, right? And less is bad. That means there's something that's slowing people down. And you can correlate that
 To that North Star metric of velocity. Yeah, so truck is that too at two levels. Why is that the like the Michelangelo level? And we look at all the training pipelines run last week or all the models deployed last week. That gives a very good idea about the, like, velocity, or how people are using our platform. We also tracked this at project levels for each project. We have dedicated dashboards to show all this metrics for just for that dashboard. I just for sorry, that's for that project. Yeah, so we know if there's certain things going wrong with certain projects we need to fix. So we do have, it's worth noting that
 All of these different projects. I'm sure you are powering
 A million different models that I can't even fathom because I was telling you earlier before we hit record about a few blog posts that I read on that were more on the predictive ml side. And it was a breath of fresh air to read these posts about one was around recommended systems out of application. And the other one was I think it was like a multimed bandit scenario. Yes. And they were so unique. These use cases were like, wow, this is so cool to see how mature and how advanced these use cases are. And it made me think when you enable the teams, when you have something that's not blocking them and they can quickly iterate. Like you were saying evaluate, if it's worthy to continue forward and then get in a production.
 You can throw ml anywhere, it's like, the more creative ideas, the better.
 Yes, you can. Yes, for sure. You can throw him. I'm everywhere. But do you want to do it?
 Because ml is expensive, right? It's the if you want to use different learning it's very expensive. Yeah. Does your does your use case actually require machine learning like or even deeper learning? That's a, that's the first question you need to answer because that the machine it hasn't come for free. Yeah so you have price to pay your socks person. So do you want to you actually want to do it. You want to evaluate it on that front first, then start building your projects, train your models, prepare your data, train models, do things like, and are you looking at the cost? Because I remember in the, from predictive to generate Vlog, you talked about how there's different tiers of models support. Yes. And so if it's obviously, the righty ta, that's probably one of the most important models that can never go offline ever. And then if it's something maybe a little bit more experimental, you are more relaxed about yes, when my kanjo started back in 2016, at that time, our mission was to enable
 Originally for women, basic and Uber started with machine learning.
 And at that time, one which one my candle started, we only had like three use cases on my calendar, but now we have, like 1000. Yeah, so each one has their own dashboard. Each one has their own dashboard. Yes, you are looking at. Hey, if the metrics aren't going, right, we gotta go look and see why. Yeah, for example, if your model performance decreased yeah it automatically send out alerts to that team and then they look into why and for you if they're not shipping enough then you're gonna go look into why is there something blocking exactly in the platform? Yeah, is our system. Is there some system bug that blocking the development is the pipeline keeps. That's the pipeline keep failing and all things we need to look at. So but anyway, sorry, I just, I distracted, you of dashboards is just blows my mind. Yeah, so um, whichever one we use machine learning. Come to Mike Angel. We're help you get started.
 And fast forward a few years.
 Actually, most of teams they already had Incorporated emotional into their core use of flows. So this is the, I think the end of 2019 early 2020. So, at that time, we actually pivoted our strategy of a little bit instead of focusing on enable machine learning, because that's where they don't, we pivoted to improving the key project performance.
 That's when we introduced that machine learning Interior Systems. So, based on the business impact mostly based on business impact, we actually tear all the different machine projects over into four different tiers with the Tier 1 being the most business critical project. This some examples like you mentioned the pricing matching like Rider driver match and also ETA fraud detection. Nothing is right, if this model doesn't work, it will cost like a high level outage for services. Yeah, so this out here, run projects and all the way to like tier 4, these are some personal experimental projects. You just like you mentioned user, just want to try different things. So those are tier 4 projects. So today we have about 40, tier one projects about 100, tier two and five six hundred here for projects. So that's how the theory system works. And when it comes to prioritization, right?
 Which project should we support first? There is very clear. What's your folks out here on first? Then to, the more fun to 324? And a lot of times,
 We just don't have out with your support here 3 and 2004 to be honest the majority of the use cases still and I can bet that almost all of those tier one use cases are still predictive ml versus generative AI again to be in the tier one project, you have to be in the core a woman's core user flow. Meaning when when user book a ride. Well what the food or buy some groceries? Yeah. Is your model actually in that critical path?
 If it's not, that means you're probably not your one projects. So based on that definition.
 100% of our tier one today, they're still pretty predictive my mail. Yeah. And I can imagine, there's been a lot of people that have tried to think about ways to put generative AI into that core product. But when you think about the flow of me signing on to an app and then getting a car or getting food,
 Where can you even put it in, right? That's
 Very good, very good point. Very good question. Here there are some there are some scenarios and use cases. I think where we can actually apply jri and to improve the experience. For example, we have to we're actually working on two projects right now on the Uber Eats side. You see the first project is we use the large entry model to improve the presentation experience to generate better like these descriptions to actually match your your
 You know, users.
 Pinterest to what we actually show on the East home theater. Sorry, he is home field screen. That's one thing that I can really help. Yeah, the second one is, we actually use a large amount of search quality. When you use a search something in the, in these app where you search engine model to improve that search quality because it can pick up if I'm saying, I want something fancy and then understands fancy a little bit more than if you understand. Yeah. Yeah. That's one thing and also we can use the larger model to build taxonomy of all the dishes and restaurants with its that's down with a large amount. In a way it is a little bit of this recommending with llms. Especially on that first use case, you were talking about where you understand me. I'm a vegetarian I go on Uber Eats, you know, show me any food and then when you talk about different restaurants
 You're not going to highlight their meat dishes, you're gonna highlight their vegetarian dishes. Exactly. So in the past, if you I don't know if you notice this in the past if you log out to the movies app,
 We have this carousels, right? This Carousel will show different restaurants with dishes and in paths. We only have like Korean food. Chinese food, Burgers. Hot dog. Very boring. Yeah. You look at it. You know what, I don't know what it is. Just as Korean Chinese it. Yeah it doesn't really intrigue me. Right. So what we do is we first of all we use the large engine model, try to understand the menus and dishes from all our restaurants then come up with the new set of Carousel titles. Now we have
 like,
 Spicy Sichuan, food, meat, lovers, and all those caching, you know, Carousel titles, and then we use such large model to to attack each of the restaurants. With the new, with this new Carousel titles. This is how we act. Then we use this information to match our leaders with the restaurants. Oh, that's how the larger used for recommendations. Yeah, I can imagine. So you could do it with like user reviews, maybe not in Uber's case, you're not giving reviews after you eat, right? Normally we actually do but not many people doing that. Yeah.
 That's good. I didn't even know that you had that in, but it in a way it's like that sentiment. You can see if somebody or if there's many people that are ordering the same dish, then you can know that. All right, we really want to figure out how yes to message this dish correctly.
 It feels like it's a bit of predictive and generative ml. Like, because if I'm ordering the same dish over and over, you don't need a LM to be able to siphon that out. I guess that's more. That's very clear. Tabular data in a way. Yeah, but that's a very good point.
 Coming back to that review thing, right? We actually we just launched a project and I project into an online experiment. What we do is we
 we use the large actually model to look at all the reviews for certain restaurants and also the scores and other sources of data, then try to summarize how the heaters actually like or dislike about this Russia.
 Then we give that feedback to the restaurant owners and they can make improvements on the dishes or on the services based on that feedback. So that's one of the project we call this customer feedback summary for merchants. Are there any other interesting use cases that you like talking about or that you've seen? And you kind of got your mind blown by? I can talk, I can talk Brawley how much money has been used at Uber from the you know, predictive missionary all the way to generate here. So
 Like at any sort of moment, we have we have more than five five thousand models in production.
 Making 28 million real-time predictions per second Epic.
 So that's just, yeah, that's a scale. We are operating on and everything happens on my calendar. So you have to break.
 Yeah. So
 Virtually every single interaction. Now users have with the Uber app involves machine learning under the hood. Let's take the rider app, for example. Right. This is all pretty, predictive machine learning? Yeah. So far. So when you try to log in to your account,
 We actually use machine learning to detect if this is actually you tried to log into account, is account takeover. Oh wow. Yeah, just part of our fraud detection mechanism here. Yeah. Once you log in machine learning used for, you know, when you search for a destination is used for search price, for use for ranking your search results and once you identify your destinations then have machine is heavily used for match you to the right at your driver, for the pricing, which year calculation, even to recommend or recommend the right product to, you know, if you notice this, if you open the web app, it actually shows your lot of different like, uberX, Uber black. Yeah. Everything. Right Comfort. Yeah, actually, that list is personalized. Everybody's different lists. Uber thinks I'm way richer than I am, because they're always recommending me. Come for, like, give me that cheap option. Why are you better? I know. I don't like that.
 I don't like that at all. I do. She's Comfort, though. Those, they got me. I'm a sucker. Okay. I let our product of representation to know. Yeah, but coming back to this, right? All the way to like where you are on trip, we will give you the real ETA. That's also machine learning and all the way to payment fraud is actually in payment and also the of course, the customer support. Now we use AI for customer support, then if we take the Uber Eats app is the same story, machine is everywhere. And with we focus more on the personalization recommendation and all so easy ctd. So this is estimated time of delivery. Oh yeah okay so so yeah that's on the predictive which is now let's talk about G. Yeah if we look at the Genghis is over, we can
 roughly divide them into three different categories.
 The first category is we call this magical user experience?
 These are the generator AI applications that directly impact our end users. For example, any of the chatbots, our customer support chat have this personality Chapel for our drivers, throughout the questions to guide them. Where should you drive? So that you can maximize your earnings. Oh cool. Things like that is still it's still in the works. But yeah, that's something in progress.
 Yeah. Something else like the personalization experience I just mentioned that's one of the one of the use cases belonging to the magical user experience. Then the second category here, we call it process process automation.
 We basically use generative AI to automate some of our internal processes. Well further, automated some of the internal processes. And for example,
 we use Ani to generate the
 the menu item descriptions for restaurants.
 Because a lot of times, if you log into overseas app, you will see all the manuals, they only have the title, the dishes only description at all. So we're using general to generate descriptions. 100% of all the dishes will be well. And what else do we do? We asked so use GMI to try to identify that fraudulent accounts.
 One of the key characteristics of this project is that they are usernames. Are usually gibberish users like interesting. Yeah. Like I love dogs, probably not real person. That's like fxy Wi-Fi. So we use such a model, just through all the accounts, try to identify this, you know, potential fraudulent accounts, something else. In terms of this in this category is the
 Driver background, check.
 When the new drivers you know when they try to avoid to the platform, we do very thorough strict background. Check for each of the drivers and we use our larger model to accelerate that process. Oh well, so that's the second category. We call this category, we call this process Automation. And then with the process automation isn't it doesn't have anything to do with like back office. Processes of hey are
 Like in a way bureaucracy of what you need to do something, and if I want to submit time off, maybe I mean that's the third category. Okay. What do you call that? We call this internal employee productivity. Okay. Yeah. That makes sense. This is where we build all those tools. Yes, you mentioned the workflow automations. And for example we also have this will be this data GPT.
 which is because woman has
 awesome amount of data and we have so many data scientists products, then you need the data every day today they do that by writing sequels. So we build this data GPT to to help them first of all right better sequels a second just you know Corey data with the electron language
 So that's one of the internal employee productivity projects. This text to SQL is becoming a very standard gen AI use case, you see it in so many companies and that
 lift that you get by allowing the data analyst to
 write more queries, more quickly or just get answers quicker. Yeah, is very valuable and it's something that you can show value really easily the other stuff. Maybe it's debatable if you do it right? Yeah. That's keep that in mind. Yeah, you can't do something. You can do things really fast but it has to be
 You have to give your credit answer. Yes. See that's the key. Yeah. Yeah. Because you can spend 20 minutes trying to get a simple answer because you didn't do it, right? Exactly. Yes. So you're talking about doing it right from the end user perspective, not doing it right from the infrastructure side of things and how your architecting the agent to go in and write because you're from the first side. If you do it around for me, first side, you're screwed. Exactly. There is another thing I wanted to talk about which is
 The scale and all of this inference and what are some gnarly challenges that you've gotten into? Because of the amount of models that you're using. And you're Now using
 What not smaller models. But you're using all these predictive models. And so those have a certain style and flavor of needing to use inference and then you're using the generative models and those are a different style and flavor. And so how do you attack that within Mike Angelo to be able to serve both of those? Yeah, let me very good. Very good question here. So I think there are few things we
 we did to enable generate AI to extend my casual supporting the predict from predictive machine learning to generate AI first is the
 The Computer Resources, the computer amount. So Different. Yeah, right now you need
 A lot more gpus and high-end gpus. H100 others. Gpus tier 4. Doesn't get that kind of stuff just here for. Yeah, so we prioritize higher tiers over lower tiers for sure. Yeah, we also prioritize production jobs over offline, training jobs. Yes. Yeah, yeah, so that's how we prioritize, but coming back to the church to the infra change for j&i. So that's one thing we need to procure more Computer Resources. That's the prerequisite for you to do anything for Johnny, right? And secondly,
 the tech stack has to change, has to evolve, but for, yeah, I models because now the model is much larger, right? It doesn't fit into one GPU. Usually it doesn't fit into one GPU, so you need a model Paris. And for the traditional deep learning, you should get by using data Paris and it's holding enough. So so for two to enable the genre because of this restriction, we actually integrated deep speed and use the CBD in Junction with Ray. Poor, the other. Oh yeah. You have to. Why is the illustration are these model? Yeah. So you spell for a large angry modifying tuned. We also enabled to try to from Nvidia and media. Yeah. For serving larger models. And now
 We actually using the same infra to serve one of our large traditional deep learning models. Like I would recommend a vegetables. Okay. So,
 It's not just for 4gi. If we also use that for some of our traditional machine learning, which is, which is really cool. Wow. So you had to add it's almost like you had to add a few new tools into the text stack. But then when you did that, you found those tools. Help for the other stuff is actually useful for something else wherever you're doing for years for those bigger deep learning models, but not for the smaller model, three models. You don't need that. Yeah, it's not needed. Yeah. And then you can choose what they want as far as that or you're the one that's optimizing. The inference on the first side is us, but there when we talk about pets and of a of serving model, right? It's always true things. Why is the optimization that's what we do? The other thing is on a bother inside, that's what our users do. Yeah, for example you can like quantization, that's something they should do. Yeah, but we should support quantization.
 A quantized. The model survey. That's an infras. It's always collaboration. We also collaborate with other teams. I imagine they're coming to you all the time, asking you for support for certain things like, oh, well, now we want to distill this mono, can we figure out how to make it easier for us to distill it or compress it or quantized? It all of that? Yeah. How do you go about choosing? Because there's no tears. There is there. Or are you still bringing back in this idea of the impact. There's still impact by still impact. Based they are
 Larger. There are generally use cases with larger impact, compared to the other, like more like experimental you projects. So even they're still here three today because they're not in the core use a flow but they still can prove their impact business impact.
 So, once the assaults, they can prove their business like we can prioritize, we talk about generative, AI use cases.
 How are you looking at?
 The agentic use cases and when the agents are going and doing things it in the end of the day, it still is just hey, you've got models, you're focused on that inference of those models. Are you adding extra infra support for the agents quote, unquote agents because as we said before, it's probably good to Define what you think when you think of Agents, but in my eyes it's like the LMS or the generative AI that can go and actually do stuff. Yes, to just giving you these answers. So, starting this year, our Focus has shifted to support exactly what you just mentioned. We want to actually enable Uber to build a genetic AI systems going forward so where you extending my tangential. So, in the past, we call that more like model Studio, where you manage your models, and all the, you know, red events, you know, components religion, model training, and so, I mean now,
 We want to also build a agent studio for agents for you to actually build. You evaluate deploy your agent and manage your agents. So that's something actually in the works. I just got out of the meeting this morning with our engineering team, who is actually building this building just right now. That's very cool to know. So how are you looking at it? And
 Is it any different than the other stuff or if? So, maybe the better question is, how is it different?
 so,
 In the past, we all that we carry is the model. We make sure you can trade a model then deploy the model to endpoints. Now you can call the endpoint to make predictions. Yeah that's what we that's what we can as the platform.
 But the agent is different agents application by itself. Yeah, so when I have application Level, so we have the model Studio here, then agents throughout our top of model Studio. The agents in ages studio will leverage the models to building models to do so it's that whole application Level now
 How do you actually accelerate that agent you know decoration evaluation and deployment flow? That's something. We are we are actually evaluating and deciding. How do we allow our users quickly, spin up some agents evaluation performance, then go back to iterate on agent and very again and deploy. Same thing as what we do for models but now it's more an application. Now that's a major, I think that's a major. How do you make sure that the agent has the right permissions? So you're not writing to some database that it shouldn't be and all that stuff. And yes, very good points because one of the major value prop of the ages Studio mic Angel, is that no matter what model you use, allow you to access Uber internal data and also pretty internal tools.
 And then the, you know, the security question you just mentioned coming to the picture. There's some there are certain data you have access to send that you don't have. So to enforce that we actually work with our intersecting, we have engineering security team.
 To work with work with them to build this security protocol for the for this agent for. So if I create an agent then it knows my permissions. Yes. And it is only allowed to do those things. Yeah, it's either yourself or your team. Yeah. So it depends on, you know, your Authority and authentication, but then the utopian scenario here is that
 You have this agent studio and another team can come and grab my agent that I built. And now start implementing it for their use case, or they tuned it, and do a little bit more. And now, cool, I'm up and running with an agent. I didn't have to build it from scratch. Exactly. But the big question in my mind is, does it automatically know that? Now it's another team it's another person and now they have these permissions issues. Yeah sure. So we're still building this. It's not in place yet but still building that and the thing just mentioned is actually we call it Asian registry. So we have a ripple of all agents building with Uber and every single team can looking to, you know, look at that agent, Rapport. You received, there's any anything or any existing. So I can just reuse when you have that because you have the you have the model repo, that's classic one, most folks have that but then
 I've read The Prompt toolkit blog that you have a problem and you have the prompt repo, which is another great one and where folks come and they see. Oh, here's a prompt for this model and it is for this,
 Specific thing. Somebody already spent the time to tune this prompt. I don't have to start from zero. Yes. Yes. And now we have agent Rebel. Yeah, we also building the, I'm CPU rifle. Oh, really super cool thing now, yeah, yeah. Wanna build mcps for a lot of the Wolverine ton of services so that can be leveraged by the agent by the users to build agents. You're thinking all right, well, all of the internal Uber tools should have their own MCB server, not all of them but some of them. Yeah, we look at which tools are used a lot today. Bye. Bye. Our, you know users then we won't be on CPP for those tools. There's agent Builders and then there's two builders in a way and maybe it's the same person.
 it can't be, just it just is that depending on the state or their
 Their moment in time they're building tools or they're building agents. But at the end of the day, I think all of us want to be building agents. Yes. And not as much tools but at all very usually the two owners. Yeah. They build the MCPE server for that tool and that will be used by all the other agent Builders and then they'll say hey this is standard practice is what we want you to use the tool for. And I probably took Now read the documentation to see if this useful for you, if yes follow this and to use it. Yeah, yeah. That's how we envision that still works. Yeah, I can imagine you'll come up with some cool stuff again.
 Because you can throw AI anywhere right or ML on.
 Helping Focus select tools. Oh yeah. And that search getting that search really clean on, I want this and so it's almost like I described what I want and I see a world where that internal tool can look like okay.
 It may be you want to use these agents or maybe you want to use these servers MTB servers, and they have these tools that you can leverage. Yes. So we talked about supporting different inference, right? But what about supporting different evaluations? I think evaluating a Gemini applications for agents is totally different story compared to all the variation for the predictive machine learning where you just, you know, you know exactly what metrics you're looking for. You imagine AUC, you know, Precision recall and you have standards pipelines truth. Yeah, exactly. To imagine to manage the accuracy there but for G is totally different. I think this is still a problem, the whole industry trying to figure out how to do the evaluation right. But at Uber we build our own college and I inbox which allow users to do things. Why is the
 Our Master judge basically use another LM to judge the hopper from this aims. I think that's a lot of teams out there in the industry are using. The second is to include the you know, human in the loop. Basically, whenever you need human to make a judgment, you just evolve the human to do evaluations. Those are the major two methods. We're doing. We're using today and also the third one is. Yeah, of course. You can, you can provide a golden data set, even for your general, use cases right here.
 Provide volunteers at the user governance data set to to invite your performance. Those are the few things we've been working out.
 Sweet. Dude. You want to touch on? So we are. Yeah, we actually open source in my country. You want to touch this? No.
 We are what? Yeah, holy shit. Yeah really. Yeah.
 Applying.
 Two years open. This was a fullback Angel, starting with our
 Australian framework called uniform. Wow.
 That's huge news. That is so wild. Yeah.
 So your open sourcing first, the orchestration framework and then everything else.
 Yeah. And it's gonna come out in spurts or it's going to come out. You have to clean the code base. Oh yeah. A lot of cleaning work. Yeah. In the works. Yeah. We have to clean the code base and our plan is to
 At least for this year, 2005 we want to selectively work with some Enterprise Partners, you know, closed Source fashion. So we give access give us to them to our open source raffle and they can count your bills. Then next year, probably want to fully open the web for to the hole. That's so cool. That's why I was asking about the prompt toolkit and if it was ever going to be open source, then it was yeah, probably.
 Probably H2, sometimes Q4 maybe, yeah, this year this year. Oh, so there's gonna be something that will come out. Super, super fast. Ready have something. Oh but again, this year is our clothes Source. Only those few partner teams have open. Yeah, but next year we try to fully open. Wow, so that's that's so cool. Why why? Now
 Well.
 you know, one, when I can just started like back in 2016
 There were, there was not, there were not many options out there, so we had no choice. We had to build everything by ourselves from scratch. Of course, we use open source Technologies, like, spark and other things, but then fast forward 10 years now, nine or ten years. There's so many startups out there. The there's all the cloud players, they provide their own ml Ops tools and they're such a huge amounts Community out there like yours. So we do think allowing like external contributions to my angel can
 can actually trust quickly accelerate in innovation of back Angel. That's why I want to open source this.
 Yeah, which, you know, in turn will benefit our Uber community. So so yeah.
 And all. So the other thing is now we're in this era of ghee, right? And general, if if machine learning has been advanced in so fast, can I even faster?
 So we our team, we have that 100 people but still, it's a small team compared to the whole Community. It's hard for us.
 To how fast to keep Pace. Keep up pace with all the advancement and Industry. So again that's why we want to
 You know, allow the external contributions so that we can keep ourselves. Always at the front of the technology advancement. All right? So that's awesome. No.
 I always wondered what it would be like to work at Uber. I am not the big company type, I think I would get fired, very fast. HR would have a problem with the things that I say.
 But I want to know, like, what do you like working at Uber and what do you not like about working there?
 Sure, I think.
 What? I like first thing first the
 Engineering team.
 The Mike Angelo engineer team is
 Truly world class move really fast like Heidi capable. They can get you down. Okay. Let's put it down. Okay, and incredible, he collaborated very supportive of your PM RPM work
 So, that's a product manager. I could not ask for better engineering partner.
 That's the most thing. I like, that's the thing I like most about working in my current position.
 Secondly.
 Personally, I do believe product management is the best job in the world. If done, right if done right, and I'm deeply passionate about Ai and I'm Ops. So my current job is a perfect compilation of both.
 so I really enjoyed every single minute for the past four years that we were
 What else?
 Yeah, since as I mentioned, since all the Uber's machine use cases, I use cases and manager, Michelle. It keeps gives me this front row seat, right? To see how much money is actually driving business impact across over.
 So from pricing to recommendation, yeast to fraud detection. All the way to Chennai, chatbots. It has been a fantastic experience. I want to say. Yeah.