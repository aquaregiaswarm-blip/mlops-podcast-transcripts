And our view is inferences, the new transform. They thought started like slowly to become more of the product and not just like a back office things of the combining needed like to run the business platforms, that exist out there. Don't treat these as first class citizens, it's like, okay, you're not getting a bonus this month because your prompt was not good. Dude, who's gonna be the pets.com of the llm bubble?
 I'm excited because I was the metaphorical Cupid of yeah. Matchmaker. Yeah, you did it building. A company is always a journey. Usually people talk about what they've done after the fact. So they tended like to remove a lot of the Gory details offering. Yeah.
 so,
 Both me and Yoni. We we came from similar experience of the market, but from different angles. So, I was working, mainly like, more. Let's say traditional data infrastructure. So a lot of like the data engineering like work there, especially in the Enterprise Yanni was came from deadline again but more from the ML and we started like working on that. Okay, how we can rethink. Let's say and build new tooling around you working with data infrastructure for the problems that we have today because keep in mind that like the dominant tools that we are using were created 1213 years ago. Completely different use cases are spark 3, know even like the commercials like snowflakes they're kind of like similar of the end of the day. It's all about how we moving to the cloud and how we do like this big data, I think it's the dominant use.
 Is bi its analytics right there is a ml but ml was like always more of a niche thing at the end of the day. Nothing converts like to the bi Market. If we take it in like just like the numbers out there, right. But it's like 12 13 years. When things have changed like many new workloads came in, there's like a male. What was like you only was working on. Okay. Today we have ai we'll talk more about that later. There were things about like a bandit analytics. A lot of like product related, like data started, like slowly to become more of the product and not just like a back office things that the company needed like to run the business, right?
 and of course, like this tools were not built for that we could
 Kind of like Parts them like to do it.
 But as the market was demanding more and more and more, the pain was becoming bigger and bigger and bigger. And we felt that like, okay, this is like, the right time to go out there and start building something in this space. Yeah, no. I think you're you're right on there. I think now more than ever, what we were seeing is teams saw the value in putting data products into production as quickly as possible because they realized the direct correlation that they have with business outcomes, the more effective they can put data products in the production, the Better Business outcomes, they get and it's not just now, the Silicon Valley companies right? You think about it. It's all the non-tech first companies, all. So saw this value there, right? But the tooling didn't enable them to do that, right? So I can tell you how much time we spend, like, thinking back my time at Tech time, like, helping customers debug through their spark logs, right. And working with them, to figure out, you know, we would go and onboard them and these warrant Silicon Valley companies that run sparklers to perform. So, here is spark 101. This is how you this is a spark. Config this is
 You go through and get your boy, you're in for a tree and and at the end what we saw is the main thing that's evolved from you know 15 years from now and like you know the brilliant guys from databricks went out and built Spark.
 Is that there's a couple things that have really changed since that time, right? And one thing like Hardware's evolved tremendously, right? Which is like a cosas. And I always talk about this is that, you know, at that time distributed workloads were a lot more necessary, right? Because you were a lot more limited on the resources, you can get through AWS. Now, you can go and check out, you know, a huge instance on on ec2 and and duck, DB's come in and data fusions come in and they've kind of flipped down on Linux World on its head where you can do a lot more in single note, right? And we saw that at techton too, where we'd have these
 These clusters but and you'd be running like 100s of gigabytes of workloads, that weren't really necessary to have distributed spark clusters there, right? You can really do a lot on single node.
 So that's one thing, one thing and I think one of the catalysts that we were kind of identifying the other things ai's come into the picture too, which is totally changed the nature of workloads, right? It's no longer only about structured tabular data, which we have a great platforms for right now, in a more or less a solved. There's a lot of challenges there, but now there's unstructured data. There's a lot of text Data, there's images, there's videos all the different modalities there that these platforms just weren't built for right. They're first class citizens are structured tabular data. They do that really, really well but they don't have, they're gonna mix the capabilities. They weren't building from the ground up with first principals having these characteristics of AI workloads in mind. The other thing that like AI did was actually acting like, it got the least to the Monica's in the industry because if we take and see what was happening again, like in the hospital 13 years, we have the analytics Market, which is huge.
 But still is a buck and kind of War rights. You have reports that you need to build and deliver. These reports are consumed by people who make decisions blah blah. But it's not a product. Like it's not the customer facing product of every company else there, right?
 Now, these things started changing gradually but it was changing slowly. We had the male male was like part of that because when you have recommenders right now, you the data becomes the product itself. Like you need to use the data in order to recommend and increase your Revenue, right? And so like bi doesn't do that but the link between
 Let's say that how the data is used and the value that's created, it's like kind of hard like to identify, right? That's been a traditional problem. And I know that there's so many teams that have talked about that. Like, how do you Champion for the work that you're doing? If you can't draw a direct line from what you are doing to revenue, generated or saved 100%, And the special, the ways that data teams are build? I think in the successful things, like take it take a component, like lift, for example, right? Or Uber again, Silicon Valley like data driven combining. And if you like the data teams in there, they layers that you have of people from the srees, that's they take care of your AWS account. To the data platform people, who are making sure that there is always capacities there for your power, spark classes to the day lines, in there, who are making sure that the data is delivered. And then the analysts and the ml people, and the data scientists on top.
 Like the guy at the bottom has no.
 Idea how like his work is used actually to make a decision of how like to improve like the revenue that you have like, right?
 And that's like part of like what John was saying also that the model that these Technologies were billed and the requirements in terms of like the talent needed to maintain the next scale them, right? It was not sustainable. If you want to make it accessible like to everyone in the market out there, right? Contrast that to how we build up, right? If we were building up like SAS applications, the same way that we did with data.
 like the industry would be, I don't like one tenth of what it is today and like the people that are working there are probably would be
 2% of what is today, right? So
 While AIDS because we started getting into the data becoming more. And more of a product is that the AI came in and said,
 is everything about data.
 Now whatever product, we are building one where another like consumes data and spit sales data and it's not just the data, we were used to as like Beyond said. Now we have also unstructured data that we can process like in ways that we couldn't do like before, and there is an opportunity there to make this Technologies. Even like more accessible to more people. Yeah, it's easier than ever. Yeah, exactly. So AI kind of like accelerated, that's what I'm saying. Like it's a cat. It's not like it wasn't happening. Like all great is. The industry was leaving the SARS and selling the software as a service as the way, like to build value and getting more into the data is becoming the driving force of the next iteration of the industry. But AAA means accelerated, like 100x mean by that exactly the leaving this as because I've heard so many different versions and viewpoints of that idea of
 like SAS is not the way forward. Yeah. So,
 If we think about that, we need to, you need to be old like me like to do that. Okay? Because you have to seem like the how the market developed in the past, like 20 years. Okay. I'm revealing a little bit of my age now. So what happened with let's say the the past from the 2010 to 2020, right? Where we had all these companies like like the sales force became like really big. The work days of the world out there. Right. What was actually happening is that we took all the activities that humans are doing and we tried to automate them using like software and deliver that software over the cloud, right? The cloud was important because the cloud allowed for Ephesians is like in the finance Health Point of View that like you could do it like with other delivery methods, right? And that's held like grow a lot and commoditize the software at the end, right?
 So we are using taxes, right? You go out there. Hey hello or
 Call someone to come and pick you up. No. Now you can use an app like to do that, right?
 So we kind of created platforms or esops, for example, have like Shopify, right? We created for its activity like humans do like in business and their personal life software platforms.
 To make this process like more efficient, right? And that was like, delivered like through SAS.
 We kind of saturated that I mean like okay I don't think there's like much left out there that we have and turned into like a software Platform One Way or Another, right?
 So if we want to keep the growth of the industry, the way that was like okay, what's next?
 Now, the magic of like turning every process in the software is that you got a lot of data, you capture a little bit of data, right? And this data is like sitting somewhere and now the question is like, what do you do with this data?
 So,
 The first thing to be I reporting, right? Like do your finances like bi, by the way, like for people don't know, like the first owner in the in the business of the I was like the CFO because it was like, primarily the first need was going to build financial reporting, right?
 We came into like okay. Now let's do marketing reporting, let's do sales report, blah, blah, blah, like all that stuff. And then well, you know what? Now that we have all the interactions of these people, with our products, maybe we can build recommenders and now, we can automate and send like an email and be like, hey, you know what? I saw that you were looking into that stuff. I found this one that might be interested, right? And this can drive literally Behavior the world going by more.
 That's Based on data, right? So we have all this data and we have a lot of fun structured data. We have all these customers supports that people are like, calling like thousands and thousands of like people to tell like their issues and all that stuff but we couldn't like
 work on that before or at least like easily enough, right?
 So there is the opportunity out there to take all this data. It's like an on top like opportunity to turn more and more value out of that. Now the question is okay, can we do it with the technologies that we had? And actually, it was like, it's kind of hard.
 and then AI came in, like, actually a little lambs game in, right, and they offered a number of tools and most importantly, in my opinion,
 To a much, much broader audience out there than ever before. So people now they can work and do things with their data, that previously was like, really, really hard like to do. You had to be
 In Lyft or Uber or like apple. Yeah I like that. Highlighting how mature the company's data?
 Teams and just overall team and vision of how to use their data needed to be before, AI came and then you relate that to app Builders and how simple it is to build an app versus how simple it is, to put a recommender system into production. Yeah, yeah. And I can give like a an example, like, for people like to understand how big of a gap there is between the maturity that, let's say, the album building part of the industrial thousands of data on, right? So in Madden like today you have from the engineer builds, like the front ends, they work in react, blah, blah blah, whatever they are doing there. You have a bucket engineer, they're doing their stuff, like we Firebase or like, whatever and you have an app that you can put out there right now, these two guys, there they are working. Each one of them like is working on their own thing, right? And somehow like they can merge their work and have an app like working at the end. Now,
 Out and YouTube probably have like more experience than me on that.
 Try to contrast that with what is happening, like with the data teams, like the data scientist is going to build a notebook.
 And what happens? Next, you have this notebook to data engineer and what is the data engineer doing rewriting? The code to put it into production. Now, take these and put it in like up building.
 And how many cells we would have? If we had like to literally the front, end developer hands, the code to the back end developer in the backend developer going to rewrite the thing in order to release that. Right? So that's like the the difference between like the level of maturity that let's say the one side of the industry has convert like to to the other one and AI is that great equalizer.
 Actually, it's the Catalyst. I would say it's not there and I think like a big problem that we are seeing today. We say AI is that getting things in the Productions like really, really hard. It's great, like, for demos like we can't build demos
 Very impressive demos but getting into production is really hard in the reason for that is because the tools that we have and the engineering practices that we have are not there yet to deliver, let's say the same capacity of delivering software with the quality that we need as we do like with building applications.
 It's, it's funny. Yeah, you mention that. And how
 You're you now have a new persona that is able to access that. So that is that Catalyst. And it's it's like the market is demanding that you have the ease of use because you have all these back-end in front-end Engineers who are able to use Ai and they're coming in and then they're like, okay, I got something 80% there, I can show a demo, I'm good. But then you're like, so let's, let's use this in our app. Yeah, and I think part of that is like, kind of what you're describing is if we take a look at the AI Journey that companies go through, right?
 Chat interfaces. That's the primary thing that came out that is still I think the dominant way that people interface with llms and AI. Right. And so you have let's say interactive. Yeah. That's where people tend to start building apps building AI products. Right. Go and grab link chain or they're going grab a llama index.
 And and really expect either have like a human in the loop or you know, interactive on the other like an agent waiting for like an AI response that comes from these llms right.
 And I think that's where companies tend to start, primarily, because that's where most of the tooling exists. Now then they tried they already start seeing value, right? Okay, we're building this product, it's adding some value. But now I really want to go in scale this like, how do I now do AI at scale? How do I actually build a product in production and the challenge. There is the nature of all. These models are non-deterministic, right? And so, a lot of the concepts that were once used for data teams with structured and tabular data don't necessarily apply for unstructured data because of the nature of non-deterministic. And so what are the theory is, what our thesis is is that we really want to create and help companies and build a platform that can help them create deterministic Pipelines.
 Up with intro, with using very familiar interfaces that they're already used to in Concepts on top of non-deterministic models for example, right? And that's where a lot of the challenges that's what really the problem set that we're excited about helping teams do is how do you, how do you build a lot of clarity, and a lot of stability and reliability, and help companies. Actually take that next step, where I think about how do we scale our AI products and how do we put these pipelines into production in a resilient in a stable fashion. And that's where a lot of the tooling is kind of missing, right? They kind of start doing the one-off interactive Ai and then when it comes to putting this actually in production at scale, you know, you're you're talking about Concepts like context, windows and tokenizing and partitioning and chunking, all things that are new, muscles that data teams really need to build to be able to run these things. Reliably in production even just is anthropic working right now is the pi. Yeah, exactly. So what are the latencies like right? And then
 All the new versions of the model is coming in introduce new things for cost optimization. Right? Inference is expensive right time to First token. Oh, there's so much, that's so true. There's so many new metrics that you're looking at that. You previously had not been exposed to and the platforms that exist out there, don't treat these as first-class citizens which means that teams that have great engineering teams are diying it, they're saying, okay, how can I go and build something around these constraints that I have and around these properties of non-deterministic models to try and simulate how we're currently interacting with our structured and tabular pipelines, right? So, they'll, if you're like a great spark shop, for example, you'll go in right. These complex udfs which are brittle heart to maintain and, you know, on top of spark and not taking advantage of all of any of the performance at spark has to write offer because it's not running a distributed fashion, or you'll go and build some complex Logic on top of Lambda functions where we're trying to hit the apis of these models.
 Um, but you know, lots of engineering Costco's into that that's hard to maintain. And so teams are trying to piece things together to try and help them run AI at scale in production. And so that's where we see that there's a really nice opportunity. Is how do you build a lot of these properties really natively into an engine in a platform that helps companies go and scale their Ai workloads and as much as everyone and I think it's true like agents are here and you know 2025 is the year of Ages. We're going to see a lot more of these come into production, but I also think you have a lot of teams that are starting to think about, okay? Like
 You know, I'm collecting all these transcripts, right? I have, you know, a call center of 60 people that are doing thousands of calls per day and I want to perform semantic analytics on it, right? And our view is inferences, the new transform, right? It can be used very much in the same way as what teams are used to when they're building struck pipelines for structured data, right? It's a very powerful form of transform so if you're trying to do semantic analytics at scale where there's no human in the loop,
 how do we enable teams to really take advantage of this new powerful form of transform? That's their
 I like that, I've heard that before from the head of AI at WISE, he was saying, you know, we should be thinking about these different llm calls as just a way to take structured, unstructured data and turn it into structured data. That's what, that's what you see, that's what we're trying to to help teams really doing. We seem to think that there's things they're doing that already on their own, right? But can you build something that treats this as really the first class problem that you're trying to solve? And, and let's take a moment real fast to highlight the difficulty of that scale in production. Because I remember I was talking to my buddy who works at decagon, and he was saying, you know,
 To get the agents working for the customer service stuff that's not really the hard part. We had a working demo up and we were able to sell within two weeks of when we generated the idea later, what's really hard, and what kept us up at night is how do we do that for a company that is receiving thousands of customer service requests per minute? Yep. And that's where batch inference becomes really relevant, right? And really prominent there, but then all. So what you were saying is like, you're taking all of these transcripts at scale and there's a lot of things that you need to think about that are properties of, let's say, the AI models, right? So the contacts Windows, right? So if I have this transcript, is it enough for me to take each individual message and pass the llm know? Because it turns out that you need to have the context of the entire conversation very going and applying arbitrary buffer around each individual message. Well that's going to have you increase your input token account which is going to make
 Things more expensive. So, there's a lot of these nuances and complexities that when you're trying to run things at scale, you need it. You need to give people the expressivity to be able to one experiment before pushing to production. How do we build this? And kind of a notebook and
 Test out different prompts that we have to make sure that the data quality is coming back. But then also what you're saying is true is like how do we create structure of the unstructured? Right. So really long transcripts Blobs of text into nice neat structured tables. Right? And so and and that's where the true power lies because if you can take things and create structure out of it, now, there's a lot more that you can do to control input, token, count, and and you can do a lot more on understanding, what the context window, and that you want to send to the llm. And so and then you get into things like rate limiting and like modeling seas and model cascading. And things of that sort that are also really important when you're running things at scale, it does feel like to if you're doing that kind of stuff in spark, you're over. Engineering it? Yeah. Yeah. 100%. And if you're building it on spark, right. I think it it just
 The architecture that was built and it's a brilliant super powerful platform, right? But it wasn't built with unstructured data in mind, right? And so, and so, you're not getting a lot of the guarantees that you can get. If you had a platform that was built for and around unstructured, data processing, I also remember, there was this guy Zach, that was putting an agent platform into his company and he was talking about how one of the things that they did was they were trying to allow everyone at the company to build their own agents for whatever the use case was. And I know that's a common thing that most people are dealing with. They're saying we know that each individual department needs agents and we know that these agents are going to be better served. If the subject matter expert builds them. So, how do we create a platform that can allow folks to build their own agents? And one thing that he did was
 He created a metric for when someone is building an agent. So that hey, if you put this into production and wherever you're going to expose it, the expected traffic is going to be this much and this is the expected cost. And so that's a fascinating piece. But then the other side of that is, I wonder how much you have been encountering, folks. Like, the people that we talked to at process that say, we don't really look at cost in the beginning. We just want to make something that is working so is it possible? Then we go into, how do we optimize it? How can we Delight the users? And then can we actually like optimize that? That's a great point because their results. So he didn't complexity that's we didn't have before again. We like the thing with AI is that we really have like to think how we're building software. So one of the things that AI
 A name is that the people who are actually the domain experts. Now they can, they have to be bought the technical like building the technology itself. I give you a thing, how we were like building software before, right?
 We will get, okay, our customers. We have like a product guy. We have a Persona. Let's say, like our Persona is, like, salespeople. We try to understand what the salespeople need. Then we'll create some prds. We'll go to the engineering team like, hey now, you have to build that. How many story points? Yeah. How many story points?
 Roll out like the software, give it to themselves and salespeople.
 Somehow will work, okay?
 But now, that's not enough, because this behavior of the software depends on the sales person directly, right? So how do we involve that person right in the process? Is like something that I don't think we have figured it out yet.
 And also how we we let like the engineer's keep doing engineering because we still need engineering, right? Like, it's not like we can't leave in the world where
 Everything is like kind of random on the end of the day and the sales people at the end of the day, they are still sells people. They get paid for selling shit. Not for building software, right? So yeah, yeah, that's fine. So it's not like okay, you're not getting a bonus this month because your prompt was not good. Dude, like we can't do that. Right? So there's like I think like a big challenge specialty for the people who are building, like, more customer facing products. But thankfully, the salespeople don't have to write SQL though, right? They can just write prompts, which is like and like that's easier for them to be able to go and let's say, iterate with and having to go through and try and do like the old business analysis, but also it brings up the point of this is built for engineers. It's not built for sales folks. And so the fact that like you were saying playing around in or jupyter notebook, trying to give that to a salesperson is not going to work. How do you create an environment that
 someone is able to natively go in and it's it's like
 Do you focus on the lowest common denominator of someone who is technically apt at being able to do something? Or are you trying to push for, how do we enable our power users? The role of like the heads of AI that we see now coming in and becoming very common at a lot of companies. Most companies that we talk to have a head of AI at this point, right? And really, I think the interesting part about that role is doing exactly what you're talking about. Is trying to bridge that Gap. They're essentially like the AI Sherpas within the company, right? If you think about it, they're going to like the head of product marketing. They're going to the head of sales. They're going to all the other business functions there and they're saying, hey we I I can, I'm the gatekeeper for AI, what do you want to try and build for this? How can we try and add value to your teams to be able to leverage Ai? And so the head of sales will say, hey look like we're losing out on a lot of deals and all of them information, we have is in this sales slack channel that we have, there's a lot of data there in context around
 Why did we not hit our numbers over the last week over the last month in quarter? Right? Can we build, can you build me? Some pipelines that are going to go through and leveraging llms try and identify the patterns in there so that we can then go and improve and increase conversion rates, whatever it is. If I know the sales team, I know the answer and it's probably marketing marketing, fucking sucks. But then you go to the marketing guys and they're like, okay. Like look at the sales guys with marketing, they're like leads, right? That's kind of the the thing, right? And and I think there are there will be more advances there where it's like, okay how do we get? But at the end of the day, like do you want your head of sales to be going in and trying to work in a platform and experimenting with different prompts?
 I think there's a balance there, right? But that's what these I think heads of AI, is they're trying to do is like create the AI roadmap based on all the feedback they're getting from other, like, business functions within the company, which is like a really interesting. The sheriff is a great turn. So there is a ux box, which is like a big things that needs to be like, think of dog. But there is also the, like, the platform, like the entrust rocks or side of things. Right? And I think, and going back a little bit like, to
 Like why you shouldn't do that. Like, on spark, for example, right? The thing with with AI and a little lambs is that the workloads
 Changed their characteristics, like dramatically because of llms and inference and gpus.
 So in the past again 12 13 years. Everything was like pretty much CPU bound.
 Spark is like an amazing tool if you want to crunch numbers and make sure that your CPUs are always like, a operating under 100%. Like, that's what they're trying to do. First and second, okay, how we move data around? Because how we Shuffle? Because we need to move from one server to the other. And if you have thousands of them, then how do you make this reliable? So it's something breaks, we can resume.
 Now.
 Put like a little in the equation.
 CPU doesn't matter anymore, because your CPU is going to see the idle there, waiting for the GPU to return like a result, right? So, from a CPU bound workload, we're going to like more of an IO bound workload.
 Then.
 Okay, you're building your UDF, you're running these things but guess what? There's zero reliability. When you are talking to gpus, right? Like GPU because the systems are not mature yet. We're kind of I would say how kind of like the internet was like before 2000. Like literally like back then, you know, like you would connect on the internet and give your mom was like, picking up the phone like you would lose your connection right? Like does this happen today? No, right. But the kind of what happens like with a little aging yourself again because it's okay, it's okay. We can discriminate on me when some friends.
 But then with the llm, like okay, you send a request, you send, let's say, I don't know. Let's say you have
 You're using Jamie and I don't have like 1 million tokens like context window. You said like one million tokens there, right? Starts doing its magic there and
 the middle of that, it breaks.
 What happens? You can't resume that.
 Right. There's no that literally happened to me yesterday. I did that exact same thing. I'm pairing up all of the attendees for the event tomorrow, and I'm sending an email to them saying, hey, you should know each other because I think it would be great. And so I asked trying to get a new new company. I'm trying to be that. Exactly that. And so I'm actually Gemini without me prompting it to, it says, why it paired up each one, but it's taking a long time and halfway through I said, all right, well I'm gonna go get lunch and it should be done by the time I get back. I got back. And the whole thing was just not there anymore. It magically disappeared. So I had to prompted again and then ask it to create it again. And you already used, you already wasted costs on the input tokens for the first time that didn't work, right? Even before you read the whole thing, it's like the reliability thing, right? Because think about. Now, you are doing these, like, 12 hours before your event and it breaks.
 And you can't send the emails, like we are not going to be in a very happy position, right? Imagine you are AT&T, and you are going to process all the transcripts of the previous day, right? To create tickets for your engineers, for the next day, like to work and we are talking about like, probably tens of thousands of hours of transcripts that you have to do there in that breaks, right? Whereas the SLA, there's no SLA, there's no delay right now, there's not a link, right? And data teams are used to slas, right? That's how that's how they get rated and qualified. And like, are they doing a good job? Are they hitting their slaves, right? Are these pipelines getting executed in the amount of time that they need to throw llms into the loop? It's a whole
 So wild, that's the wild west, right? Exactly. So you have this super reliable CPU focused technology, that is spark. You put a little lambs in the
 Equation there, and it doesn't work anymore. Like the liability. Do you expect from something like, spark? It's not going to work and it's not, it's fold, right? Or your clusters are going to just shooting their and just waiting for the GPU to return something back. And the reason I'm saying that is because people might say, okay, why not just go and like build on top of that, right? For our, like, a lamps. And the reason is that because
 A very, very soon. And again, let's say you have infinite money, like you don't have a problem with that, right? You will have an extremely hard time creating reliable systems that will reliably deliver value to your company. And the moment that happens, people will lose faith in the technology itself. And we see that like with AI, right? Like people do get the loose like face because they don't care at the end of the day, like they're not doing it just for the technology. Like the technology is like making their lives harder like why they would care, right?
 Not everyone is like, you know, like a tech junkie in like, Silicon Valley that have like to work with towards like Advanced general intelligence, right? I think it's a, that's an important thing too, right? Easy. We talk about that, that 70% of AI projects, never make it into production, everyone that's building or like trying to innovate in the ice, space, always famously claims that thing and then they fit in somewhere under that umbrella for why? The problem that they're trying to solve is the main reason for that and and it's all based on conversations, right? That they're having with these heads of ai's and teams to try and figure out and pinhole what the problem is and there are multiple layers to that, right? The one that were very focused on is taking it and running AI at scale in production is a very hard unsolved problem that teams just really need to struggle with because they're used to the world of things working relatively well, right? They don't always hit the rest of LA's. But when they have these
 Planes that are running across large data sets, you know, they have on calls they have processes for being able to run these. They have retries, they have orchestrators like airflow. That are kind of good at helping, run that retry logic and build all of that. And that's where we think that they're. They're AI. Like the new workloads that are introduced in the new paradigm that we live in. Now, the AI, the infrastructure behind it needs to be rethought. Well, it's funny you mention that because I remember back in the day, my friend Diego was telling me about how he felt like there was going to be this new term for folks that focused on reliability but specifically for the ml systems and he was like maybe we could coin it as the ml re and stead of the SRE it's the mlr but now it feels like what you're talking about is the AI re that AI reliability engineer who is going to be 100% heads down focused on. Can we make this reliable and
 That goes into all of these new metrics that you're talking about. It's not just does. Can we look at the logs and traces and pipe it into datadog? Or can we like have that kind of analysis? They want to be able to feel confident around their pipelines and be able to even think the next look. Can we put an SLA on this right? And and given the current tooling it's very very hard to do that based on the ergonomics and the tooling that's there and platform that exists for working with these models, right? And so that's the AI infrastructure. Kind of opportunity that we're thinking about
 Something about like their roles that's exist already because I think that there is a gap there. Someone has like, to fill this Gap. I don't know if it's going to be the like AI engineer. Although my experience so far with AI Engineers is I feel like they're coming more from the data scientists aside which is great. Like you need like people who can like model and other somehow to work with models and the end of the day like creates something that's delivers. Well has to be delivered there.
 But I think there's like a huge opportunity there for people space only from the day, and Engineering background, and the melons engineering background.
 To step in and actually take these role. Because at the end of the day there,
 Reasons that the existed was to add reliability to working with data because working with data was always an unreliable business. It was always really really hard, right? And the job of these people were to make sure that
 Everything is delivered on time and is with the quality that we need, we have slas, we have all these things and if you want to understand like a group of people, you have to look into the language that they are using, right?
 and,
 One of the most commonly used terms for deadlines in the potency, in the potencies, like the attribute of a system that if you put the same input, the output will always be the same, right? That's great. Because when you have that, you can ensure that there is reliable system there, right?
 Now.
 That's something about it Engineers that they literally breathe and believe and exists for reliability. At the end of the day, the same like with ML Engineers. But the ml Engineers, I think they have not added advantage that they've already had to take into account that we are working with systems that they are not deterministic, right?
 And llms get this like to think to the extreme. So I think these two groups of people, they have like an amazing opportunity to actually transform themselves like into something that will be.
 Extremely important for the future of the industry. And if they don't do it, someone else will do it, right? Like, it might be like the SRE. Yeah. I was thinking 100%. It's a lot of folks that were in the MLS community in the beginning.
 Came from that SRE background and they were tasked with figuring out the reliability of the ml systems and the ml platform. And then you had this, the rise of the platform engineer and a lot of those folks were srees that were rebranded and you have the platform engineer. Now, you are probably going to start seeing more and more of these AI platforms. And one of the jobs that the AI platform engineer is going to be tasked with is exactly that how can we make sure that whoever's buildings with AI can do that confidently? And I hear a little some people special like from the data science Community world because yeah. Like if your whole existence is around, that means right? Like when something comes that's like so
 A different to what you are doing like your initial reactions like to reject it, right? And that's like the biggest I think danger for them right now like you see a lot of like rejection. It's like oh where deadlines you need. I was like, leave us alone without like llm stuff. We are going to do our thing. You crazy, AI engineer, go, do whatever you want. I don't want to know about it. No, because
 If you.
 Go. Like you don't focus on that.
 And you are okay to feel a little bit uncomfortable. You have a tremendous amount of value that you can deliver. Because reliability is what is literally like what is missing.
 To turn like llms and AI into like, what they are promised to be. So I think there is like, for both data engineers and the male Engineers like a huge opportunity here, they need to link. I mean, like it's not the existing tooling enough for that stuff, but the tooling is different conversation. They are not, it's not their job. To build the tooling, the industry should build the tooling, but the most important thing is the mindset that they bring, and the experience that they bring, and that's something that not tooling can do, right? So these people have, like, literally sitting on like gold, but they have like to do something with that. Otherwise, they will meet like a huge opportunity in my opinion. Yeah, I've heard some people talk about how they can't connect the dots, you hear. Everyone banging in and screaming from the rooftops on how AI is only as good as the data that you give it and garbage in garbage.
 Out. These are like the tropes that are so common and then I saw someone but
 explain to me how that's possible. Because right now I just go in and I give a prompt and there's no data that's going into that.
 It's just the prompt. And so I was trying to put two and two together to really encapsulate why it is like that. And on one hand you have me just going and doing one-off tasks with Chachi BT or Gemini. And that's used as a bit of a productivity or I'm talking to it. I'm trying to learn something new. I'm trying to understand something or I'm using it more like a browser. I'm asking it to tell me these different things, that's not necessarily a data product, you have, but then you have products that the company uses and like you were talking about with the support and all of the data that's going to be going into the context window. There is not something that you're doing one off with. That's something that should be very operationalized. Yeah. So and so here's like an example that we use one of our early design Partners to help kind of crystallize this too. Like for a use case, right? As like we talked about it, this team has, let's say 60 call center folks.
 It's a insurance tech company, right? And whenever you go and you get a new policy for insurance, you get this thing called a deck page, just what the call it in the trend. It's a declaration page shows, like a summary, all your coverages, and all of your policies there.
 Now, the problem here is that if a call center representative misrepresents, what's in the decks page. So let's say there was someone this is an example at Coastal is like, you know, I have roadside assistance and it says in the deck page, it's only for 15 miles but the call center representative during the call of the customer tells them that it's for 50 Mi, right? That's a liability that they're then taking on and could potentially get sued for that because they misrepresented what the actual policy is, right? And so, how do you build these pipelines that are going to let say, and this is where the data quality portion comes into, you want to have the the declaration's paid structured in a way and be able to manipulate it through using some nice, like text and chunking and partitioning and capabilities along with a transcript side by side and be able to go through and and look for and filter all of the questions that
 By the customer. And then the answer is that the support person gave them and then match that to the portion of the deck page that they're actually talking about, right? You don't really care about like the niceties the Hi. How are you? Like, you don't want to feed all that shit, shit. So, okay, to say into into, like the llm, because gonna be more expensive, you don't want to just take the whole transcript in and of itself. You want to be able to partition and chunk. It only send the relevant information that you need in order to understand whether the customer support agent represented what was in the Declarations page accurately. So you're taking the question in the answer and you have the expressivity in the tooling to be able to do that and then feed it. And that's where the data quality portion comes in to feed it into the Alum, to make a decision as to, whether yes with the customer support representative said is correct, or no, he actually missed represented. The actual amount of roadside assistance. This customer had was 15 miles as stated in this portion of the Declaration phase, right? So then you want to
 Create that Rapport very quickly, that there was something that they misspoke, or they misrepresented and send that up and escalate it to the team that can then be proactive around. Handling this case, so that they don't end up getting sued in court. And, and having to pay out for this misrepresentation, right? And so this is an example of what teams are thinking and trying to do at scale, right? Where it's not like I'm sending this response immediately back to like an AI agent or there's like a human in the loop. Sure. You want to have some what real time which is like an overloaded term. But like, you know, it's within the next few hours is totally fine to be able to do that, but you're getting these transcripts that are coming in, you know, thousands a day. How do you go and actually build these pipelines in a way that is trying to create some determinant as them on top of the ability to work with these models. And so contacts, Windows, chunking partitioning, all these things we need to arm.
 And Engineers, AI Engineers data Engineers. With the ability to actually build these in a real bust Manner. And then give them some of the guarantees like Costas talking about that.
 They're already used to write like I need each transcript, the medium time to have it being reviewed needs to be or the meantime to have a review, needs to be three hours, right? And so be able to give these teams, those guarantees that within three hours of a customer conversation happening will know if the representative did well, or if we need to go and and fix things on the back end, right?
 I want to add something here.
 there's always data like you can say that there's no data even if you want to keep it just to the oh wow, I'm asking questions to the llm about
 I don't know like how to change like my baby's diaper. Yeah, right in a proper way. Yeah. Like your prompt is the data actually the structure of the whole Dialogue on its own like it's important.
 Just because of how the limbs work, right? Like a little limbs like you.
 Take also like the previous conversations that you say like you feed it back there, like so you create data, you feed to that.
 The llm itself is built on data, right? And then anything that is
 Let's say outside of like the trivial things of asking, what we would ask like on Google for example.
 It requires like extra data like there is a reason that tools are becoming so important like we wouldn't have agents if we didn't have tools, right? A lot of the work that we are doing with tools is actually fetching data.
 now, it might be, let's say if you are using cursor,
 It cursor didn't have the contacts of your code base. It wouldn't help you.
 And what is your code base? In this case it's data, right? When you use clothes colder, whatever it's called and you're like hey like find me the file that does this or that in my code base and it runs a tools does like a find in the grip.
 It gets data back, write your code base again and the outputs of this tools. Are the data you see that all ready, we are creating actually something that I think like the deadline engineers and the engineers again, will be like very Farmers like we're building pipelines of feeding data, getting data out and use that like to the next day. Blah blah blah blah, right?
 The Deep research functionality, it's pretty much. Okay, I'll search on Google or Bing, I'll get the row HTML, that is returned from the queries that I sent there and I'll work on that that's data. Again. Anything non-trivial using like an llm requires data.
 so, I think
 The way that we think of like a little lambs of, let's say these kind of like Oracle that like it's not, it's not accurate. Like yeah you can do that. That's like part of what made them like so successful because it was like so easy for people to experience something by just like talking to it.
 But at the end of the day, what we are doing with a little lambs, is that
 And let's go back to the to the SAS example, right? So what we were doing was like building software and we were forcing people to learn how to think and operate the way that the machine can do it right.
 And now we change the equation there, because we made the machines be more like thinking and working, like the humans do, right? That's what's like mixed like so accessible. But at the end of the day, we still have a machine that has like, to do something. We tell it like, with natural language to do that and they're very generic, right? They can do like many different things but still they are going to do the vacuum, right? And everything is driven by data at the end of the day. So
 Even if you are even in the online use case where you are just like chatting with the board and asking about things, right? You will copy paste, something you will take a picture from showers be like, hey like, but the CSS here doesn't look good. Like look at this picture that's data, right? The difference is like okay how they're like the things that you do just you as like the materials and like Gemini and your Excel seeds with the attendees there.
 And there's the company that has to do that at scale every other day for all the new leads that they have, right? So lots of different approach you can't like do it like with chat boards anymore and that's what we are talking about like in how you put this thing like in the production.
 One thing that I feel like we need to hit on. If we're talking about reliability, is
 Evals and how you think about?
 Reliability in the context of evals and getting that all. So like where do they fit in in your worldview? One of the problems that we have is that
 the first iteration of Evil's platforms were inspired primarily from
 I would say more of like the engineering that happens in the application layer.
 So you think about like evals like in the common case you have model and input and output, okay? And that's what we care about.
 So you're saying okay, I put this into this model. I take this output. Is this output. What? I expect like to get
 Now the problem is that in my opinion with these model is that there's a lot of context that is actually missing. Write a special in case is where you have to involve many different models like what if a goal right? So let's take the case of processing a transcripts, right? What's most people do is okay, the first part is, I'm getting my audio file.
 I'm going to use something like whisper, and like, a transcript. I'll have some issues. Right? Like, the output. I mean, it's usually really good, but still things that need to be corrected.
 You get this big chunk of like takes that you have there.
 And then what people do will like, okay clean it up, maybe use a little lamb to go and fix some of these issues. So let's say something is misspelled. Write a little like great like to go and find these and like fix and but still they can make a mistake, right?
 Now, this is like, on the very, very first stage of like processing that, right?
 Now, the next step is. Okay, let's start like creating some summaries, so we will break it down into some pieces. Create a summary for that store, the summaries then. On another level, we'll take all the summaries. Create a summary of the whole thing. And we will end up, let's say with the summer that you will put on your like websites when you put the podcast episode out there, right?
 Now.
 If you.
 Consider, let's say.
 the evil, just as
 Individual steps there, right?
 You are having like the problem and you can't evaluate the whole pipeline of creating like going from the audio to the end result, which is the summer that you have their right to do that, you have to trace all the calls and you have to consider all the calls and you have to see, maybe the llm corrected, some references in there, made like a huge mistake.
 And that's changed completely all the summary. Right, I'm exaggerating. But the thing is that step of the beginning, right? Can affect the result of the end, but if you take like a calling, the middle must still be perfect, right? But the data was like wrong. So of course, like the output was like wrong. So the question is like, okay, how we can
 Work on that. How we can build these more complicated workflows and I think,
 With, if you take into consideration like the, like like agents, for example, it's like even worse because like the agents can make tens of different calls go back, like, run calls again. And how do you evaluate the output at the end? Because that's what I see at the end, right? Like I asked, Claude to write some code for me, it can take like a few minutes.
 Who knows what is doing there, but definitely they're like many back and forth and calls like to the llm. So what do I write?
 So I think this is like an important things that is missing and we'll get there, but I think we need to rethink also like the infrastructure that we are using for that because now we're talking about like a lot of data and data that it's, it's not like a unit test. It's more of like how let's say in the observability world, we were doing like traces over, like, distributed systems, right? So that's what I
 I want to see out there and I think that is going to change a lot like how people work and how they can build actual reliable systems and Incorporated. I think as we talk about that theme of 70% of projects, don't make it in a production. That's one thing that we hear from that AI leaders out there, right is great. I built a lot your you tell me inferences a new transform, right? So now I'm counting on the results of these pipelines to be Mission critical. They're making business decisions for me. So, so they say, how do I know that this pipeline this multi-stage pipeline, like cosas is saying this multi-stage pipeline that's going through a bunch of different inputs, and outputs into llms and ended up making the right decision for me, right? And that's really what top is top of mind for that. And now these eval platforms that have come out, there's lots of them, they all have different flavors and different angles. I would say.
 Very important part of building trust in your AI pipeline, right? But really, what we need to be able to do is if we have these multi-stage pipelines that are running in production, we need traceability all the way up. So, how can you Traverse up from the final decision?
 What was the, what was the input for getting there? But then also, what were the stages before that? And how do we provide visibility and observability to AI leaders to really build confidence? And understand which step of the pipeline was actually the wrong, did the model not perform well, which then propagated down to the next stage of same thing for like AI agents, right? Like they have 10 steps that they need to go and do which Step of that of that AI agents workflow. Does it wasn't not good. Do I need to go and tweak The Prompt? Do I need to go and modify and iterate on? Right and so that's where I think this next stage is all. So for agents, all for these multi-stage production pipelines that we think are very important for businesses to run to and you need it, you need to provide them that level of confidence. So you can do things like give confidence score is and things like that, that models have. But what Coast is saying, is true, is you need to have all of the outputs and the reasoning Behind These models, and how they made decision and give them a really nice interface and tooling to be able to then go.
 And review it very quickly and know what they need to go and iterate on order to get that output data, quality to be feel the good, like maybe it's not 100% of the time, they have confidence. But when they're running these things in batch at scale, if we can get 99% confidence that the output for this Mission critical pipeline is good, that's probably good enough for us, but there's there's tooling it in for that needs to get billed for that. And so that something here because that's I think that is relatively little Lots, the previous generation, let's say of like data infrastructure.
 So, like a thermal that every deadline is in here and probably like every data practitioner is like, familiar with is like data Linea, right? So everyone's thinking like very important part of ensuring, the quality of our data is like keeping track of the linears like, how okay, have these end result here. There's a report
 How these reports came into what it is when there are literally hundreds of tables that we have to operate on like to get that, right?
 And the bears that when we're working in a fully deterministic world, just keeping track of the column level, he's enough because knowing the data type is enough to reason about what is happening, right? And it fits also will like with the colonel. Let's say like nature of these systems. They all up systems that we have.
 But you can't do that anymore, like slightly differences, small differences in like the input that you put and the prompt that you are there which you didn't have before, right? As like an additional piece of
 Data. There contains dramatically like the output that you get on the other side. So what you need now is more of like row level units which is a very hard problem and it's not something that
 Has been.
 Developed primarily because it was hard enough and not needed enough for people like to invest in that. But I think that that's something that we are also like working ourselves from that. I think this is part of how you can make people create like a traceability there, that it will definitely change completely how like the quality of the evolves that you are doing. I'm not sure I fully understand role level lineage so they're all level line ads. Is that, let's say you start, let's take again, the example of the transcripts, write the output of your like whisper model is going to be a blob of text, right?
 Now you might do a few things like okay, break it down into pieces, chunky, that whatever.
 Now, the next step is like, for each one of these.
 Chunks that you have. This each chunk is a row, right? This is going to be fed into an llm with a prompt that says, do you find any references there here for the names of the participants that they are like mistakenly transcribed? And if yes, fix them, right?
 Now.
 Your first row got into the llm and the second will do the same. The third will do the same blah blah blah right now the next step is that you are going to take each one again of them, right? And create a summary of its chunk, right?
 So you see that it's row goes through steps of processing but because of like the difference is in like the data that you feed up the role level, right? And the how the prompt might change for each one of them, you might have like different results. So you want to be able like to track that.
 Something that you didn't you didn't need like to do like a tree traversal exercise, you wind up in this Leaf node but then what were the the notes before that? That led to that, right? So that's what you want to kind of Traverse back up this multi-stage Pipeline and see.
 So if let's say you have your end summary, right? Because at some point you get all these like mini summaries and you put them all together into like an element and you put the output, which is like your summer and you see the summer there and you're like no, I don't like that.
 So now, you want to go backwards, right? And you're like, okay, what contributed, what data contributed in this particular output that I have here, right? And then you see, oh, it's like this Five summary mini summaries that I have.
 You want to be able like to track these five and recall them. So you as the human or whatever like a Velma in your Machinery, you're using can have access like to this particular data.
 With the lineage as it was before, you can't do that because the lineage is just keeping track of the metadata of like, the columns that participate, right? But now you can't
 Do the same thing because the actual data has like a big effect. It's not just like the data type. It's not something. Let's say it join breaks because
 I was expecting a data type of integer and it was a string, for example, something like that. Like, again, just like making things up now, it's much more like you have to get into much more detailed views of the data itself to understand how the llms at this stage, operate to give a good or a bad result, and you have to be able to navigate that and the data infrastructure does not keep track of that.
 So it's not an information that you can recall and that's like one of the things that I'm saying that it is a big problem to solve because it's not just like the evals from point of view of like how scientifically do an evolved that has validity. It's valid. It's also how to capture all the data because that's overhead store, all the data without overhead and process all these valves that now like explode in terms of like the number of you have like to do there, right? What your talking about is
 The logs and this data lineage that we have is not sufficient, it's not painting a good enough picture for us because even if we know that yeah this call went through successfully or this data was transformed in this way.
 We can't get that granularity that you're talking about and so you don't see that as something.
 Of a job of an observability tool. It's like the same thing as we had. Like, if you think about like the data platforms, right? So you had something like databricks that, or spark that would go and actually execute the whatever logic you deploy their, right?
 Then you most probably would have another tool that is doing, like, analyzes the line ads or like that's like the quality checks, blah, blah, like whatever. Now,
 take the QA of data and little linear's there as input and substitute the name's with evals instead of QA, right? And the linear still has to be there. I think what people, I think one of the problems is that we, we've been building applications around the little limbs for SATs modalities primarily, right? So, of course, when you do that, it's all about, okay? That's the problem of the user. That's like the output is like, a good one, right?
 But when you start getting into an environment where the interactions with the llms for an outcome become much more complicated than they have dependencies between them, right? You have to expand like your understanding of that to the whole pipeline that is built like to do that, right?
 Again, you might need.
 Probably like a different tool to do that, but still this different tool. We need needs to access data that the engine that does. The processing has to capture, which on its own is like a hard problem.
 And then it has to work with.
 It will face like the same problems that observability in the up world have, which is like a lot of data.
 The value per piece of data is not that high. So we have to be extremely good storing these data to make it affordable and then,
 You have the additional problem of like a false being slow so in the expensive. So again, how do we pick the right valves like to do
 and what kind of tooling we have like to give to the users to build this plan of LMS of the end of the day, right?
 A, I don't know what I'd love to see.
 How this is going like to come out? Because I think it is like
 A pretty lucrative space to build although I know they're like, very like as in any other llm related activity, like, thousands of companies trying to do it.
 But I think I still believe it's like a lot of noise and not that much like that. And again, that's like, well, I'm trying to say is that I think it's like tremendous value for like the data people to go and build.
 a solutions for a little lambs because Primal is driven by like more like application engineering people and there's
 A lot of like foundational stuff that comes from the data world that they are needed. If we are going like to be building with the lamps and I wonder in your time talking with folks
 It feels like right now because there's so much open space and there's so many new pieces that we're trying to add to our platform and our ecosystem of putting this into production.
 There's a lot of things that you could do, where have you seen people focusing on what absolutely needs to be done.
 Before we can do anything else, like, what are the main bottlenecks? Do I need to go out and get an evals tool? Do I need to go out and get a proxy or like, AI Gateway? Yeah, the eval tools are important, right? You need that feedback loop, right? To be able to help you understand if you're building effective AI if the output of what you're doing, if it isn't what you're expecting. Why isn't that, right? So this eval 2 is provide you that feedback loop to be able to go and do that. You know, something that I always think about when you're talking about tabular and structured data world, right? Is engineering teams were really good at like building, like Canary builds for example, right? That go back and try and sense. Any sort of regression that happens in these pipelines that are more or less deterministic because you're dealing with structured in tabular data, right? And so we'd run these nightly Canary builds and the output of that would be like, oh, there's like four percent drift or like 5% drift because you introduced some regressions by adding some application code that actually
 A call some Drift from what we're expecting to have the output of these pipelines, being, right? That's that same kind of mindset needs to be applied now and thought about and that's where I think we're talking about the opportunity here is like, how do we take that same concept and allow teams to be able to build these kind of canary pipelines, for example, on top of output for non-deterministic models. And that's a very big challenge, because the nature of the data is totally different. You're dealing with lots of text, right? These valves are very expensive to store and process and and build Insight on top of right now. The more complex the problem, I think the bigger the opportunity from an engineering standpoint and in the more fun it is to go and solve it. And that's why I think it's kind of wide open for helping teams build that, right? Like imagine if you're able to like the transcripts example, right? You you you're able to sign certain scores and confidence levels to the output of the pipeline that you ran on a daily basis based on certain properties that you consider to be
 Successful outcomes of the pipelines, right? Which is what kind of canary builds were built for, right? It's a very deterministic way of being able to evaluate your software and your pipelines. And so applying that same concept to unstructured data processing is, I think going to be very important in a huge unlock for now, ai and data teams feel confident around putting these and leveraging inference. As a new transform in production, everyone was like to do something with AI, right? Everyone's like, oh, like okay, we shouldn't listen to. That's like, why? I think that's one of the big reasons behind having the role of the head of AI is because there's a mandate like from the border, like whatever that we need to do things like with AI. We have no
 Idea what to do. But it's a race because all of our competitors are doing it. We need to be on that. So we'll bring this poor guy here. Who's job is like to go and find use case. He's so he will go beyond said like to the marketing folks, but like, hey guys, like how can I help you? I have budget by the way. So that's great, right? Like you don't. So, let's let's do it. Sales people. The same thing like engineering, the same thing like product. The same thing,
 And then they have like to go and build something.
 and,
 So the first thing is like, okay, many components are still at the stage. These where should we focus? Like from all the different things that we can do, like, where we should focus, right? Which project like we should run and this like, typically converts that are like early in the journey,
 then you have cases where
 they build the demos but you get it's hard to deliver consistently the value to whatever. Like the stakeholder is because again, salespeople, don't care about your signing technology. Like okay, you're going to help them like you have to help them, right? It's like and you are talking about business lines here that they are like asked one day because they can be. It's like I have a quote I do like or I'm going to lose my job so like, can you help me with my quota or not sweet? Not if you help me set 30% of the time I have to think is that like worth the time like doing it and potentially, you can mess up what I'm doing 30%? Oh yeah, I'm because you're sending the wrong emails or you're sending some hallucinated jargon 100%. And then there's, I had like, some, we had like, some interesting conversation with folks that they were saying well we run like a few experiments and our experience was that like I give like an example. Let's say we have tickets, right? And like support tickets.
 And we want to be able to extract and like first of all label them somehow like into categories and then extract some information from there that can drive like our products like decisions, right?
 So what they usually do is like they find the company that provides labeling with a little lambs or whatever as a service.
 The go there. Like, okay, I have to take Almighty tickets. I have to upload the tickets here.
 I'll run this thing something comes up. Well, it's not exactly what I expect. Kind of a black box. Yeah. Like I do rate I rate iterate actually like we did like a lot of
 Prompting gymnastics to make it work. And actually one of the very interesting feedback on that was, like, we needed. Like, when you are doing a classification, you still have the problem. Like you have to tell to the llm like was the classification scheme of that, right? Or use an Alum like to figure it out. But still, you have to figure out what are the classes there, right. It's not like so, no. Actual, I will just come out and be like, yeah, that's what you should do and like, shut up, you don't have an opinion on human, you know?
 so,
 We did that took like sometime. We ended up getting like an output which was like a data set in CSV that we could download.
 it's like, okay, that's like
 too much work.
 To start because here's the thing like when you get your labels that's one like the real work starts, right? Like you still have to go and figure out insights out of this like labels. So the guy was like okay I can't do that. Like I can't like being like in the process of like downloading uploading.
 Getting csvs, put them like somehow like on my snowflake and then have another list also like to go and done analyze these things. Like, okay, that's not going to work after work. Exactly. So
 it's a lot of like, I think like the the kind of it's like a probably, like more like a product mistake, but I again, people need to understand like they need to meet the users where they are and not try like to force them to do things that
 Do not feed their like, workflows because a product person is a product person again, like he's getting just by the business based on the product work that they are doing not the labeling that they're doing. Right? And this yeah, it's potentially a huge distraction. Oh yeah. And it is it is
 And then you have the companies that the managed to get to the point where they have things that they work.
 But then they're like okay how do we put these things like in the production which on its own is like a big, big conversation, like what that means and what risk it puts to the project itself because again like the people on the other side, they're waiting results, right?
 And then they're like a very few companies, I think like, okay. It's like, usually these are like Fortune 20 type of combinations or very like Silicon Valley, high tech companies. Which by the way, many of them, they say, they are AI, they are not really AI, right? But there are some of them that they are doing, like they put things in production like seriously, but we are talking about, I don't know, like, maybe 10, or like low, hundreds of companies out there that have, like, successful in the other thing we see too is the common theme. Is everyone gets wide eyed when like the head of AI comes to them, be like, what can I build you with AI? Like, just tell me, I have a whole team will go and build this guy's gonna be great. I got budget, I got everything going, whatever you want. We'll top it off with the Cherry for you and they'll go and they'll come up with these ideas, they'll create Theory, AI roadmap for the quarter, the team will go and execute on it, and then he turns out and says, yeah, we build it, we spent the time on it, we gave it delivered to them with a cherry on top and a nice bow and they don't use it.
 Right. And so, I think a lot of it all, so has to do with you really need to go in and identify, the high value use cases that are business critical for you, right? If you get something that's like kind of supplementary and not really easily, easy to integrate and you're like in the day-to-day workflows of like, a product marketing manager. For example, they're probably not going to use it, right? And so, and so I think it's very important when you thinking about considering and taking on new AI projects that the business outcomes. And there are key success metrics that are there, right? Like, okay, I, you know, like content moderation, for example, is another big example, besides, like, the transcript thing, right? These, these companies that are building communities where the users are applying content and they're going through and spending tons of money on content, moderation teams, which means
 Humans are going through and reading every single message.
 In order to understand, whether the the meat and whether the message was safe or not to be published into the community, it's all about building, very safety. So, if there's any, like, racist undertone or sexism or things like that, immediately want to disqualify office, profanity things like that too, right? And so, when you have a use case like that and you're spending hundreds of thousands, sometimes millions of dollars on hiring this Workforce that's just literally sitting there reading messages manually.
 Now there's a business outcome Associated, how can I reduce my cost, right? Because that model doesn't scale. Now let's say I want to expand it a different locales, right? I want to now offer this and Portuguese and I want to offer the same community in Spanish or French. I have to go and build out these content. Moderation teams that are French speaking, and all of that, right? And so, LMS are great at that kind of use case too. And the value is, is very apparent there, right? Like now no longer do I have to go and spend millions of dollars on the hiring? These folks, I can dedicate that cost towards inference build these really robust pipelines that are helping me do content, moderation, and even get better performance outcomes in metrics. Like my my meantime to review a message or a conversation is no longer eight hours because I need to wait for the content, moderator shift to start and France or whatever it is, right? And so, I think that's one of the key lessons that we're seeing a lot too. Is as your company thinking about it bringing
 Building on AI use cases, make sure that it's good that there are their key success metrics and business metrics that you're targeting and that you're able to track as you're putting an AI into production for it. I heard it like on a x and y axis. And I think the guy's name was Sergio, that said it was when I was at a, the Gen AI conference in Zurich, and basically, he said, put on an X and Y axis, the impact, and then confidence that you can actually implement it. Yeah. And whatever is the highest up in that top right quadrant, that's what you should start with. And the conference, you can Implement comes down to the tooling and infrastructure that you have to do it, right? And that's where I think we're seeing a lot of innovation going into now. It is helping build that confidence for teams to be able to say I I know I can put this into production and it's a very common. Now the only other yeah. Now, the only thing question is, is there business value?
 And that's potentially up to the stakeholder to be able to decide that, right? Yeah, there's something else. That's like, I want to watch out because we see
 You know, having Bots at these kind of like a blazing in the carousel, at the same time like for these themes special like in the market does these right now because one of the problems that we see is that okay? These are like the Innovative. AI is not necessarily like technical person themselves, right?
 Ah so they rely a lot on the teams like to find the tooling that they need. Right. And obviously there's like a lot of offerings out there.
 Now, we are still at the stage.
 Of the industry where like they're like so many vertical eyes. Like very specific tools. You know that do like one thing and they're pretty much like a black box, right?
 So they have money so they go like, get that stuff and it's good because the way like it helps you Kickstart like your project.
 But there's like a huge drop in that you can't engineer systems with black boxes. You can't do that. And people need to understand that like no matter what like a little empowered like there's two engineering on needs to happen to make them like robust and deliver value at the end.
 So my advice like to the people is that you should try and invest more on the infrastructure and your knowledge of like how to build the right things and what practices will drive you there. Instead of like oh you know what? Like okay I need to see are something here. Let's go and like, use every black books OCR thing there that says that they are
 5% better than Mistral or like whatever out there, right?
 And do it for like, prototyping 100%. But I guarantee you that if it's successful, you'll get to a point where you'd be like, okay now what do we do with this thing? Because it's either like too slow, it's not reliable. Oh, now like we are getting outputs that like, we can't really understand why we're getting the outputs that we are getting or. Oh, now we're getting into different use case that like different documents that these models are there. Using here are not probably like as good as they are used to be. So now we're are we adding another tool that we have to manage and who's going to do that, right?
 so, I think
 Engineers should keep thinking like, Engineers, invest, like in tools that they are.
 Let's say a good with like they are in their infrastructure for their work and they will build the value on top of them. And one last thing on that,
 There is a reason that these, like revolution of AI goes through, like the engineering.
 Practices first.
 The reason for that is that they work that we are doing as Engineers is this validate, right? So you cannot like if I asked like the yellow limb like to build me a function.
 It's almost like you can almost automate figuring out if it's like working or not. Like you run, it, you just run it, you compile it right now, they're like many.
 In super impactful.
 Problems to be solved with all the lamps that you don't have that. If you are doing the things that like you only was describing was a transcript, right? How do you validate like with like 100% confidence? That this thing is going like to work.
 And data, and that's like something again. I'll go back to the data practitioners and how important their knowledge and experience is, like, to make this successful because they know that data drift
 That was always the case. Like, there's nothing that you can just build it once. Put it out there and it was going like to be working forever and that's going to become even more true like with a little lambs. So you have to build systems. You can't just like throw black boxes in there and makes things like work. You have to be an engineer, you have to engineer this and you have like to keep iterating on me. That's like the data drifts as like the needs drift. Also like from the users and with llms is going to happen like
 In the much, more accelerated base. So again, like focus on core skills and infrastructure.
 I want to connect the dots on two things, that it feels like we're dancing around, which is
 that business value and the
 skill of being able to sniff out that business value and understand how to properly implement. It is one thing. But then going back to since 2020, I've heard almost
 everyone that has presented at an mlops Community event talked about, in some way, shape, or form in different words, saying how
 I can build the best model I can build with the highest accuracy. Score, I spent five weeks tuning it. So it went from 98.1 accuracy to 95 or whatever that like metric is inside of it. But then I gave it to the people that were going to be using this model and they didn't use it and it fell flat on its face and all of that time that I had spent on it was for nothing. And so the whole idea of being able to make sure that what you're building is the right thing and that you are spending ample amount of time getting it into production as quickly as possible to know if it is the right thing or where you need to tune. It is so important and that's like what you're saying this here is get it in there, start working with it Engineering in a way that you can then go and debug it when you need to figure out if it's not working because we've absolutely missed the mark on the product or we missed.
 Mark on one of the steps in this pipeline. Yeah, it's not about the models anymore. The models are going to keep getting better, keep getting more accurate, probably cheaper and a lot more performant, less hallucination, all that kind of stuff. It's all around the infrastructure you have around it now, right? And your ability to have that really tight feedback loop to be able to know and build confidence around the outputs of the pipelines that you have and be able to trace all the way back to be able to iterate and make improvements incrementally on that. And that's where we lie now and like the AI Innovation spaces. There's lots of great Innovation happening towards that and I think in the next couple of years, it's going to be a lot more prominent where we're starting to see teams. Feel like you're talking about the X and Y access. The confidence level is going to be going way up because of the infrastructure and tooling that's being produced. Now, I want to give a quick shout out Demetrios. You you are me. Yeah, you know, you're the ultimate Community Builder and we
 Costa scenario. Always super impressed and admire like you just seem to be everywhere all at once. And we're like, oh my God, I don't know how he does it so, and and it's like amazing seeing the price of the community and like, how you've been able to grow it and all the conferences that you're doing, so like lots of respect man and and it's been been really fun just like over the last few years seeing all of it happen. Well I'm glad that we got to make this happen. Yeah and thank you big. Thank you for making type Def and our company happened. Like it was one of the things that's super hard when you're building a startup is like finding co-founders and we know we're both second time Founders, we put even more emphasis in it.
 So, we don't take for granted the fact that you made us meet up at some random blue bottle in San Francisco. And then, it worked five minutes into the conversation decided that you had better things to do, it rarely works, but this time it did. So I'm going to put that on my resume now. Yeah, so thanks for everything, man. Listen and communities. How always been like very important ones in Building Technology and they will always be. So,
 That's another kind of like service that you provide bringing like the people together especially like with when you're solving problems like they are not even well-defined. Like at the end of the day, it's all about like emerging patterns, like through like people, interacting, who are passionate about, what they are doing and like trying to like to find Solutions. So that's like probably like the most important thing. And I think it's beautiful too, that you mentioned that people come from different backgrounds. Like the data engineering background, like the modelling background data science like the SRE background and getting this space. And specifically, this space getting to see how each one of these folks is attacking. The problem is really cool and it makes for fertile ground for Innovation. Oh, 100%. And I think we want to succeed, we need to somehow like, increase the cross-pollination like this communities, and that's your job to do obviously, but they really like tremendous value like bringing like these.
 Diverse. Let's say engineering disciples like together and like trying to because he has like the what is like very interesting and kind of like, well, I am excited with llms is because when we talked a little about my age but a little limbs like let me make me feel young because it reminds me like how technology was when technology was young you know like people.
 Complain today that like, oh, like this thing is like not reliable, blah, blah, blah blah. But they forget that in order to get our databases to be transactional and probably kids don't even know. Today, that's like, there is like transactions that they ensure that when you write something and I write and journey, right? It's going to be the correct thing. It looks like Decades of research and development to get to that point, right?
 So we kind of like we are again in these like early stages of like a new
 Potentially very transformative technology and it feels nice. I mean, it's not easy. But yeah, you get back like to how it was like hacking with networks around and like networks, not been reliable and not like having fiber at home, like each one of us and have liked them to think that there is a root somewhere, right? And
 There is.
 What one of the bad things that fast did was that?
 It managed to hide.
 From the vast majority of the engineers out there, the complexity and the efforts to make things rely.
 But it was always about that. There was no technology that the data did was produced was reliable. It took a long time to make it reliable. The same thing will be also like with other lamps, right? And that's where engineering comes in. And, but we are not like in 1995 anymore where like in 2025, and there's like, so much experience with all these different disciplines and like bringing these people together can really, really accelerate and make the things that took like decades to happen before, like now happened like in stuff like a few years, so go out there, bring them together. It's top of mind and Innovation being here in the valley like thinking about Ai and infrastructure and the new world that we live in now. But building, as a community, is also very important having the community one, but also being able to contribute together and innovate together, right? So part of our launch two for type def as we're open sourcing. One of the libraries that really amazing at going through and helping people build and let's say like, jupyter notebooks and inter.
 Thing with llms very nicely. And so that's one of the things I think is a huge help in being able to move the pace of innovation forward is once you can have a project and have multiple projects, and everyone's contributing. There's a lot of excitement, it helps build a lot of momentum. In that space, he's gonna be the pets.com of the llm bubble. Okay? I'm not going to say like a name on that, but
 I do think that's the problem to website company is they are going to
 Be something to have a good Awakening.
 Yeah, yeah, I think so. I'm I think we need them. Like, we need the beds. Don't come over the world, like to happen for things like, but, you know, that's like if you reflect back like the.com, bubble, like people say, if you think like what happened back, then it was like an extremely vertical, Eyes solution, that was built for like, pretty much like everything.
 Crossed. And then,
 People realized that we need platforms, right? And then he puts calm, but more. Yeah. And you have like soapy and you have been pretty well. Yeah. Where the Amazon know? I don't know. I digress. Yeah. My my pets.com is
 Whoever has that fucking billboard? That says Don't hire humans. That is on whoever those guys are
 Well, there's so many things that you have to do to get a billboard and the fact that it got so many marketers and top-level people to sign off on that billboard. I don't even know who it is, but I know that I don't like them. Like, I don't know, like I
 I personally feel that anything that feels like too easy cannot be real.
 I might be wrong, like I'd love to be wrong for myself, at least, but
 Like problems that they are valuable. They tend to be hard, you need to put their Force. Like, you need to work hard like to make them successful. So there's no easy path to. It's like the little limp just because you are doing a little limbs like you are going to be reads. Like it doesn't work like that. I think there's tons of
 Eval companies that are out there now too. Right? And I think they're solving very hard problems but we might start seeing some consolidation there too. Right. As you get think about the the main observability and big players out there like the data, dogs of the world that are also very much thinking around, how do they go and integrate into AI? So that's gonna be an interesting thing to see. I think they're still cropping up like there's lots of different eval platforms there but the ones that are solving the hardest problems. Like, kind of what Coast is describing I think are the ones that are going to be able to really stand on their own and substantiate their, but it's a very important part of the AI life cycle, right? The problems at the third going after, I don't know that we need like 100 of them. But it'll be interesting to see. I think, you know what happens in that space. Generally the track that over time. Yeah, I think like a little, like, I think the good thing was evolved components, is that to build and all components requires, like a bass line of technical competencies that the end of
 the day the teams that they build something, they will have some kind of
 Like volleyball Lake seats. Let's say a little, like, at least like the values, don't like to be destroyed, there are combines that they will destroy value. Like they're all like combine is that will end up like the what was the company. It wasn't like a
 the.com era company was like much more recent, the one that was doing the
 One click, check out how fast your record in like burn. It wasn't no.
 Both was the one the other one that was doing. Well, fattening was fast. Calm. Yeah. The one that had like the broken record of like, how much money the burning like, yeah, or something like that. And the founder went on Twitter and was saying stuff about stripe and how is the mafia? Yeah, of course. Yeah, there's always someone to blame you want to but still there's value that has been destroyed, right? Like so there's always that like in the industry, I think it's part of like any fast pacing and like high rewards space. It doesn't mean like, everyone is like a scam or like anything like that.
 Well, you have a son again, dimitrios or just a one-time thing.