Spend another minority. Yeah, just in case, make sure we exhaust every last Brain Cell of mine and that shouldn't be hard.
 so, we should probably kick it off with when you go to production, something fails and
 You're trying to figure out why it's failing especially in your AI system. It's not the easiest thing it's not trivial. And so I know both of you have seen some of this. You have thoughts. Let's kick it off with that.
 so,
 We're we're focused on building an agent that root causes alerts in production. So an alert fires in your pager Duty or slack. We have an agent that can diagnose that by looking at your production systems or observability stack.
 Planning, executing tall is calling apis and then reasoning about that until it distills down. Well, then information into a root cause or at least a set of findings and so I can tip a few challenges that we've run into, so one of them is
 Ground trees, like a lack of ground, truth and production environment, unlike code or writing this, not just this Corpus of like web information that you could just download and then train or online. So you need to figure out whether the agent even success, success successfully solve the problem. Like, how do you know that? So you need to find ways either user feedback but sometimes the users don't know. Like, if you go into engineer and say, you know, is this the root cause? Oftentimes, I'll say this looks good, but I'm not sure if it's real and so the verification is also then a secondary problem. Effectively, the thing that we've learned is you need to, as much as possible, get the human out of the loop, not just from the doing the work but all. So the review process and the labeling and feedback process, because otherwise, you succeed or fail, but you're still the pendant on them. Blocked on them, to prove your agent or your AI system. Ultimately, what you want to get to is a loop like a learning Loop or failure in production and incorporating that back into your email system, and you want this to be very fast, you can order of days or weeks or months.
 It needs to be ideally hours or minutes if you can.
 Yeah, so we can dive into some of those areas, I'm sure. Yeah. So I do two kinds of areas.
 Or search.
 By the way, one is.
 Like with llm. So if you have lots of lots of unstructured data like PDFs, transcripts text Data, how can you extract? Justin vantiq, insights and aggregate them and make sense of them, turns out that building pipelines to do this, kind of has all of the same challenges as building AI agents. It's just AI agents for data analysis. So happy to chat more about. What does it mean to build evals? And this does it mean to incorporate tool? Use how do you interpret play, you know, methods like retrieval and having to walk play? Look at the data itself and of course you know what, what I think makes data processing, a very interesting, kind of petri dish for understanding llms and how humans and systems. Kind of all interact is that when you're doing data processing, you don't know all the data that you're trying to make sense of the AI is kind of telling you what's in the data. So verification is so hard here. You're not only verifying the transformation like the
 Insights that are extracted, but also that they were extracted correctly that they exist in the data that you didn't miss anything. But how do you know if that L1 missed extraction? If you don't know what's in your data. Yeah. So there are a lot of really interesting morning just there and then separately, you know, because I feel like we have this pretty rich lens on how all of these problems work, I'm teaching a course with homo. I'm all the same on AI evals in general. How do you evaluate, how do you build AI applications that you feel confident in deploying? How do you build evals around them? When you mention your question, what do you do with something fails in production? Like, that's a horror story. If you haven't even thought that out before you deployed to production, right, you need to have some sort of blocking in place, some metrics are measuring. Like, it can't just be like, oh, somebody said it fail. And then, you're like, hands up in the air? No, no way to start, right? You shouldn't have gone there in first place. That's a, really, how do you go from zero to being able to be?
 it feels like there's a bit of
 overlap here with the fuzziness. You don't understand what's in the data. And so you can't really tell if what is coming out is correct. And then all the same with on your side. Like we don't really understand if the root cause has been fixed or not and so that fuzziness.
 Is.
 In a way like how do you bridge that Gap? How do you do it? Besides just
 Well, I think it looks good.
 You have one like really reductionist way to look at it is that if we're in the production environment like in a cloud infrastructure of observability is like a lot of information time series. There's logs, there's code, there's chattering slack. And what's really trying to do is really just the first step is information retrieval, its search and you're taking a very sparse information. Let's spread out everywhere and creating like, dense like gems, like finding out of that, that is contextual to the problem at hand. And I think what we've tried to do as much as possible and I'm kind of curious, take if this is possible and hurry, use case, I actually want to see the difference between the use case as is we flatten the dependency on the agents, the parts of the workflow. Essentially if you bold agent steps on top of a Gente steps and if a base layer is wrong, then everything above letters wrong. In our case is, sometimes it's still works up because the agent can go on a trajectory that is wrong and still stumble upon a finding that is good and bring that back, but I think I'm curious.
 In her case, in your case does that work? Or is it just then it's a catastrophic failure to the user, most of the use cases were focused on? Are more batch, ETL style query. So we're building the system called donkey TL where you can
 Of it is writing mapreduce pipelines, over your data, but the llm executes the map. So the map is not code function. It's a prompt that is executed per document and the llm, also executes a reduced operation. So we do a group by and send all the documents and llm, and it does sort of aggregation there. And this sense, there's not really retrieval. That immediately comes to mind. Retrieval can be thought of as an optimization. If you know, you can understand what your map reduce pipe. Your your problem expressed as mapreduce is to map out insights, that are relevant and then Aggregate and, you know, summarize how they are relevant to the query.
 You can imagine some of these Maps can be, you know, retrieval like execution. Like you can use embedding to determine filter out before you use an llm to kind of extract the media, insights, from each document. And then that sense I would think of retrieval as an optimization putting my database have on, of course,
 So just curious at what layer does the llm come into play, is it planning the map produce operation? Or is it within the operational self? It is within the operation. There is a layer on top.
 Goes from completely natural language. Query to a pipeline of mapreduce filter. I think, like spark, but every operation is llm. Okay, and that is the natural language that comes from the user. Yes. And in a sense, I I find that we've done a few user studies on us. Nobody really wants to go from NL to pipeline because you're you want to think about your workflow as kind of a data flow? Yeah. And anyways, you're writing prompts for map reduce filter cluster, like, whatever it is Data operations that you want. So it is low code, no code. Where people really struggle is, you know, they've implemented. So you implement your application, your pipeline and Doc, dtl for your use case, which you can do. It'll just be really slow. It's, how do you go from initial outputs to improving the pipeline?
 It's how do you even know that the agent has failed because of a Down an upstream issue and then when you know something has failed, how do you encode that? Observation back into the prompt, right? Like you kind of have Vibes that are like oh this is like not quite right but to specify that what we call bridging like the Gulf of specification, that is the hardest part that we see every user do just trying to come into play non-technical. So if I'm understanding that correctly it's basically like I see something's wrong but I don't exactly know how to fix it and I'm trying to fix it by tweaking some prompts or I'm changing out a model or I'm doing a little bit of everything that I think could make a difference. Yep. And it may or may not work. It may or may not there's that and there's a specially in data processing. And I think this probably also true for your case, there's just such a long tail of failure modes when it comes to AI, they're like failing. All these bespoke ways and keeping track of
 All of that and synthesizing them to like, okay, what are the three concrete instructions that I need to give the L1 llms are great. If you have very detailed props. They are horrible with like ambiguous prompts great like for 90% with an ambiguous, but to get that last 10%, you're out here, like giving examples giving very detailed instructions, like maybe that's just me. I don't know. But I mean, in each case, this is a really good one. Yeah, too. Because I think this is where a lot of teams think. Well, it's easy. I'll just build an agent. Just Fork this open source thing and I'll just work and you get to some level of performance, but getting to, very high performance really, is really, really hard. Depending on the use case, I'm interested in like the HCI perspective as well, because I'm curious. What are the control surfaces that your users have? Like, what do you get? Because if you think about like mid Journey, sometimes the most frustrating things, like it's just takes and you just get back random thing every single time, so it's like, at the casino just so frustrating. Yeah. But like, if you have in painting, then suddenly you can take an image and improve it or even the new chat GTA.
 Image, and right, you can specify using a spec what you want. And that's a different level I and all interfaces or just inferior. I guess to something more strong. Yeah, so I'm curious like, what are those interface is today? And what do you see that going over time? Yeah.
 Okay, so now I have to go into history lesson. If you ask Stacey, I question the perfect room for that and that is great. So, don Norman who's written a lot about
 The first one who came up with these golf sculpts of execution, and evaluation are the one he came up with and any product, not AI product, any interface requires users to like kind of figure out how to use it, right? They have some latent.
 Intent in their head, like, say, it's something as simple as you know, a booking a flight. You booked a flight from Germany to here. You need to like four figure that out in your head. Look at the Google flights interface and kind of map that out and executed and then see what happened and then make sense of it. And this kind of loop was very hard to do in the 80s and 90s. We saw this but what AI brings that's very new
 Is now this, this gulf of, I'm going to call it specification because it's just easier for me to understand in that way, it's broken down to two things. One is, how do you communicate to the AI? And then how do you generalize from the AI to your actual data or task? Because you can have a really great specification, right? You have a great mid-journey prompt and that gets sent to the AI, but there's the gap between the intent. That's well specified. And the AI, and it gets wrong. And then you're like, oh my gosh, should I do something wrong? So I think the first thing that we have to do is a community and this is something that we're talking about in research circles too, is just recognizing. These are two, golfs both need bridging but the tools in which we have to do that need to be different. So, for golf of specification, I in, at least in the docket TL, stack, we're building tools to help people do prompt engineering to Mark examples of bad outputs and automatically suggests prompted improvements. And the goal is to get a very complet
 We've had so many years of humans interacting with computers that we figured out a way. Like it wasn't that hard for me to book the flights the interface. Exactly the interface has been polished. But now when you throw natural language into it, it is so much harder because a we're not used to it and B. It is very fuzzy again going back to that. Fuzziness of. When I say a word, it may mean one thing, you may interpret it as another thing, and the llm can just, it's that going to the casino and you're playing slot machines. And I think this is partly not just the natural language element of it, but it is having a model of what the system is doing. And if you can't figure that out, then you're just stuck at like the starting line. So, if you take like a cursor and you doing some, you know, software development, if you understand that it's just doing rag over your code, then you have a better mental model and, you know, okay, I can introduce these balls that can index that or if you know that the tabs that you have open or, you know, weighted higher. So having that model,
 All affects how you promptly in the agent and the history too. Like, I know that I have something exactly. Or exactly at some edit. Yeah.
 Yeah, I don't know why if you feel like you have to know those things. Yeah very difficult and I think that's most like a lot of people make is they it's not that they want to infantilize but they want to abstract this from users and just think, okay? This is just like a black box down, worry about it, but it makes it much harder to use their products. And another thing that we were experimenting with is sometimes giving the user a little bit less and giving them more, ux affordances that allow us to get more feedback from them. But these are orthogonal. So it may make the products slightly harder to use. So for example, we might give you, these are the, like, findings the key findings, but in like a midi style, we will give you buttons that say expand for more information or search further using this finding. And so if we give you something that's free useful if you click on that and expand we know. Okay. This is actually good if you keep ignoring something to give you then we know this is bad and so there's some implicit feedback that we get back and I'm not sure if you're incorporating anything like that or yeah. So
 Easiest thing to do is to get binary, click or don't click yes or no. And then to be able to kind of drill down on that with open-ended feedback, one of the things that we did that was quite successful to help people write prompts for dog. ETL, pipelines was always have an open-ended feedback box anywhere, where you can kind of highlight the document or output that you think is bad. And just stream of thought why, it's like a little bit off. Oh nice color code that or tag that that lives in a database and anytime you invoke that AI assistant or you we also have a prompt Improvement feature which has can read pretty much all of your feedback and suggest targeted improvements. So the prompts are visible to, these are, yes, the problems are visible. I I don't think we're out the state yet, where there's anything better than riding prompts for steering, especially for data processing.
 I think if you have a better scoped task, it's possible that they don't have to write the prompt like in your tasks, your very specific you're searching people's logs, helping them do root cause analysis. But say, you were using docket TL to write those pipelines. You absolutely have to write the prompt. I guess this was just a trade-off where, how much control do you give the user? So we give them like a control surface, like takes input where they could inject some guidance either globally or contextually. So, on a specific category or class of alert, we can inject some guidance. So maybe there's like an isolate burn rate alerts. Then you can attach something contextual that they would say, always check data documents, always check the latest like conversations, always check the system because it's always complicit or involved in some way to the failure. And sometimes you need the users to give you that, God is so much context. It's just latent in their heads that they need to somehow encode in your product. What it reminds me of is
 When you download a new app on your phone and you have these moments going back to, there's that gap of, I don't know how this app works. And so you're kind of swipe around and you press buttons and you figure out. Okay, cool. I think I know what's going on here. And so having that, but now we're working with text, and we're working with prompts. And so being able to really figure out what's the interface that's going to best interact with the human, and the text. And so that expand button is I really like that because then it gives a signal it kind of just gives you a little bit and then you get to know, all right? But the thing is, you can't give them all the information, then they get them like a summary. Just enough to like, get them to kind of more. Yeah, and many times actually not even useful people don't click on it, but that's very good signal anyways. Yeah, yeah, that's helpful and same with the like High.
 Lighting the text and being able to just stream whatever you want. Yeah. And what I'm thinking about is when you have that like highlighting the text and then streaming of Consciousness, how are you then incorporating that back in to the system so that it learns and gets better from it? And that's
 That is why I think you need AI assistance there. Because users cannot remember all of the tail that long tail of failure modes. That is why we have a database of feedbacks. And when users need to improve prompts or want to do something new, we can suggest a prompt to them because we already know things that they care about because we've read their feedback and we always provide a suggestion or diffs to their prompts and then they can click accept, or they can click reject or it's re incorporating it into the prompt and and all the eval set. Or yeah, email sets right now, we don't have a good workflow for it. We're still playing around with what we think is best. Yeah, like generating synthetic data or having users. Currently users would have to bring their own data right now, it's fascinating to be like, all right, cool, there's insights, there's thousands of insights here because of all this, toying around with the output that I've had.
 I have left, my my precious human time has been taken to leave insights here. Now, what do we do with them? To make sure that they are incorporated into the next version of whatever I build. And so, having an assistant and being able to suggest, well, you might not want to do that. Because remember, you said, you cared about this and whatever prompts that didn't work out well. And it's not even that we find that I think in
 Any Wheat. It's not just data processing that has this problem. It's also code generation. It's also like bit Journey or image generation, but when you're starting out with a session, you almost always do some exploratory analysis. There's a term for this in HCI called epistemic artifacts. And it comes from how artists use tools, like, if they are given you paints or new medium, they're going to play around with that before they paint their thing. And all of the interfaces that I think we build in this new journey, I
 A like arena for lack of a better term, need to have the ability to quickly create epistemic artifacts like when you're in cursor, you want to try something out and you want to be, if it doesn't look good or if it doesn't work, you want to be able to toss it. Yeah. And keep moving forward. I think that's one of the big failures today. Yes. Sometimes the biggest failures. Yeah, some of the costs are just too high to experiment and people just kind of back out of the playground or this often just not a playground available, it really makes me think that this is a UI, ux problem. And it's very much in the product of how do we make it as easy as possible for people to not have these? Oh, I know a little black magic on cursor because I understand it's a rag, and it's the tabs. And if I copy and paste it something, then it does better. That is something that should not be like.
 The gate kept, right, right, so, how do you in your product design? Something that is very much keeping that away from like, oh, you have to know if, you know great, it's very difficult really models. These kinds of ideas or interfaces is that there's so many entry points and to bridge that Gulf, right? There's the Gulf of the specification where you need to externalize your intent as fully as possible and there's a gulf of generalization which is you need to make sure your prompt works like, regardless of rag, regardless of like, whatever hyper parameters that were selected, and right now, humans are specified, or humans are relied on to give those hints for both the Gulfs, like you have to know how rag works. So you can give the appropriate hints to bridge the generalization. Like, that's crazy. Yeah. Well, one of the things that we intentionally made of decision to four, which we originally started with like a slack.
 Like an agent. That's in your slack, a teammate, essentially and we actually started my helped disc and we're Fielding questions from engineer. So, one engineer be like a platform team, supporting another engineer coming in with, like, a question and getting in between Engineers was very hard because there's a lot of like chit chat. There's a lot of back and forth. Often the questions are. They need immediate answers. Their something is somebody spend a whole day on, and this is a very synchronous engagement where with the alert flow, it's a lot more asynchronous, there's not necessarily, it's a system. Generating alert, you can investigate that on your own time. And so, if you take the cursors and the Demons of the world, it's kind of similar right with cursory in the loop. It's the most important thing of your day that you're trying to solve with cursor with Devin. It's different because you're saying could this thing for me but it's like a side tasks you give to internally and I think in our case we're also trying to take the grunt work away from Engineers that they're not immediately trying to solve. So it's more like an ambient background agent that's just doing all this work for you and
 If you check in your like, well, okay, it's all like 20 alerts from me and you have to go and look at those. How do you think about this like
 The economy or like the difference between these two worlds. Because I think these use cases are actually different. They're very different. And you can't rely on the human like hints.
 To work because you're not the Premier iD for human attention. Exactly. Yeah, and also going back to this like how have you thought about
 I don't want to have it so that people need to know this secret sauce for cleric to work, right? Or like some people have a better experience because they know these things and they are understanding of how AI works or just rag systems or whatever. And
 You want to avoid that? It will if you can. Alright. So
 Engineers often come to us and say wait. So you're going to be better at solving these problems than me and they spent years these companies and we don't claim that we just see. There's so much low-hanging fruit in terms of automation that we could automate away for you with these agents. And then just lay out where T up like all the key things you need to make the decision. So we want to lean into their domain expertise. They are the experts and we just want to make it easier for them. So what they should be assessing is the findings and metrics and the logs and the dependency graphs and all those things that they already know. Well, we don't want them to have to understand the internals of our product. I think that's failure but also because we're not a synchronous flow, you're basically looking like it's the AI is leading itself to an answer and it'll bail. If it can't find something and continue to certain point if it is on the right path. But for most part, you just producing artifacts that they can understand and Intuit already. The stuff that takes along time is just maybe switching from one tool to the next tool gather.
 In the data, trying to put two and two together and then once you have a picture, you can start to really use your expertise. But all of that before you get to the place where you have that picture, that's what you're saying, we can automate the shit. Exactly often Engineers will just grading dropping into a console or terminal and keep cuddling in and it's the same thing every single time. And there are, of course, Black Swan events and like really tough problems that maybe even an AI can't even solve for you. But there's so much gonk and like, bass mechanical rote work that Engineers have to do. And remember, they have a full-time job and a lot of cases to write software to actually make the business successful, it's not just debugging and routine investigations in the background, right? I really appreciate like these, I keep forgetting the word that you're using. I'm using like the valley but you're using golf, you know? Just knowing that there's a gaping that is helpful. Yeah. There is such a gap and now
 I'm going to start thinking about like, all the places that there's gaps and so maybe there's
 Other places that you've been thinking about because you said there was two gaps and one we went over, I think heavily, what was the other one again? Well I think now there's three gaps three before AI too three being
 Specifying then generalizing from your specification, to your actual task or data, then third is comprehension understanding.
 What the hell happened? Like, how do you tame the law? How do you even look at the long tail? If AI outputs, how do you look at your data, like, did it do it right? Like all validation Falls in that comprehension and like we can go down deep dark rabbit hole?
 But it's a really big cycle, right? Like after you've comprehended then you need to specify again and then that specification needs to generalize and just bridging. These three golfs is, so I think every IDE is going to have this problem. Every product that does something moderately complex is gonna have the problem. I'd love to also get into the edge cases if we can. One of the things we were speaking to Adam Jacobs and he was so talking about the problem in devops and a lot of space is we know that there's like a model claps effect model quote unquote you know whatever your AI system is I just there's no guarantee that you can just keep adding evils and scenarios and improve your system to get to 100% at a certain point. You may like solve one problem and then you know another problem rears its guacamole it's guacamole and so I don't know if you've got any technique or experience from your products that you've been. Yeah, I think it's about saturation, it is not 100%, it's about building up the minimal case of set of evils and
 And then to the point where you're adding more trying new things and nothing changes like that, you're done, you can't do any better like you gotta wait for a new gbt model. I asked this question. Like every single HCI talk that I go to because I think there's so much work and you know, trying to steer trying to make models or agents better, but there's a ceiling and I don't think we figured out yet.
 What defines having hit the ceiling. I'm really curious. If you have heuristics for your use case on that, we didn't really know where the ceiling is. We know that if you can sit the right expectation with the user. So what we do is we
 so, we have a lot of data on where, which types of alerts we can attack and actually 12
 And so we start with something like we do. We ask them to or our customers to export all their alerts over the last two weeks, and then we try to identify ones that we've solved, either novels, or in other, you know, companies or for other teams where we are very confident. So, there's almost three buckets. There's the first buckets you want. You're very confident you can solve if you deploy us. And the second bucket is the ones that are you need to learn in production. You're pretty confident. You can learn those. But you don't know what point it gets to the third bucket which is like you can never solve these and it's very customer dependent but that's good to markets standpoint. The first bucket is the only one that really matters. If that's big enough then they're like okay, it's valuable to have you in Broad and that's what gives you the right to stay in their environment. And then the second one is the one you want to expand and really, like, prove your worth and try and find where the third one, where is that line? Where you cross over to the third? But I think you're you're unique and thinking about this way, because many people don't even know what the first book. It is.
 Or like have a characterization of it.
 It's like anything goes with AI, right? You could ask it anything, it'll give you an answer something. It's honestly the worst thing that you can say because a customer will come to us and say, okay, if I deploy you, what can you do for me? And if you say, well, we'll figure it out, then they're like, okay, that's not good enough. Exactly what you can solve. Yeah. So, just put us into production for a few weeks and all tell you exactly what. Yeah, exactly. So that's what we focus heavily on really nailing that first class in our evils and then getting a fly will going this like learning Loop to get the second bucket. The like learned. Yeah, set of alerts really high. I like this framing. I think I'm gonna pair it to people that's doing this, but I have seen. The problem is just like not knowing, not even having a reasonable idea of what the ceiling is and not knowing where you are right now and it's it really is not about numbers. It is about Vibes, I'm very Pro Vibes but about having
 Like some confidence band around like numbers per five and not like, overall we've hit 97% or whatever. Every time I read some case study, or like some, I also do a little bit of consulting or some client might say like, oh, we're at, 94% accuracy. And I'm like, hey, what does it even mean? Yeah, prove that
 Yeah, like what I want to know is what are the three to five Vibes that you like really are trying to nail? Or like if you have well-defined accuracy metrics like time to closing or like and then give me your confidence fans on a sample like and and then just make sure that we're kind of in those. I really like this idea of hey, there's that third band that we're trying to figure out where the ceiling is, we don't know. We don't know exactly where it is.
 Have you found it? It's very logarithmic on the amount of time and you get to this point of diminishing returns and you start to be like, you know what, might just have to give it up on this one? Yeah. All the time. Well, that's fine. Like, I can still be impactful if it's not 100%. Reading your mind, all the time. What makes us good at using? AI is knowing when we can use it. Yeah.
 As you said, like, as long as that class is big enough, like yeah, there's plenty of work to automate. Yes. Long as you don't wasting Engineers time, if this is a productivity focussed product and you can be quiet if you are unsure about something, then it's okay because then if you do prompt them, it just needs to be valuable. It's like having infinite amount of interest but that one come to you. If they don't have anything good to say, we're all asked, yeah, that is that goes back to what I think about a ton is just like
 How disrespectful llms are of my time. It's like I don't need a five-page report. When only one of those sentences was actually valuable to me.
 And I think a lot of proxies there is a very in your face AI. So it's like AI. This is Stars and Glitz everywhere and that's very like ubiquitous but I think what people would really want to just like work being done for them in the background right? Or to do this is getting cleared, whether it's jira or linear or whatever. But are you trying to help folks with their prompting to or you abstract all that away? And you just give them the alerts know. We give them the findings on an alerts. So we didn't give them access directly to June the prompts but we do give them control surfaces so they can inject some guidance and it can be contextual as well, but we don't, they don't have full control over the agents. Yeah. And is that, is that because of going back to this it's like what you're saying early or where?
 They don't need to know the black magic of how to work with AI. They just need to see because it's a totally different Persona. You don't want them having to dig through the prompts now to figure out if that's the correct way to go about it. Or if there's a better way, we actually did do that at the start and we realize that what happens is every customer would then
 Dreadlock it is syncretic instructions that doesn't necessarily generalize hmm and so that's one of the we you kind of want to build muscles that you know, benefit everyone and it's kind of like a compounding effect. And what we realized is that if we had that control on our side, it's harder to start because the users have a poorer mental model but it's better for us over time. And so to this point of like model collabs, we found that you can get to a good Plateau or Baseline that everyone benefits from and the like General models, like the GPD is the drop in the summons. They helped a little bit but because this environments are so it is in critic and there's no public data set like I can't just ask you to export your whole company is like it's nothing. So we we can textualize what the agent can do but we did centrally. So based on performance Matrix we will say oh you are agent, has a certain set of skills. That's different from another agent but we can
 subselect those. So maybe if you're running dead or from ethias and you're running on kubernetes and it's all Golling, we will not have like an agent's skill set that's for python or specific to technologies that, you know, don't apply to you. So we'll try and like
 Delete or garbage collect memories or instructions as much as we can just simplify what the agent can do. So, we've got a bunch of these techniques that you know, if you look at our base agent it's our base products, the same but we do contextually like modify that slightly. So the performance numbers for each customer can be higher, but that's also risky because then like the measurements don't always like aren't able to Apple, right? Yeah.
 the other thing about exposing prompts, if you're building an application is often, when something is a little bit off,
 The first thing that people do is like go and like do some walking around in the prompt and it makes it worse. You've already gone through years of doing that, right? Like they just saw the prompt for the first day and then like it doesn't make sense, right? It's like exposing I know way I really think prompts are like code. Yeah that's like exposing your code base to the user and it's like no need to see how the sausage is made. But I think it's like not a question of like proprietary secret sauce or whatever but it's just like don't invite them to do something that's like bad for them and if you give them that surface then you can't really take it away. Yeah. They'll feel like well just spit this time doing this thing. Yeah. And you see a world where they spend so much time and it ends up being like wow this actually was waste of time and this tool that was supposed to save me time is now taking me more time because I'm yeah, tuning these problems and I'm trying to make the system work the best that it can and well. So they're going under
 The hood wasting time in that regard. That's that's true. But sometimes users feel higher like sense of attachment and Affinity to products that they've customized. If you change the colors, you put in dark mode, all those things before, you know it, like, you'd like this product, because it's yours. Right? So there's a balance, but I wouldn't expose the process. Maybe just some color sliders, so much easier. Yeah, and actually, you know, when you were talking about the different agent attributes, it reminds me of when you're playing any kind of like sports game and you have the sports player and it has that Circle and what they're good at and what they're bad at. That's what I want to see with the different clerics that all right. You have this agent and it's good to go Lang but python know the skills and python but it doesn't need it. That's a good.
 Here, you should see our latest marketing. We have some things coming out soon. That is in that vein. Yeah, we should get you on the like brand. I love it. But anyway, what else were we gonna talk about? I remember there was, there was more items on the dog. Well, I was like, kind of curious from shreyas point of view. Like, if you see feathers in production, from, let's say, users something that stuck with do they give you like a debt of dump or how does that work? And what's it like, into encyclical for you? Like how quickly can you go from a failure, back into product or something new version?
 We don't have big, like a
 Eyeball to write AI pipelines. So our it's very software. Engineering us like people will say that there is a bug, like, there's an infinite Loop here and I'm like, okay, I'll fix it and it's like typescript. I don't have anything great there. It's on them. To figure out that research project. So open source, research project, like everything comes through like Discord or like GitHub issues or whatnot for
 clients that I work with with Consulting because they were actually companies. I find that there are actually stuck in just even like being able to detect whether something is wrong. Like it's not even like a question of like their users complaining. Like they're so early stage, where they're just like, help did we like get this right? Like can I deploy that?
 But maybe that's just the people who sign up for consulting, or people who are just don't have the Comfort to even get there.
 But I think everything every single person that I talk to, you got to have some metrics whether or not they correlate.
 Very strongly with what users think and say, that's fine. But like
 Having something there to look at is a first step. And the other thing that indirectly gives you is like, if you've already instrumented in your code, it's much easier to like add new emails,
 But people think about like, oh, like having to add evals. So there is a huge thing because yeah, it is like adding observability. And instrumentation is so hard. I think there's like, there's like production failures and then there's two parts. There's one is how do you say what happened? And so we started with traces so individual run, you can see what the agent did each tools open to freezing, was the prompts all those things and then the next step for us is how do you convert that into like a new scenario. If you want to test the problem that we had is that it's not, it's, it's so manual to do the trace reviews. And so you drop into these traces, its super low level. And so we both like a summarization or like a post-process, this completely. Well, it's like Port, AI part like to demonistic, but it it collapses and contains this and clusters. All these things together, along many dimensions. So we will see four, the major groups of and we focusing on the tasks that were failures happen. They say one pulse is
 Analyze logs, you know, for indebted or the next one is, look at the conversations and the alert channel over the last couple of hours or days, there's specific tasks that frequently reoccur. And then we try and cluster those and then we look at the metrics for you know, that the agent successfully call these apis. You may even from like an engineering standpoint or they're like API failures. They're going to like Loops. Did it get a distracted? Was it efficient in finding, or solving these thoughts? And, like, many of these metrics, you don't need humans for these completely. Like, you can just parse the information. And then from that, we bought these heaps, and then the heat Maps, the roses are essentially tasks and like, what the agent did, and the columns are Matrix, and then you can see these. Like, it's just lights up like this is really just sucks. At this one thing, like it's sucks. A querying Matrix. Is that a dog? You're only person who's doing the heat maps and it was when you not do this. Yeah. And then the third person has told me
 Yeah, and then it's easy to write an email because you're like, okay I just need an email that can you know, help us. And in the next problem we ran into was creating these emails. Sometimes it's like a whole week because you need to live prod infra and so then we both like look at this is like a deeper topic but like a simulation environment that models the production environment. So as asking if you can get data from your users because in our case we couldn't and so we had like, innovate on like the Evil Lair. This simulation there, that's very interesting. People do send their pipelines and some data to us, so it's very easy to debug for us. But I think the simulation idea is super interesting. Yeah, I really like that.
 But if also feels very bespoke,
 You meet for the use case or? Yes. Yes. And not even just like like data, dogs. Simulation.
 Data drug log retrieval. Simulation is different from like I don't know like, whatever other agents that you guys have like you probably have to build like an environment for agent. Well, some extent or build some spec for agent. Sure. But so what we do is a little bit more into like the sweet Asian does with his sweet kid as he gonna have like cloth. There's a lot of similarity in the observable layers this Matrix but there's like 20 different metric systems but the idea of like a line graph or seeing metrics is not different time series, right? Yeah. So you want your agent to operate like the layer above that system.
 Um of course, there's idiosyncrasies of the technology but if you abstract that away then there's transferability between those, they give good at like data logs. You're probably good at open search logs and so you don't change much. You don't have to change much. Yes. And so often you can get dropped into, you can just plug in an mcpp for a new logging system. As long as the integration Works it'll have good performance. It's not going to be 10 out of 10 then maybe some unique things. Yeah. Having the simulation also makes it better A lot of. I tell a lot of people like you want to test their reliability of something. Just run it on a bunch of like different logs but like slightly varying terminology. I don't know whatever it is and just like make sure that your answer is the same. And a lot of times that's not true.
 Um, and you can do some sort of like, anomaly detection on those outputs to figure out like, okay, what are like the common failures that it gives you and now if you have this environment, that becomes very easy. So what we did originally was we spin up actual environments, like, actual gcp projects, actually, that's so much, everybody's doing this. The moment that's kind of in the space. I'm not sure if anybody's doing this simulation approach, but
 What's not surprising, but it's obviously, I just like these systems are good, like, repairing themselves, like, kubernetes. Like, once to bring applications of it all these software or software, like make sure that it works. Yeah, not broken. And keeping a system broken in a state that is consistent because this need to be deterministic. Otherwise, you run your agent once run it again in fails, but if the whole world has changed in, that was five minutes because the time series of different, the logs are different. You're screwed. Yeah, it's worthless environment. Change or is my agent and then you're like, just doubting and then you need a system to monitor this environment as well. So that was very hard and sometimes you like backflow, he's environments with data and you love the wrong data and then all you need to, like, delete your accounts and like reprovision them, it's just so slow. And so we all then, when a different route of like apis that are like, mocked by all limbs, but then there's also great non determinism, right? So that's also a very, very challenging direction to go. And so that's where we land on the simulation approach. These fakes, the
 Downside with the simulation approach is that it's not a perfect similar crime or replica of this like data log, right? We kind of every API because then we're like building data dog, effectively? Right? But you just need to get it to like, there's like an 80/20 rule where it's good enough, that the agent gets fooled by it. So at least some of these latest models, the agent actually realizes in the simulation know. So it's, it's in, there's a screenshot and Linkedin that my co-founder share where it figured out, it's in the simulation and it's like, that's crazy out of an investigation because it's like, this looks like a simulation environment or a government, right? Would be something like that or like a testing and wow, then you see if like change the, the Pod name and you have to change like logs to make it more realistic but that's pretty cool. What if you like tell in the system? Prompt the agent up front like this is a simulation, but it won't work. Just what we're doing. That's what that's going to backfire but doing that. It'll be like, yeah, I'm good. You basically have to tell me you're in the dojo. Yeah, you're gonna go into the
 Streets, you're training now but just trained properly as if you're gonna go streets, that's really trippy and it's cool how quickly the iteration Loop then becomes. And so you're able to figure things out and when you learn it once does, then it get replicated throughout all of the different agents. Yeah, if you if you, then it depends. Okay. This is like part of the problem. The other problem is like, how do you improve the agent actually? Fix the problem, right? So then you have to think about if you need causality, that's like spanning multiple Services. She had a knowledge graph or service graph, or do you like a learning system or there's other components to the agent that you have to expand or introduce. So I'm getting all of those things. But okay that's that's where the work really is.
 No, if you can.
 Add dog right there, the dog lamp. Yeah is legit. There are two other little dogs there cuties. Yeah they are happy dogs. It's my first language does not seem like it. All right now we're cutting this.